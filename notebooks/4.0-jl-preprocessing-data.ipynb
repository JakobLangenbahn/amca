{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pipeline from spacy\n",
    "# python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn' based on false positives\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy_langdetect import LanguageDetector\n",
    "from spacy.tokens.doc import Doc\n",
    "from spacy.vocab import Vocab\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem Tweets die über mehrere gehen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Speeches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein Problem sind die Fälle wo wir englische und deutsche sätze in einem tweets haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"Remove non alphabetic words\")\n",
    "def remove_non_alpha(doc):\n",
    "    return [token for token in doc if token.is_alpha]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.factory(\"Detect languages\")\n",
    "def create_language_detector(nlp, name):\n",
    "    return LanguageDetector(language_detection_function=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"Keep only German documents\")\n",
    "def remove_non_german(doc):\n",
    "    res = [sent for sent in doc.sents if sent._.language[\"language\"] == \"de\"]\n",
    "    if res:\n",
    "        return [token for sent in res for token in sent]\n",
    "    else:\n",
    "        return Doc(Vocab([]), words=[], spaces=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"Remove stopwords\")\n",
    "def remove_stopwords(doc): \n",
    "    return [token for token in doc if not token.is_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"Lemmatize text\")\n",
    "def lemmatize_text(doc):\n",
    "    return [token.lemma_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"Lowercase Text\")\n",
    "def lowercase(doc):\n",
    "    return [token.lower() for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_codes = re.compile(\"[\"\n",
    "                         u\"\\U0001F600-\\U0001F64F\"\n",
    "                         u\"\\U0001F300-\\U0001F5FF\"\n",
    "                         u\"\\U0001F680-\\U0001F6FF\"\n",
    "                         u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "                         u\"\\U00002500-\\U00002BEF\"\n",
    "                         u\"\\U00002702-\\U000027B0\"\n",
    "                         u\"\\U00002702-\\U000027B0\"\n",
    "                         u\"\\U000024C2-\\U0001F251\"\n",
    "                         u\"\\U0001f926-\\U0001f937\"\n",
    "                         u\"\\U00010000-\\U0010ffff\"\n",
    "                         u\"\\u2640-\\u2642\"\n",
    "                         u\"\\u2600-\\u2B55\"\n",
    "                         u\"\\u200d\"\n",
    "                         u\"\\u23cf\"\n",
    "                         u\"\\u23e9\"\n",
    "                         u\"\\u231a\"\n",
    "                         u\"\\ufe0f\"\n",
    "                         u\"\\u3030\"\n",
    "                         \"]+\", re.UNICODE)\n",
    "\n",
    "@Language.component(\"Remove emojis\")\n",
    "def remove_emojis(doc):\n",
    "    doc = [token.text for token in doc if not re.match(emoji, token.text)]\n",
    "    doc = ' '.join(doc)\n",
    "    return nlp_twitter.make_doc(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"Remove URLs\")\n",
    "def remove_urls(doc):\n",
    "    doc = [token.text for token in doc if not token.like_url]\n",
    "    doc = ' '.join(doc)\n",
    "    return nlp_twitter.make_doc(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"Remove mentions\")\n",
    "def remove_mentions(doc):\n",
    "    doc = [token.text for token in doc if not re.match(\"@.*\", token.text)]\n",
    "    doc = ' '.join(doc)\n",
    "    return nlp_twitter.make_doc(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"Remove stopwords and punctuation\")\n",
    "def remove_stopwords(doc):\n",
    "    doc = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_exclude = ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'ner', 'morphologizer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein Problem sind die Fälle wo wir englische und deutsche sätze in einem tweets haben."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.1 Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_explored = pd.read_csv(\"../data/interim/tweets_explored.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.lowercase(doc)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create spacy pipeline\n",
    "nlp_tweets = spacy.load('de_core_news_sm', exclude=pipeline_exclude)\n",
    "nlp_tweets.Defaults.stop_words |= {\"amp\", \"rt\"}\n",
    "\n",
    "# The add_pipe function appends our functions to the default pipeline.\n",
    "nlp_tweets.add_pipe(\"sentencizer\", last=True)\n",
    "nlp_tweets.add_pipe(\"Detect languages\", name='Detect languages', last=True)\n",
    "nlp_tweets.add_pipe(\"Keep only German documents\", name='Keep only German documents', last=True)\n",
    "nlp_tweets.add_pipe(\"Remove non alphabetic words\", name=\"Remove non alphabetic words\", last=True)\n",
    "nlp_tweets.add_pipe(\"Remove stopwords\", name=\"Remove stopwords\", last=True)\n",
    "nlp_tweets.add_pipe(\"Lemmatize text\", name=\"Lemmatize text\", last=True)\n",
    "nlp_tweets.add_pipe(\"Lowercase Text\", name=\"Lowercase Text\", last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2c3a4728e944682a2a2095a4b9d3f99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/257689 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tweets_explored[\"text_preprocessed\"] = tweets_explored.text.progress_apply(nlp_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a28898b2f2f34b709ee0e95688fe2729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/257689 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tweets_explored[\"text_preprocessed_sentence\"] = tweets_explored[\"text_preprocessed\"].progress_apply(\n",
    "    lambda x: \" \".join(x))\n",
    "tweets_preprocessed = tweets_explored[[\"full_name\", \"date\", \"party\", \"text_preprocessed\", \"text_preprocessed_sentence\"]]\n",
    "tweets_preprocessed.replace('', np.NaN, inplace=True)\n",
    "tweets_preprocessed.dropna(inplace=True)\n",
    "pickle.dump(tweets_preprocessed, open(\"../data/processed/tweets_processed.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_explored = pd.read_csv(\"../data/interim/speeches_explored.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.lowercase(doc)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create spacy pipeline\n",
    "nlp_speeches = spacy.load('de_core_news_sm', exclude=pipeline_exclude)\n",
    "\n",
    "# The add_pipe function appends our functions to the default pipeline.\n",
    "nlp_speeches.add_pipe('sentencizer', last=True)\n",
    "nlp_speeches.add_pipe(\"Detect languages\", name='Detect languages', last=True)\n",
    "nlp_speeches.add_pipe(\"Keep only German documents\", name='Keep only German documents', last=True)\n",
    "nlp_speeches.add_pipe(\"Remove non alphabetic words\", name=\"Remove non alphabetic words\", last=True)\n",
    "nlp_speeches.add_pipe(\"Remove stopwords\", name=\"Remove stopwords\", last=True)\n",
    "nlp_speeches.add_pipe(\"Lemmatize text\", name=\"Lemmatize text\", last=True)\n",
    "nlp_speeches.add_pipe(\"Lowercase Text\", name=\"Lowercase Text\", last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "169e0280c42a478d994f5611d57f14eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4099 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "speeches_explored[\"text_preprocessed\"] = speeches_explored.text.progress_apply(nlp_speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dd427ca1bfa42108f21b0a9d447a1a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4099 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "speeches_explored[\"text_preprocessed_sentence\"] = speeches_explored[\"text_preprocessed\"].progress_apply(\n",
    "    lambda x: \" \".join(x))\n",
    "speeches_preprocessed = speeches_explored[[\"full_name\", \"date\", \"party\", \"text_preprocessed\", \"text_preprocessed_sentence\"]]\n",
    "speeches_preprocessed.replace('', np.NaN, inplace=True)\n",
    "speeches_preprocessed.dropna(inplace=True)\n",
    "pickle.dump(speeches_preprocessed, open(\"../data/processed/speeches_processed.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Speeches"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
