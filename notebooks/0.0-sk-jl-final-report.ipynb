{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f8ee1a1",
   "metadata": {},
   "source": [
    "# Comparing the communication of German politicians across Twitter and plenary speeches using topic modelling and sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83337b4",
   "metadata": {},
   "source": [
    "# 1. Introduction (Jakob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23ec0eb",
   "metadata": {},
   "source": [
    "Twitter as a medium of research for understanding politicians' communication is a classical approach [citation](https://doi.org/10.1108/AJIM-09-2013-0083) that gained much publicity through the prominent tweets of the 45. president of the United States of America Donald J. Trump [citation](https://doi.org/10.1080/15295036.2016.1266686). To better understand the medium, we will compare the content and style of the communication of politicians with their speeches in the German Bundestag.\n",
    "\n",
    "The importance of understanding how politicians communicate on Twitter and other social media is steadily increasing with the significant influence of their content on societies at large [citation](https://doi.org/10.1080/15205436.2019.1614196), [citation](https://doi.org/10.1145/3414752.3414787), [citation](https://doi.org/10.1007/978-3-642-23333-3_3). Besides decentralizing the reciprocal transfer of information between politicians to citizens [citation](https://doi.org/10.1007/978-3-642-23333-3_3), there are also increasing problems with manipulations [citation](https://doi.org/10.5210/fm.v25i11.11431) and fake news [citation](https://doi.org/10.1075/jlp.21027.wri). An improved understanding of the medium can help identify harmful practices and interpret the content and style context-dependent. \n",
    "\n",
    "The presented work aims to help increase the understanding of the communication patterns of politicians on Twitter by comparing the content and sentiment of their tweets to their plenary speeches. We execute this analysis for prominent German politicians of the 19th Bundestag in the time range from 2017 to 2021. For this, we defined the following six research questions:\n",
    "\n",
    "* **RQ 1.1** What are the main topics of tweets of prominent politicians of the six parties in the German Bundestag differ in the period of the 19th Bundestag?\n",
    "\n",
    "* **RQ 1.2** What are the main topics of speeches of prominent politicians of the six parties in the German Bundestag differ in the period of the 19th Bundestag?\n",
    "\n",
    "* **RQ 1.3** How do the main topics of tweets and speeches of prominent politicians of the six parties in the German Bundestag differ in the period of the 19th Bundestag?\n",
    "\n",
    "Our approach uses data scraped directly from Twitter and plenary speeches obtained from [open discourse](https://github.com/open-discourse/open-discourse) for creating topic models and sentiment analyses for the tweets and speeches of the politicians. For this, we set up a pipeline in Python that preprocesses the data for the modelling part. Before choosing the final best performing model, we try separate models for topic modelling, including Latent Dirichlet Allocation, Non-Negative Matrix Factorization and BERTopic. We use an unsupervised dictionary-based approach that we test with two different sentiment dictionaries for the sentiment analysis. We validate our results with the current state of the art evaluation methods.\n",
    "\n",
    "The remaining work is structured into four sections Literature Review, Methodology, Results and Discussion. The literature review will analyze existing research approaches and showcase our contributions. The subsequent section comprises the preprocessing and modelling for our analyses, presented as commented code complemented by explanations. Based on the results from the methodology part, we will analyze and validate the results in the fourth section. Finally, we discuss the obtained results and outlook on further work in the last section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6817b5ca",
   "metadata": {},
   "source": [
    "# 2. Literature Review (Stjepan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a3b99e",
   "metadata": {},
   "source": [
    "**Needs to be added** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0aa7d8",
   "metadata": {},
   "source": [
    "# 3. Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3483d817",
   "metadata": {},
   "source": [
    "# 3.1 Technical setup (Jakob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c2e18e",
   "metadata": {},
   "source": [
    "We present the results of our work in a Jupyter Notebook, that contains the commented code and additional explanations, analaysis and evaluation. The project was programmed in the programming language Python using various preexisting packages. It is possible to reproduce all results with the provided complementary files. For this we recommend to setup an conda enviroment using Python 3.8. One can then install all the packages using the provided .txt file. Besides these packages you will need to setup an docker enviroment in section 3.3 if you want to reproducde the data collection. There are seperate introductions in the section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d095e3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment when you setup the enviroment the first time\n",
    "# ! pip install -r requirements.txt\n",
    "# ! python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1665ef",
   "metadata": {},
   "source": [
    "After installing the packages one can import them with the following lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf5b14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "\n",
    "# Import basic Python packages\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import random\n",
    "from pprint import pprint\n",
    "from imp import reload\n",
    "import warnings\n",
    "from operator import itemgetter\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "\n",
    "# Import util packages\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Import data procesing packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "\n",
    "# Import visualisation packages\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Import natural language processing packages\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy_langdetect import LanguageDetector\n",
    "from spacy.tokens.doc import Doc\n",
    "from spacy.vocab import Vocab\n",
    "\n",
    "# Import topic modeling packages\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel, LdaMulticore\n",
    "from gensim.models.nmf import Nmf\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "from bertopic import BERTopic\n",
    "\n",
    "# Import metrics packages\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# Import interface widgets\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Set options\n",
    "pd.options.mode.chained_assignment = None\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "tqdm.pandas()\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c515ba7c",
   "metadata": {},
   "source": [
    "After loading all the packages and installing all dependencies you can run the whole code. Parts that require longer execution time are commented out and the results of the code part are imported seperatedly. If you want to reproduce all analysis you need to uncomment the parts and commented out the importing of the results. The necessary steps for this are always described in the code. We do not recommend this step if not necessary as some models have runtimes over eight hours."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb73ce9",
   "metadata": {},
   "source": [
    "# 3.2 Scrape Twitter data (Stjepan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673ba6fd",
   "metadata": {},
   "source": [
    "**Needs to be added** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da83f46",
   "metadata": {},
   "source": [
    "# 3.3 Retrieve plenary proceedings data (Jakob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e161870",
   "metadata": {},
   "source": [
    "In this section we retrieve the plenar protocoll data from the 19. German Bundestag. As the data is publically available, they can be downloaded from the official [website](https://www.bundestag.de/services/opendata). Currently the format of the files is not very convenient for automatic analysis, this is why the researcher from [open discourse](https://opendiscourse.de) published a preprocessed version of the plenar protocolls. We use their data, as setting up an own preprocessing pipeline would be very time intensive and out of scope for this work. We use their provided [Docker container](https://github.com/open-discourse/open-discourse) to quickly setup the database for our use. We then query the needed data and export them to a csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7489cb3d",
   "metadata": {},
   "source": [
    "## 3.3.1 Setup local database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26aa3b81",
   "metadata": {},
   "source": [
    "To use the database we setup the Docker container from [open discourse](https://open-discourse.github.io/open-discourse-documentation/1.0.0/run-the-database-locally.html#use-the-database). Before this we have to download and setup Docker according to these [instructions](https://www.docker.com/products/docker-desktop). After this step we can launch Docker and proceeed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c48340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define connection details\n",
    "con_details = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"database\": \"next\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"postgres\",\n",
    "    \"port\": \"5432\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733b632c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to Docker container\n",
    "# Uncomment if you want to set up the Docker container\n",
    "# os.system(\"cd ..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5e1139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to Github for Docker access\n",
    "# Uncomment if you want to set up the Docker container\n",
    "# os.system(\"docker login docker.pkg.github.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15271435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Only on the first run) download the Docker container\n",
    "# Uncomment if you want to set up the Docker container\n",
    "# os.system(\"docker pull docker.pkg.github.com/open-discourse/open-discourse/database:latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93e7c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start and run the database in the Docker container\n",
    "# Uncomment if you want to set up the Docker container\n",
    "# os.system(\"docker run --env POSTGRES_USER=postgres --env POSTGRES_DB=postgres --env POSTGRES_PASSWORD=postgres -p 5432:5432 -d docker.pkg.github.com/open-discourse/open-discourse/database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaea5697",
   "metadata": {},
   "source": [
    "## 3.3.2 Retrieve plenary proceedings from database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652d20b0",
   "metadata": {},
   "source": [
    "After we have setup the PostgreSQL database, we now can query the required data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd794eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define query\n",
    "query = \"\"\"SELECT * from open_discourse.speeches WHERE electoral_term = 19\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d77f820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create connection\n",
    "# Uncomment if you want to query the database\n",
    "# con = psycopg2.connect(**con_details) # If this fails, repeat execution of the cell.\n",
    "# cur = con.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121f626e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute query\n",
    "# Uncomment if you want to query the database\n",
    "# cur.execute(query)\n",
    "# rows = cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e057dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform results in dataframe\n",
    "# Uncomment if you made a new query to the database\n",
    "# speeches_retrieved = pd.DataFrame(rows)\n",
    "# speeches_retrieved.columns = [\"id\", \"session\", \"electoral_term\", \"first_name\", \"last_name\", \"politician_id\", \"text\",\n",
    "#                       \"fraction_id\", \"document_url\", \"position_short\", \"position_long\", \"date\", \n",
    "#                       \"search_speech_content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b00259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export resullts to csv\n",
    "# Uncomment if you made a new query to the database\n",
    "# speeches_retrieved.to_csv(\"../data/raw/speeches_retrieved.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab8aaf5",
   "metadata": {},
   "source": [
    "We save the retrieved data as a CSV file and can use it now for further processing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8387f0",
   "metadata": {},
   "source": [
    "# 3.3 Data Exploration (Jakob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d488f703",
   "metadata": {},
   "source": [
    "## 3.3.1 Tweets exploration (Stjepan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d91634f",
   "metadata": {},
   "source": [
    "#### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbd3585",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load tweets data\n",
    "tweets_scraped = pd.read_csv(\"../data/raw/tweets_scraped.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d8a2e7",
   "metadata": {},
   "source": [
    "#### Check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6375b453",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_scraped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a2b213",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_scraped.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2202be29",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_scraped.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4bb168",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_scraped.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8359260",
   "metadata": {},
   "source": [
    "#### Drop missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb99dee",
   "metadata": {},
   "source": [
    "We can drop all records with missing data, as we cannot use these records for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8336e732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing data\n",
    "tweets_scraped.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4dbb07",
   "metadata": {},
   "source": [
    "#### Clean names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1107d700",
   "metadata": {},
   "source": [
    "For better comparability, we harmonize the names in the tweets and speeches data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c9d4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create twitter username to real name dictionary\n",
    "usernames_to_fullname = {'rbrinkhaus': 'Ralph Brinkhaus', 'groehe': 'Hermann Gröhe', \n",
    "                         'NadineSchoen': 'Nadine Schön', 'n_roettgen': 'Norbert Röttgen',\n",
    "                         'peteraltmaier': 'Peter Altmaier', 'jensspahn': 'Jens Spahn', \n",
    "                         'MatthiasHauer': 'Matthias Hauer', 'c_lindner': 'Christian Lindner',\n",
    "                         'MarcoBuschmann': 'Marco Buschmann', 'starkwatzinger': 'Bettina Stark-Watzinger',\n",
    "                         'Lambsdorff': 'Alexander Graf Lambsdorff', 'johannesvogel': 'Johannes Vogel',\n",
    "                         'KonstantinKuhle': 'Konstantin Kuhle', 'MAStrackZi': 'Marie-Agnes Strack-Zimmermann',\n",
    "                         'larsklingbeil': 'Lars Klingbeil', 'EskenSaskia': 'Saskia Esken',\n",
    "                         'hubertus_heil': 'Hubertus Heil', 'HeikoMaas': 'Heiko Maas',\n",
    "                         'MartinSchulz': 'Martin Schulz', 'KarambaDiaby': 'Karamba Diaby',\n",
    "                         'Karl_Lauterbach': 'Karl Lauterbach', 'SteffiLemke': 'Steffi Lemke',\n",
    "                         'cem_oezdemir': 'Cem Özdemir', 'GoeringEckardt': 'Katrin Göring-Eckardt',\n",
    "                         'KonstantinNotz': 'Konstantin von Notz', '6': 'Konstantin von Notz',\n",
    "                         'BriHasselmann': 'Britta Haßelmann', 'svenlehmann': 'Sven Lehmann',\n",
    "                         'ABaerbock': 'Annalena Baerbock', 'ABaerbockArchiv': 'Annalena Baerbock',\n",
    "                         'SWagenknecht': 'Sahra Wagenknecht', 'b_riexinger': 'Bernd Riexinger',\n",
    "                         'NiemaMovassat': 'Niema Movassat', 'jankortemdb': 'Jan Korte',\n",
    "                         'DietmarBartsch': 'Dietmar Bartsch', 'GregorGysi': 'Gregor Gysi',\n",
    "                         'SevimDagdelen': 'Sevim Dağdelen', 'Alice_Weidel': 'Alice Weidel',\n",
    "                         'Beatrix_vStorch': 'Beatrix von Storch', 'JoanaCotar': 'Joana Cotar',\n",
    "                         'StBrandner': 'Stephan Brandner', 'Tino_Chrupalla': 'Tino Chrupalla',\n",
    "                         'GtzFrmming': 'Götz Frömming', '3': 'Götz Frömming', 'Leif_Erik_Holm': 'Leif-Erik Holm'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4200ac45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add full name\n",
    "tweets_scraped[\"full_name\"] = tweets_scraped.username.replace(usernames_to_fullname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b96d33c",
   "metadata": {},
   "source": [
    "#### Check time data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a91c6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add normalized date\n",
    "tweets_scraped[\"date\"] = pd.to_datetime(tweets_scraped[\"datetime\"], format = \"%Y-%m-%d\").dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ccab9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_scraped.date.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753bccb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_scraped.date.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a24e84c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tweet number per time\n",
    "tweets_scraped.groupby('date')['tweet_id'].size().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36300cff",
   "metadata": {},
   "source": [
    "We now can drop all data that are not also represented in the speeches dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577cea24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unneded data\n",
    "tweets_subset = tweets_scraped[np.logical_and(tweets_scraped.date >= pd.Timestamp(\"24.10.2017\"), tweets_scraped.date <= pd.Timestamp(\"07.05.2021\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79fdf94",
   "metadata": {},
   "source": [
    "#### Checkt party distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1449e7c4",
   "metadata": {},
   "source": [
    "When checking the distribution of tweets per party, we can see differences, but they do not significantly alter our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3221a84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweets per party\n",
    "tweets_subset.groupby(\"party\").size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bc4944",
   "metadata": {},
   "source": [
    "#### Check politician distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4e5720",
   "metadata": {},
   "source": [
    "We see significant differences between the number of tweets per politician ranging from nearly 29665 to 658. We have to consider this in our work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a581ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweets per politican\n",
    "tweets_scraped.groupby('full_name')['tweet_id'].size().sort_values().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bceecb1",
   "metadata": {},
   "source": [
    "We see an strongly increasing trend of tweets per day. This is caused by two new parties entering the bundestag in 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e08a3c0",
   "metadata": {},
   "source": [
    "#### Check text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c58c8d2",
   "metadata": {},
   "source": [
    "We check the texts of the tweets with a word cloud. We can infer the need for data preprocessing from a first analysis of the visualisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf6f74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a word cloud\n",
    "long_string_tweets = ' '.join(tweets_scraped[\"text\"].tolist())\n",
    "wordcloud_tweets = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue')\n",
    "wordcloud_tweets.generate(long_string_tweets)\n",
    "wordcloud_tweets.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b735122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a counter object\n",
    "counter_tweets = Counter(long_string_tweets.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9094c6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the most common words\n",
    "counter_tweets.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29e840d",
   "metadata": {},
   "source": [
    "We can identify the need for a stopword removal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a846d2",
   "metadata": {},
   "source": [
    "#### Drop unneeded columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4eb7610",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop unneeded columns\n",
    "tweets_subset.drop(['datetime', 'tweet_id', 'username','name', 'reply_count'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f6cf99",
   "metadata": {},
   "source": [
    "#### Export data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2388702",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_subset.to_csv(\"../data/interim/tweets_explored.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859f0785",
   "metadata": {},
   "source": [
    "## 3.3.2 Explore speeches of politicians (Jakob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9adf460",
   "metadata": {},
   "source": [
    "In a first analysis step we get an overview of the retrieved data and do first simple preprocessing tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd9db0b",
   "metadata": {},
   "source": [
    "### 3.3.2.1 Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2198986",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load tweets data\n",
    "# Comment out if you retrieve the data from scratch\n",
    "speeches_retrieved = pd.read_csv(\"../data/raw/speeches_retrieved.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700d60aa",
   "metadata": {},
   "source": [
    "### 3.3.2.2 Check data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce641611",
   "metadata": {},
   "source": [
    "We use standard steps of data exploration to get an overview of the retrieved data including datatypes and missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4480535",
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_retrieved.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31552a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_retrieved.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b19c0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_retrieved.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4e7a8b",
   "metadata": {},
   "source": [
    "Based on the first overview of the data we can identfiy different variables, that we have to deep dive into to better understand the data quality and prepare first processing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049695b2",
   "metadata": {},
   "source": [
    "### 3.3.2.3 Drop missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3da83f",
   "metadata": {},
   "source": [
    "We can drop all records with missing speech content, as we cannot use these records for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa4db0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing data\n",
    "speeches_retrieved.dropna(subset = [\"text\"], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980bc538",
   "metadata": {},
   "source": [
    "### 3.3.2.4 Clean names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3df084",
   "metadata": {},
   "source": [
    "For better comparability, we harmonize the politicians names in the tweets and speeches data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b873d87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add full name of politicians\n",
    "speeches_retrieved[\"full_name\"] = speeches_retrieved[\"first_name\"] + \" \" + speeches_retrieved[\"last_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ebd1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset to the selected politicians\n",
    "speeches_subset = speeches_retrieved[speeches_retrieved.full_name.isin(tweets_subset.full_name.unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26449813",
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_subset.groupby('full_name')['id'].size().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e69f4f3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Speeches per politican\n",
    "speeches_subset.groupby('full_name')['id'].size().sort_values().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e90fa19",
   "metadata": {},
   "source": [
    "There are significant differences between the number of speeches per politician ranging from 252 to 5. We have to consider this in the interpretation of our results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c47b94",
   "metadata": {},
   "source": [
    "### 3.3.2.5 Check time data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c81d515",
   "metadata": {},
   "source": [
    "For an analyis of the topic per time, we need to have the data in a pandas dateformat. Additionally we controll the time span with retrieved speeches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec0f258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add normalized date\n",
    "speeches_subset[\"date\"] = pd.to_datetime(speeches_subset[\"date\"], format = \"%Y-%m-%d\").dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c21d4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find first day with speeches\n",
    "speeches_subset.date.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fde2bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find last day with speeches\n",
    "speeches_subset.date.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c66145d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Speech number per time\n",
    "speeches_subset.groupby('date')['id'].size().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dbc0ec",
   "metadata": {},
   "source": [
    "We see some patterns in the time series, however there are no signficant gaps in the observed time frame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdc0c9b",
   "metadata": {},
   "source": [
    "### 3.3.2.6 Check party distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b925561",
   "metadata": {},
   "source": [
    "To controll the distributions of tweets per party, we assign the party of the author to each speech. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae338430",
   "metadata": {},
   "outputs": [],
   "source": [
    "fullname_to_party = {'Ralph Brinkhaus': 'CDU', 'Hermann Gröhe': 'CDU', 'Nadine Schön': 'CDU', \n",
    "                     'Norbert Röttgen': 'CDU', 'Peter Altmaier': 'CDU', 'Jens Spahn': 'CDU', \n",
    "                     'Matthias Hauer': 'CDU', 'Christian Lindner': 'FDP', 'Marco Buschmann': 'FDP',\n",
    "                     'Bettina Stark-Watzinger': 'FDP', 'Alexander Graf Lambsdorff': 'FDP', 'Johannes Vogel': 'FDP',\n",
    "                     'Konstantin Kuhle': 'FDP', 'Marie-Agnes Strack-Zimmermann': 'FDP', 'Lars Klingbeil': 'SPD',\n",
    "                     'Saskia Esken': 'SPD', 'Hubertus Heil': 'SPD', 'Heiko Maas': 'SPD', 'Martin Schulz': 'SPD', \n",
    "                     'Karamba Diaby': 'SPD', 'Karl Lauterbach': 'SPD', 'Steffi Lemke': 'Grüne',\n",
    "                     'Cem Özdemir': 'Grüne', 'Katrin Göring-Eckardt': 'Grüne', 'Konstantin von Notz': 'Grüne',\n",
    "                     'Britta Haßelmann': 'Grüne', 'Sven Lehmann': 'Grüne', 'Annalena Baerbock': 'Grüne',\n",
    "                     'Sahra Wagenknecht': 'Linke', 'Bernd Riexinger': 'Linke', 'Niema Movassat': 'Linke', \n",
    "                     'Jan Korte': 'Linke', 'Dietmar Bartsch': 'Linke', 'Gregor Gysi': 'Linke', \n",
    "                     'Sevim Dağdelen': 'Linke', 'Alice Weidel': 'AFD', 'Beatrix von Storch': 'AFD', \n",
    "                     'Joana Cotar': 'AFD', 'Stephan Brandner': 'AFD', 'Tino Chrupalla': 'AFD',\n",
    "                     'Götz Frömming': 'AFD', 'Leif-Erik Holm': 'AFD'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da0955c",
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_subset[\"party\"] = speeches_subset.full_name.replace(fullname_to_party)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5587806d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speeches per party\n",
    "speeches_subset.groupby(\"party\").size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dc5713",
   "metadata": {},
   "source": [
    "When checking the distribution of speeches per party, we can see differences, but we do not expect them to significantly alter our results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57897c5",
   "metadata": {},
   "source": [
    "### 3.3.2.7 Check text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9448d4",
   "metadata": {},
   "source": [
    "We check the texts of the tweets with a word cloud. We can infer the need for data preprocessing from a first analysis of the visualisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc66169f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a word cloud\n",
    "long_string_speeches = ' '.join(speeches_subset[\"text\"].tolist())\n",
    "wordcloud_speeches = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, \n",
    "                               contour_color='steelblue')\n",
    "wordcloud_speeches.generate(long_string_speeches)\n",
    "wordcloud_speeches.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb713935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a counter object\n",
    "speeches_counter = Counter(long_string_speeches.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ad7507",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the most common words\n",
    "speeches_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3aca27a",
   "metadata": {},
   "source": [
    "There is a clear need for extensive stopword removal, to reduce noise in the topic and sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6a9779",
   "metadata": {},
   "source": [
    "### 3.3.2.8 Drop unneeded columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39e073b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop unneeded columns\n",
    "speeches_subset.drop(['id', 'session', 'electoral_term', 'first_name', 'last_name', 'politician_id',\n",
    "                      'fraction_id', 'document_url', 'position_short', 'position_long', 'search_speech_content'],\n",
    "                     axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd875c5",
   "metadata": {},
   "source": [
    "### 3.3.2.9 Export data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfba633",
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_subset.to_csv(\"../data/interim/speeches_explored.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76738976",
   "metadata": {},
   "source": [
    "This section explored the speeches dataset and controlled the data quality of different important variables. The data quality is satisfactory, except for an highly skewed distribution of speeches per politicians."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d053ee7",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3.4 Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939ab097",
   "metadata": {},
   "source": [
    "## 3.4.1 Prepare spacy pipelines (Jakob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b010d00",
   "metadata": {},
   "source": [
    "In the last section we identified the need for an extensive preprocessing. We build an flexible spacy pipeline strucutre, that can easily add or remove different preprocessing steps. We base our model on the pretrained [spacy pipeline](https://spacy.io/models/de) for German documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf166ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"Remove non alphabetic words\")\n",
    "def remove_non_alpha(doc):\n",
    "    return [token for token in doc if token.is_alpha]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a924bb2c",
   "metadata": {},
   "source": [
    "We identified the need remove non German text, as they reduce the quality of our topic and sentiment models. For this we use an an language detector and an additional component, that only removes sentences of other languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133f7f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.factory(\"Detect languages\")\n",
    "def create_language_detector(nlp, name):\n",
    "    return LanguageDetector(language_detection_function=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa36feeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"Keep only German documents\")\n",
    "def remove_non_german(doc):\n",
    "    res = [sent for sent in doc.sents if sent._.language[\"language\"] == \"de\"]\n",
    "    if res:\n",
    "        return [token for sent in res for token in sent]\n",
    "    else:\n",
    "        return Doc(Vocab([]), words=[], spaces=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ed094f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"Remove stopwords\")\n",
    "def remove_stopwords(doc): \n",
    "    return [token for token in doc if not token.is_stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8c9e15",
   "metadata": {},
   "source": [
    "We lemmatize the resulting tokens to keep the semantic meaning of resulting words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec679aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"Lemmatize text\")\n",
    "def lemmatize_text(doc):\n",
    "    return [token.lemma_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ee82d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"Lowercase Text\")\n",
    "def lowercase(doc):\n",
    "    return [token.lower() for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a035e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_codes = re.compile(\"[\"\n",
    "                         u\"\\U0001F600-\\U0001F64F\"\n",
    "                         u\"\\U0001F300-\\U0001F5FF\"\n",
    "                         u\"\\U0001F680-\\U0001F6FF\"\n",
    "                         u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "                         u\"\\U00002500-\\U00002BEF\"\n",
    "                         u\"\\U00002702-\\U000027B0\"\n",
    "                         u\"\\U00002702-\\U000027B0\"\n",
    "                         u\"\\U000024C2-\\U0001F251\"\n",
    "                         u\"\\U0001f926-\\U0001f937\"\n",
    "                         u\"\\U00010000-\\U0010ffff\"\n",
    "                         u\"\\u2640-\\u2642\"\n",
    "                         u\"\\u2600-\\u2B55\"\n",
    "                         u\"\\u200d\"\n",
    "                         u\"\\u23cf\"\n",
    "                         u\"\\u23e9\"\n",
    "                         u\"\\u231a\"\n",
    "                         u\"\\ufe0f\"\n",
    "                         u\"\\u3030\"\n",
    "                         \"]+\", re.UNICODE)\n",
    "\n",
    "@Language.component(\"Remove emojis\")\n",
    "def remove_emojis(doc):\n",
    "    doc = [token.text for token in doc if not re.match(emoji_codes, token.text)]\n",
    "    doc = ' '.join(doc)\n",
    "    return nlp_twitter.make_doc(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce65828e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"Remove URLs\")\n",
    "def remove_urls(doc):\n",
    "    doc = [token.text for token in doc if not token.like_url]\n",
    "    doc = ' '.join(doc)\n",
    "    return nlp_twitter.make_doc(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490cce47",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"Remove mentions\")\n",
    "def remove_mentions(doc):\n",
    "    doc = [token.text for token in doc if not re.match(\"@.*\", token.text)]\n",
    "    doc = ' '.join(doc)\n",
    "    return nlp_twitter.make_doc(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a7278b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"Remove stopwords and punctuation\")\n",
    "def remove_stopwords(doc):\n",
    "    doc = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe251f3",
   "metadata": {},
   "source": [
    "## 3.4.2 Topic modeling preprocessing (Jakob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a5f801",
   "metadata": {},
   "source": [
    "We do not all preptrained pipeline elements and therefore exclude them. In the next step we will add additional needed previous defined components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e54bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude not needed pipeline elements\n",
    "pipeline_exclude = ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'ner', 'morphologizer']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f6ae42",
   "metadata": {},
   "source": [
    "###  3.4.2.1 Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4d92b7",
   "metadata": {},
   "source": [
    "In this subsection we define a pipeline for the preprocessing of the twitter data and execute the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cf5c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "tweets_explored = pd.read_csv(\"../data/interim/tweets_explored.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71833239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spacy pipeline\n",
    "nlp_tweets = spacy.load('de_core_news_sm', exclude=pipeline_exclude)\n",
    "nlp_tweets.Defaults.stop_words |= {\"amp\", \"rt\"}\n",
    "\n",
    "# Add needed pipeline components\n",
    "nlp_tweets.add_pipe(\"sentencizer\", last=True)\n",
    "nlp_tweets.add_pipe(\"Detect languages\", name='Detect languages', last=True)\n",
    "nlp_tweets.add_pipe(\"Keep only German documents\", name='Keep only German documents', last=True)\n",
    "nlp_tweets.add_pipe(\"Remove non alphabetic words\", name=\"Remove non alphabetic words\", last=True)\n",
    "nlp_tweets.add_pipe(\"Remove stopwords\", name=\"Remove stopwords\", last=True)\n",
    "nlp_tweets.add_pipe(\"Lemmatize text\", name=\"Lemmatize text\", last=True)\n",
    "nlp_tweets.add_pipe(\"Lowercase Text\", name=\"Lowercase Text\", last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5f8cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply pipeline to text\n",
    "# Uncomment if you want to update the preprocessing of the data \n",
    "# tweets_explored[\"text_preprocessed\"] = tweets_explored.text.progress_apply(nlp_tweets)\n",
    "# This takes approximately one hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56f3675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add sentence structure\n",
    "# Uncomment if you want to update the preprocessing of the data \n",
    "# tweets_explored[\"text_preprocessed_sentence\"] = tweets_explored[\"text_preprocessed\"].progress_apply(\n",
    "#    lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1275a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset needed data\n",
    "# Uncomment if you want to update the preprocessing of the data \n",
    "# tweets_preprocessed = tweets_explored[[\"full_name\", \"date\", \"party\", \"text\", \"text_preprocessed\",\n",
    "#                                       \"text_preprocessed_sentence\", 'retweet_count', 'like_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c306428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop empty texts\n",
    "# Uncomment if you want to update the preprocessing of the data\n",
    "# tweets_preprocessed.replace('', np.NaN, inplace=True)\n",
    "# tweets_preprocessed.dropna(inplace=True)\n",
    "# tweets_preprocessed.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6edf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data as pickle file\n",
    "# Uncomment if you want to update the preprocessing of the data\n",
    "# pickle.dump(tweets_preprocessed, open(\"../data/processed/tweets_processed.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1cb7cc",
   "metadata": {},
   "source": [
    "We now can use the resulting file to train a topic model for the tweets dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a88c3b",
   "metadata": {},
   "source": [
    "### 3.4.2.2 Speeches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455337d4",
   "metadata": {},
   "source": [
    "In this subsection we define a pipeline for the preprocessing of the speeches data and execute the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63315a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "speeches_explored = pd.read_csv(\"../data/interim/speeches_explored.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd992cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spacy pipeline\n",
    "nlp_speeches = spacy.load('de_core_news_sm', exclude=pipeline_exclude)\n",
    "\n",
    "# Add needed pipeline components\n",
    "nlp_speeches.add_pipe('sentencizer', last=True)\n",
    "nlp_speeches.add_pipe(\"Detect languages\", name='Detect languages', last=True)\n",
    "nlp_speeches.add_pipe(\"Keep only German documents\", name='Keep only German documents', last=True)\n",
    "nlp_speeches.add_pipe(\"Remove non alphabetic words\", name=\"Remove non alphabetic words\", last=True)\n",
    "nlp_speeches.add_pipe(\"Remove stopwords\", name=\"Remove stopwords\", last=True)\n",
    "nlp_speeches.add_pipe(\"Lemmatize text\", name=\"Lemmatize text\", last=True)\n",
    "nlp_speeches.add_pipe(\"Lowercase Text\", name=\"Lowercase Text\", last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232d027b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply pipeline to text\n",
    "# Uncomment if you want to update the preprocessing of the data\n",
    "# speeches_explored[\"text_preprocessed\"] = speeches_explored.text.progress_apply(nlp_speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bf0962",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add sentence structure\n",
    "# Uncomment if you want to update the preprocessing of the data\n",
    "# speeches_explored[\"text_preprocessed_sentence\"] = speeches_explored[\"text_preprocessed\"].progress_apply(\n",
    "#    lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4b7a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset needed data\n",
    "# Uncomment if you want to update the preprocessing of the data\n",
    "# speeches_preprocessed = speeches_explored[[\"full_name\", \"date\", \"party\", \"text\",\n",
    "#                                           \"text_preprocessed\", \"text_preprocessed_sentence\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69440fd1",
   "metadata": {},
   "source": [
    "We identified the need for additional removing of frequent words, for topic modeling. There are many words coming from greeting phrases (Sehr, geehrte, Frauen, Herren) that do not have semantic relevance for our analyses, but interfere with the model quality based on their frequency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dce0161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for removing frequent words\n",
    "def remove_frequent_words(words_list, most_frequent_words):\n",
    "    return [word for word in words_list if word not in most_frequent_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a6a0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional preprocessing for Bertopic model\n",
    "# Uncomment if you want to update the preprocessing of the data\n",
    "# long_string_speeches= ' '.join(speeches_preprocessed.text_preprocessed_sentence.tolist())\n",
    "# counter_speeches = Counter(long_string_speeches.split())\n",
    "# most_frequent_words = []\n",
    "# for item in counter_speeches.most_common(200):\n",
    "#    most_frequent_words.append(item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b34cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns with preprocessed text and removed frequent words\n",
    "# Uncomment if you want to update the preprocessing of the data\n",
    "# speeches_preprocessed[\"text_preprocessed_infrequent\"] = speeches_preprocessed.text_preprocessed.progress_apply(remove_frequent_words,most_frequent_words = most_frequent_words)\n",
    "# speeches_preprocessed[\"text_preprocessed_infrequent_sentence\"] = speeches_preprocessed[\"text_preprocessed_infrequent\"].progress_apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c395203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop empty texts\n",
    "# Uncomment if you want to update the preprocessing of the data\n",
    "# speeches_preprocessed.replace('', np.NaN, inplace=True)\n",
    "# speeches_preprocessed.dropna(inplace=True)\n",
    "# speeches_preprocessed.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4c70fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data as pickle file\n",
    "# Uncomment if you want to update the preprocessing of the data\n",
    "# pickle.dump(speeches_preprocessed, open(\"../data/processed/speeches_processed.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f60d08",
   "metadata": {},
   "source": [
    "We now can use the resulting file to train a topic model for the speeches dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8efa658",
   "metadata": {},
   "source": [
    "## 3.4.3 Sentiment analysis preprocessing (Stjepan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6237a7d3",
   "metadata": {},
   "source": [
    "### 3.4.3.1 Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c938bc98",
   "metadata": {},
   "source": [
    "### 3.4.3.2 Speeches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c72331",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3.5 Topic Modeling (Jakob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fd721c",
   "metadata": {},
   "source": [
    "To better understand the differences in communication of politicians on Twitter and in the Bundestag, we perform a topic modeling. For this we test three different approaches, before we choose the best performing as our final model. We apply hyperparameter tuning if applicable but omit classic train test split validation. We are gonna analyse the validity of the topic model in the Results section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95350a12",
   "metadata": {},
   "source": [
    "## 3.5.1 Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79473317",
   "metadata": {},
   "source": [
    "Latent Dirichlet Allocation (LDA) constitutes a state of the art approach [citation](https://doi.org/10.1186/s40537-019-0255-7) for topic modelling. LDA is an unsupervised machine learning technique that uses generative statistical models to extract topics from a collection of documents [citation](https://dl.acm.org/doi/10.5555/944919.944937). The underlying model assigns a probability distribution over the vocabulary of the documents to topics that can be used for topic detection. We will base our choice of the optimal hyperparameter combination on the coherence of the resulting topic model. This decision is based on the discussion [here](http://topicmodels.info/ckling/tmt/part4.pdf) and [here](https://dl.acm.org/doi/abs/10.1145/2684822.2685324). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d616b0",
   "metadata": {},
   "source": [
    "### 3.5.1.1 Define hyperparameters for optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e79c36",
   "metadata": {},
   "source": [
    "We optimize the hyperparameters of the LDA model based on a grid search with the variables topic number (k), the a-priory belief of document-topic distribution (alpha) and the the a-priory  belief of topic-word distribution (eta) [citation](https://radimrehurek.com/gensim/models/ldamodel.html). This hyperparemter optimization is loosely based on this [article](https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16aaf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topics range\n",
    "min_topics = 10\n",
    "max_topics = 150\n",
    "step_size = 10\n",
    "topics_range = range(min_topics, max_topics, step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed01ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpha parameter\n",
    "alpha = list(np.arange(0.01, 1, 0.3)) #\n",
    "alpha.append('symmetric')\n",
    "alpha.append('asymmetric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7a5bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beta parameter\n",
    "beta = list(np.arange(0.01, 1, 0.3))\n",
    "beta.append('symmetric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc60d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating coherence values of specific hyperparamter combinations\n",
    "def compute_lda_coherence_values(corpus, text, id2word, k, a, b):\n",
    "    lda_model = LdaMulticore(corpus=corpus,\n",
    "                             id2word=id2word,\n",
    "                             num_topics=k, \n",
    "                             random_state=42,\n",
    "                             alpha=a,\n",
    "                             eta=b)\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=text, dictionary=id2word, coherence='c_v')\n",
    "    return coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfed8ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for executing a hyperparameter optimization\n",
    "def hyperparameter_lda(data_preprocessed, title, topics_range, alpha, beta):\n",
    "    id2word = corpora.Dictionary(data_preprocessed.text_preprocessed.to_list())\n",
    "    # These hyperparameter could also be trialed in an extend scope\n",
    "    id2word.filter_extremes(no_below=10, no_above=0.1)\n",
    "    texts = data_preprocessed.text_preprocessed.to_list()\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "    model_results = {'Topics': [],\n",
    "                     'Alpha': [],\n",
    "                     'Beta': [],\n",
    "                     'Coherence': []\n",
    "                    }\n",
    "    grid = {}\n",
    "    grid['Validation_Set'] = {}\n",
    "    for k in tqdm(topics_range):\n",
    "        print(\"Number of topics:\" + str(k))\n",
    "        for a in tqdm(alpha):\n",
    "            print(\"Alpha value:\" + str(a))\n",
    "            for b in tqdm(beta):\n",
    "                print(\"Beta value:\" + str(b))\n",
    "                cv = compute_lda_coherence_values(corpus=corpus,text = texts,\n",
    "                                              id2word=id2word, k=k, a=a, b=b)\n",
    "                model_results['Topics'].append(k)\n",
    "                model_results['Alpha'].append(a)\n",
    "                model_results['Beta'].append(b)\n",
    "                model_results['Coherence'].append(cv)\n",
    "    results_df = pd.DataFrame(model_results)\n",
    "    results_df.to_csv('../data/processed/lda_tuning_results_' + title + '.csv', index=False)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46dad3f",
   "metadata": {},
   "source": [
    "### 3.5.1.1 Hyperparameter optimization LDA for tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e75413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "tweets_processed_lda = pickle.load(open(\"../data/processed/tweets_processed.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53919597",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameter optimization\n",
    "# Uncomment if you want to repeat the hyperparameter optimization\n",
    "# hyperparameter_lda_tweets = hyperparameter_lda(tweets_processed_lda, \"tweets\", topics_range, alpha, beta)\n",
    "# Takes approximately one hour of runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b145158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save hyperparameter\n",
    "# Uncomment if you want to repeat the hyperparameter optimization\n",
    "# hyperparameter_lda_tweets.to_csv('../data/processed/lda_tuning_results_tweets.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43eb7a9",
   "metadata": {},
   "source": [
    "### 3.5.1.2 Calculate best model LDA for tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc94540",
   "metadata": {},
   "source": [
    "Based on the hyperparameter optimisation from the last subsection, we compute the best LDA model for the tweets dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89498a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "lda_tuning_results_tweets = pd.read_csv('../data/processed/lda_tuning_results_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77449dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare corpus\n",
    "id2word_tweets_lda = corpora.Dictionary(tweets_processed_lda.text_preprocessed.to_list())\n",
    "id2word_tweets_lda.filter_extremes(no_below=5, no_above=0.1)\n",
    "texts_tweets_lda = tweets_processed_lda.text_preprocessed.to_list()\n",
    "corpus_tweets_lda = [id2word_tweets_lda.doc2bow(text) for text in texts_tweets_lda]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d099356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve optimal hyperparameter\n",
    "k_optimal_lda_tweets = int(lda_tuning_results_tweets.sort_values(\"Coherence\", ascending = False).reset_index(drop = True).Topics[0])\n",
    "try:\n",
    "    a_optimal_lda_tweets = float(lda_tuning_results_tweets.sort_values(\"Coherence\", ascending = False).reset_index(drop = True).Alpha[0])\n",
    "except ValueError:\n",
    "    a_optimal_lda_tweets = lda_tuning_results_tweets.sort_values(\"Coherence\", ascending = False).reset_index(drop = True).Alpha[0]\n",
    "try:\n",
    "    b_optimal_lda_tweets = float(lda_tuning_results_tweets.sort_values(\"Coherence\", ascending = False).reset_index(drop = True).Beta[0])\n",
    "except ValueError:\n",
    "    b_optimal_lda_tweets = lda_tuning_results_tweets.sort_values(\"Coherence\", ascending = False).reset_index(drop = True).Beta[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e42c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "lda_model_tweets = LdaMulticore(corpus=corpus_tweets_lda,\n",
    "                                 id2word=id2word_tweets_lda,\n",
    "                                 num_topics=k_optimal_lda_tweets,\n",
    "                                 random_state=42,\n",
    "                                 alpha=a_optimal_lda_tweets,\n",
    "                                 eta=b_optimal_lda_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec7b44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate final coherence value\n",
    "coherence_model_lda_tweets = CoherenceModel(model=lda_model_tweets, texts=texts_tweets_lda, dictionary=id2word_tweets_lda, coherence='c_v')\n",
    "coherence_lda_tweets = coherence_model_lda_tweets.get_coherence()\n",
    "print(\"The final model coherence of the LDA for Tweets is: \" + str(round(coherence_lda_tweets,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382af3ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visually inspect result\n",
    "lda_vis_tweets = pyLDAvis.gensim_models.prepare(lda_model_tweets, corpus_tweets_lda, id2word_tweets_lda)\n",
    "lda_vis_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd6efeb",
   "metadata": {},
   "source": [
    "We use the coherence value and topic visualisation to evalute the model. The model has an good coherence score, but the visual inspection shows topics, that are not easily interpretable. Based on this we cannot infer a high model quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6669ba98",
   "metadata": {},
   "source": [
    "### 3.5.1.3 Hyperparameter optimization LDA for speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5613cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "speeches_processed_lda = pickle.load(open( \"../data/processed/speeches_processed.p\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2242f186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter optimization\n",
    "# Uncomment if you want to repeat the hyperparameter optimization\n",
    "# hyperparameter_lda_speeches = hyperparameter_lda(speeches_processed_lda, \"tweets\", topics_range, alpha, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2a8157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save hyperparameter\n",
    "# Uncomment if you want to repeat the hyperparameter optimization\n",
    "# hyperparameter_lda_speeches.to_csv('../data/processed/lda_tuning_results_speeches.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ed9986",
   "metadata": {},
   "source": [
    "### 3.5.1.4 Calculate best model LDA for speeches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7078091f",
   "metadata": {},
   "source": [
    "Based on the hyperparameter optimisation from the last subsection, we compute the best LDA model for the speeches dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1365f4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "lda_tuning_results_speeches = pd.read_csv('../data/processed/lda_tuning_results_speeches.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b225c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare corpus\n",
    "id2word_speeches_lda = corpora.Dictionary(tweets_processed_lda.text_preprocessed.to_list())\n",
    "id2word_speeches_lda.filter_extremes(no_below=5, no_above=0.1)\n",
    "texts_speeches_lda = tweets_processed_lda.text_preprocessed.to_list()\n",
    "corpus_speeches_lda = [id2word_speeches_lda.doc2bow(text) for text in texts_speeches_lda]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f00973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve optimal hyperparameter\n",
    "k_optimal_lda_speeches = int(lda_tuning_results_speeches.sort_values(\"Coherence\", ascending = False).reset_index(drop = True).Topics[0])\n",
    "try:\n",
    "    a_optimal_lda_speeches = float(lda_tuning_results_speeches.sort_values(\"Coherence\", ascending = False).reset_index(drop = True).Alpha[0])\n",
    "except ValueError:\n",
    "    a_optimal_lda_speeches = lda_tuning_results_speeches.sort_values(\"Coherence\", ascending = False).reset_index(drop = True).Alpha[0]\n",
    "try:\n",
    "    b_optimal_lda_speeches = float(lda_tuning_results_speeches.sort_values(\"Coherence\", ascending = False).reset_index(drop = True).Beta[0])\n",
    "except ValueError:\n",
    "    b_optimal_lda_speeches = lda_tuning_results_speeches.sort_values(\"Coherence\", ascending = False).reset_index(drop = True).Beta[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2abf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "lda_model_speeches = LdaMulticore(corpus=corpus_speeches_lda,\n",
    "                                 id2word=id2word_speeches_lda,\n",
    "                                 num_topics=k_optimal_lda_speeches,\n",
    "                                 random_state=42,\n",
    "                                 alpha=a_optimal_lda_speeches,\n",
    "                                 eta=b_optimal_lda_speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b3f02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate final coherence value\n",
    "coherence_model_lda_speeches = CoherenceModel(model=lda_model_speeches, texts=texts_speeches_lda, dictionary=id2word_speeches_lda,\n",
    "                                                    coherence='c_v')\n",
    "coherence_lda_speeches = coherence_model_lda_speeches.get_coherence()\n",
    "print(\"The final model coherence of the LDA for Speeches is: \" + str(round(coherence_lda_speeches,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecef92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visually inspect result\n",
    "lda_vis_speeches = pyLDAvis.gensim_models.prepare(lda_model_speeches, corpus_speeches_lda, id2word_speeches_lda)\n",
    "lda_vis_speeches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a1c06a",
   "metadata": {},
   "source": [
    "Neither the coherence score or the visual inspection indicate a high model quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e334fd9e",
   "metadata": {},
   "source": [
    "## 3.5.2 Non Negative Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc7dae9",
   "metadata": {},
   "source": [
    "Another approach for topic modeling we are testing is Non Negative Matrix Factorization (NNMF). This technique is another unsupervised machine learning method, that factorizes a matrix into two matrices, that give a less complex representation of the the original matrix [citation](https://doi.org/10.1109/TKDE.2012.51). In our case we use it for creating a document-term matrix, that help to identify topics of the considered documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ae991d",
   "metadata": {},
   "source": [
    "#### Define hyperparameters for optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ea2f58",
   "metadata": {},
   "source": [
    "We optimize the hyperparameters of the NNMF model based on a grid search with the variables topic number (k)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32da66ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating coherence values of specific hyperparamter combinations\n",
    "def compute_nnmf_coherence_values(corpus, text, id2word, k):\n",
    "    nmf_model = Nmf(\n",
    "        corpus=corpus,\n",
    "        id2word=id2word,\n",
    "        num_topics=k,\n",
    "        random_state=42\n",
    "    )\n",
    "    coherence_model_lda = CoherenceModel(model=nmf_model, texts=text, dictionary=id2word, coherence='c_v')\n",
    "    return coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89004321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for executing a hyperparameter optimization\n",
    "def hyperparameter_nnmf(data_preprocessed, title, topics_range):\n",
    "    id2word = corpora.Dictionary(data_preprocessed.text_preprocessed.to_list())\n",
    "    id2word.filter_extremes(no_below=10, no_above=0.1)\n",
    "    texts = data_preprocessed.text_preprocessed.to_list()\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "    model_results = {'Topics': [],\n",
    "                     'Coherence': []\n",
    "                    }\n",
    "    grid = {}\n",
    "    grid['Validation_Set'] = {}\n",
    "    for k in tqdm(topics_range):\n",
    "        print(\"Number of topics:\" + str(k))\n",
    "        cv = compute_nnmf_coherence_values(corpus=corpus,text = texts,\n",
    "                                      id2word=id2word, k=k)\n",
    "        model_results['Topics'].append(k)\n",
    "        model_results['Coherence'].append(cv)\n",
    "    results_df = pd.DataFrame(model_results)\n",
    "    results_df.to_csv('../data/processed/nnmf_tuning_results_' + title + '.csv', index=False)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3298dae2",
   "metadata": {},
   "source": [
    "### 3.5.2.1 Hyperparameter optimization NNMF for tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7498a6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "tweets_processed_nnmf = pickle.load(open(\"../data/processed/tweets_processed.p\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07263d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameter optimization\n",
    "# Uncomment if you want to repeat the hyperparameter optimization\n",
    "# hyperparameter_nnmf_tweets = hyperparameter_nnmf(tweets_processed_nnmf, \"tweets\", topics_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9946946a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save hyperparameter\n",
    "# Uncomment if you want to repeat the hyperparameter optimization\n",
    "# hyperparameter_nnmf_tweets.to_csv('../data/processed/nnmf_tuning_results_tweets.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b997be",
   "metadata": {},
   "source": [
    "### 3.5.2.2 Calculate best model NNMF for tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409dbd54",
   "metadata": {},
   "source": [
    "Based on the hyperparameter optimisation from the last subsection, we compute the best NNMF model for the tweets dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd171c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "tweets_processed_nnmf = pickle.load(open( \"../data/processed/tweets_processed.p\", \"rb\" ))\n",
    "nnmf_tuning_results_tweets = pd.read_csv('../data/processed/nnmf_tuning_results_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6465e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare corpus\n",
    "id2word_tweets_nnmf = corpora.Dictionary(tweets_processed_nnmf.text_preprocessed.to_list())\n",
    "id2word_tweets_nnmf.filter_extremes(no_below=5, no_above=0.1)\n",
    "texts_tweets_nnmf = tweets_processed_nnmf.text_preprocessed.to_list()\n",
    "corpus_tweets_nnmf = [id2word_tweets_nnmf.doc2bow(text) for text in texts_tweets_nnmf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ce6382",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_optimal_nnmf_tweets = int(lda_tuning_results_tweets.sort_values(\"Coherence\", ascending = False).reset_index(drop = True).Topics[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8f496d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "nnmf_model_tweets = Nmf(corpus=corpus_tweets_nnmf,\n",
    "                                 id2word=id2word_tweets_nnmf,\n",
    "                                 num_topics=k_optimal_nnmf_tweets,\n",
    "                                 random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ac1d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate final coherence value\n",
    "coherence_model_nnmf_tweets = CoherenceModel(model=nnmf_model_tweets, texts=texts_tweets_nnmf, dictionary=id2word_tweets_nnmf,\n",
    "                                                    coherence='c_v')\n",
    "coherence_nnmf_tweets = coherence_model_nnmf_tweets.get_coherence()\n",
    "print(\"The final model coherence of the NNMF for tweets is: \" + str(round(coherence_nnmf_tweets,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f7ea03",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visually inspect result\n",
    "nnmf_model_tweets.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab080d38",
   "metadata": {},
   "source": [
    "When analysing the topic top words we cannot identify comprehensible subjects. Combined with the low coherence score we can conclude a low model quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0aa046e",
   "metadata": {},
   "source": [
    "### 3.5.2.3 Hyperparameter optimization NNMF for speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441635cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "speeches_processed_nnmf = pickle.load(open( \"../data/processed/speeches_processed.p\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35c396b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameter optimization\n",
    "# Uncomment if you want to repeat the hyperparameter optimization\n",
    "# hyperparameter_nnmf_speeches = hyperparameter_lda(speeches_processed_nnmf, \"tweets\", topics_range, alpha, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33e736d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save hyperparameter\n",
    "# Uncomment if you want to repeat the hyperparameter optimization\n",
    "# hyperparameter_nnmf_speeches.to_csv('../data/processed/nnmf_tuning_results_speeches.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ea283f",
   "metadata": {},
   "source": [
    "### 3.5.2.4 Calculate best model NNMF for speeches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ec8642",
   "metadata": {},
   "source": [
    "Based on the hyperparameter optimisation from the last subsection, we compute the best NNMF model for the speeches dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ed2857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "speeches_processed_nnmf = pickle.load(open( \"../data/processed/speeches_processed.p\", \"rb\" ))\n",
    "nnmf_tuning_results_speeches = pd.read_csv('../data/processed/nnmf_tuning_results_speeches.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a827d7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare corpus\n",
    "id2word_speeches_nnmf = corpora.Dictionary(tweets_processed_nnmf.text_preprocessed.to_list())\n",
    "id2word_speeches_nnmf.filter_extremes(no_below=5, no_above=0.1)\n",
    "texts_speeches_nnmf = tweets_processed_nnmf.text_preprocessed.to_list()\n",
    "corpus_speeches_nnmf = [id2word_speeches_nnmf.doc2bow(text) for text in texts_speeches_nnmf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0472b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_optimal_nnmf_speeches = int(lda_tuning_results_speeches.sort_values(\"Coherence\", ascending = False).reset_index(drop = True).Topics[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c177888d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "nnmf_model_speeches = Nmf(corpus=corpus_speeches_nnmf,\n",
    "                                 id2word=id2word_speeches_nnmf,\n",
    "                                 num_topics=k_optimal_nnmf_speeches,\n",
    "                                 random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f5c3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate final coherence value\n",
    "coherence_model_nnmf_speeches = CoherenceModel(model=nnmf_model_speeches, texts=texts_speeches_nnmf, dictionary=id2word_speeches_nnmf,\n",
    "                                                    coherence='c_v')\n",
    "coherence_nnmf_speeches = coherence_model_nnmf_speeches.get_coherence()\n",
    "print(\"The final model coherence of the NNMF for Speeches is: \" + str(round(coherence_nnmf_speeches,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e6c319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visually inspect result\n",
    "nnmf_model_speeches.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26999b3",
   "metadata": {},
   "source": [
    "The coherence of the model is rather low and also the resulting topics show no consistent them resulting in a low model usability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21687dc9",
   "metadata": {},
   "source": [
    "## 3.5.3 Bertopic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db4568e",
   "metadata": {},
   "source": [
    "The last model we apply is [BERTopic](https://doi.org/10.5281/zenodo.4381785), which employs BERT transformers model for creating topic models. BERTopic uses pretrained BERT models and UMAP and HDBSCAN clustering with an c-TF-IDF embedding and Maximal Marginal Relevance selection. This model architecture is pretty new and there is not much existing research on the topic. However first results seem promising. Based on the architecutre the model is able to identify relevant topics in the text and cluster them according to semantic similarity. The model architecture is quite complex and therefore the runtime of training BERTopic is high. We do not perform hyperparameter optimization for the BERTopic models, as we are having only limited computational power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a749b03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_coherence_bert(topic_model, docs, topics):\n",
    "    cleaned_docs = topic_model._preprocess_text(docs)\n",
    "\n",
    "    # Extract vectorizer and tokenizer from BERTopic\n",
    "    vectorizer = topic_model.vectorizer_model\n",
    "    tokenizer = vectorizer.build_tokenizer()\n",
    "\n",
    "    # Extract features for Topic Coherence evaluation\n",
    "    words = vectorizer.get_feature_names()\n",
    "    tokens = [tokenizer(doc) for doc in cleaned_docs]\n",
    "    dictionary = corpora.Dictionary(tokens)\n",
    "    corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "    topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "                   for topic in range(len(set(topics))-1)]\n",
    "\n",
    "    # Evaluate\n",
    "    coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                     texts=tokens, \n",
    "                                     corpus=corpus,\n",
    "                                     dictionary=dictionary, \n",
    "                                     coherence='c_v')\n",
    "    coherence = coherence_model.get_coherence()\n",
    "    return coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f608d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_topic(topic_id, topic_model):\n",
    "    return topic_model.get_topic_info(topic_id).Name.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ab63a8",
   "metadata": {},
   "source": [
    "### 3.5.3.1 Compute BERTopic model Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ab1719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "tweets_processed_bert = pickle.load(open( \"../data/processed/tweets_processed.p\", \"rb\" ))\n",
    "docs_tweets_bert = tweets_processed_bert.text_preprocessed_sentence.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77350230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare topic model \n",
    "topic_model_tweets = BERTopic(language=\"german\", nr_topics=\"auto\", calculate_probabilities = True, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768d32bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Bertopic model\n",
    "# Uncomment if you want to retrain the network\n",
    "# start_time_bert_tweets = datetime.now()\n",
    "# topics_tweets_bert, probs_tweets_bert = topic_model_tweets.fit_transform(docs_tweets_bert)\n",
    "# end_time_bert_tweets = datetime.now()\n",
    "# print('Duration: {}'.format(end_time_bert_tweets - start_time_bert_tweets))\n",
    "# Takes approximately eight hours of runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884136d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate coherence\n",
    "# Uncomment if you want to retrain the network\n",
    "# coherence_bert_tweets = calculate_coherence_bert(topic_model_tweets,docs_tweets_bert, topics_tweets_bert)\n",
    "# coherence_bert_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445b7817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise results\n",
    "# Uncomment if you want to retrain the network\n",
    "# topic_model_tweets.visualize_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312b26cb",
   "metadata": {},
   "source": [
    "Based on first analyses we saw that there are too many topics, so we reduce the number of topics with the inherent reduction logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26be5a62",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Reduce topics\n",
    "# Uncomment if you want to retrain the network\n",
    "# topics_tweets_bert_reduced, probs_tweets_bert_reduced = topic_model_tweets.reduce_topics(docs_tweets_bert,\n",
    "#                                                                                         topics_tweets_bert,\n",
    "#                                                                                         probs_tweets_bert,\n",
    "#                                                                                         nr_topics=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21534d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "# Comment out if you retrain the model\n",
    "with open('../data/processed/topics_tweets_bert.pickle', 'rb') as handle:\n",
    "    topics_tweets_bert_reduced = pickle.load(handle)\n",
    "topic_model_tweets = BERTopic.load(\"../models/bertopic_tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695bd952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate coherence reduced\n",
    "coherence_bert_tweets_reduced = calculate_coherence_bert(topic_model_tweets, docs_tweets_bert, \n",
    "                                                         topics_tweets_bert_reduced)\n",
    "print(\"The final model coherence of the BERTopic for Tweets is: \" + str(round(coherence_bert_tweets_reduced,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8602907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise results\n",
    "topic_model_tweets.visualize_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ea603e",
   "metadata": {},
   "source": [
    "The coherence of the model is on a satisfactory level and the identified topics are interpretable for human observer. We can infer a high model quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0ffac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign results to dataframe\n",
    "tweets_processed_bert[\"topic_id\"] = topics_tweets_bert_reduced\n",
    "tweets_processed_bert[\"topic\"] = tweets_processed_bert.topic_id.progress_apply(assign_topic,                                                                                    topic_model = topic_model_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddf7280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and results\n",
    "# Uncomment if you want to retrain the network\n",
    "# topic_model_tweets.save(\"../models/bertopic_tweets\")\n",
    "# with open( \"../data/processed/tweets_processed_bert.pickle\", \"wb\" ) as handle:\n",
    "#    pickle.dump(tweets_processed_bert, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with open('../data/processed/probabilities_tweets_bert.pickle', 'wb') as handle:\n",
    "#     pickle.dump(probs_tweets_bert_reduced, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with open('../data/processed/topics_tweets_bert.pickle', 'wb') as handle:\n",
    "#    pickle.dump(topics_tweets_bert_reduced, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee66b2c",
   "metadata": {},
   "source": [
    "### 3.5.3.2 Compute BERTopic model Speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0514a763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "speeches_processed_bert = pickle.load(open( \"../data/processed/speeches_processed.p\", \"rb\" ))\n",
    "docs_speeches_bert = speeches_processed_bert.text_preprocessed_infrequent_sentence.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba79ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare topic model \n",
    "topic_model_speeches = BERTopic(language=\"german\", nr_topics=\"auto\", calculate_probabilities = True, \n",
    "                                verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54a43aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Bertopic mode\n",
    "# Uncomment if you want to retrain the network\n",
    "# start_time_bert_speeches = datetime.now()\n",
    "# topics_speeches_bert, probs_speeches_bert = topic_model_speeches.fit_transform(docs_speeches_bert)\n",
    "# end_time_bert_speeches = datetime.now()\n",
    "# print('Duration: {}'.format(end_time_bert_speeches - start_time_bert_speeches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767ad27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "# Comment out if you retrain the model\n",
    "with open('../data/processed/topics_speeches_bert.pickle', 'rb') as handle:\n",
    "    topics_speeches_bert = pickle.load(handle)\n",
    "topic_model_speeches = BERTopic.load(\"../models/bertopic_speeches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16ae033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate coherence reduced\n",
    "coherence_bert_speeches = calculate_coherence_bert(topic_model_speeches, docs_speeches_bert, \n",
    "                                                         topics_speeches_bert)\n",
    "print(\"The final model coherence of the BERTopic for speeches is: \" + str(round(coherence_bert_speeches,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da67cfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise results\n",
    "# Uncomment if you want to retrain the network\n",
    "topic_model_speeches.visualize_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6c5b05",
   "metadata": {},
   "source": [
    "The model has a comperatively high coherence and also interpretable topics. Therefore we conclude a high model quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2e3676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign results to dataframe\n",
    "# Uncomment if you want to retrain the network\n",
    "# speeches_processed_bert[\"topic_id\"] = topics_speeches_bert\n",
    "# speeches_processed_bert[\"topic\"] = speeches_processed_bert.topic_id.progress_apply(assign_topic, \n",
    "#                                                                                   topic_model = topic_model_speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1515c5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and results\n",
    "# Uncomment if you want to retrain the network\n",
    "# topic_model_speeches.save(\"../models/bertopic_speeches\")\n",
    "# with open( \"../data/processed/speeches_processed_bert.pickle\", \"wb\" ) as handle:\n",
    "#    pickle.dump(speeches_processed_bert, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with open('../data/processed/probabilities_speeches_bert.pickle', 'wb') as handle:\n",
    "#    pickle.dump(probs_speeches_bert, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with open('../data/processed/topics_speeches_bert.pickle', 'wb') as handle:\n",
    "#    pickle.dump(topics_speeches_bert, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a64fc4",
   "metadata": {},
   "source": [
    "## 3.5.4 Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce6cd1b",
   "metadata": {},
   "source": [
    "For the final model selection we evaluate the model based on the coherence and the visual inspection of the model results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c0d8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The final model coherence of the LDA for tweets is: \" + str(round(coherence_lda_tweets,2)))\n",
    "print(\"The final model coherence of the NNMF for tweets is: \" + str(round(coherence_nnmf_tweets,2)))\n",
    "print(\"The final model coherence of the BERTopic for tweets is: \" + str(round(coherence_bert_tweets_reduced,2)))\n",
    "print(\"The final model coherence of the LDA for speeches is: \" + str(round(coherence_lda_speeches,2)))\n",
    "print(\"The final model coherence of the NNMF for speeches is: \" + str(round(coherence_nnmf_speeches,2)))\n",
    "print(\"The final model coherence of the BERTopic for speeches is: \" + str(round(coherence_bert_speeches,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2c4c9d",
   "metadata": {},
   "source": [
    " NNMF models did not perform very well in terms of coherence, while the LDA model only showed good coherence values for the tweets dataset. BERTopic could perform well for both datasets in term of cohersion. Based on the visual inspection we saw very good results for BERTopic and medium results for the other two model types. Based on these criteria we decide for the BERTopic model for both dataset to create the final topic model. In the next section we will analyse the results of BERTopic and validate the the selected models based on word and topic intrusion metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fc9ed7",
   "metadata": {},
   "source": [
    "# 4 Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9272db",
   "metadata": {},
   "source": [
    "# 4.1 Topic modelling results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998afecc",
   "metadata": {},
   "source": [
    "Based on the model selection and creation in section 3.5 we will now analyse the results to anwser our first three research questions:\n",
    "\n",
    "* **RQ 1.1** What are the main topics of tweets of prominent politicians of the six parties in the German Parlament in the period of the 19th Bundestag?\n",
    "\n",
    "* **RQ 1.2** What are the main topics of speeches of prominent politicians of the six parties in the German Parlament in the period of the 19th Bundestag?\n",
    "\n",
    "+ **RQ 1.3** How do the main topics of tweets and speeches of prominent politicians of the six parties in the German Parlament differ in the period of the 19th Bundestag?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e023869a",
   "metadata": {},
   "source": [
    "For this we visualise the results and deep dive into several topics. We cannot do an exhaustive interpretation of all topics of our models, as this would be out of scope for this work. We still provide the code for an exhaustive analysis, so that the interested reader can execute the analysis on his own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46698f5a",
   "metadata": {},
   "source": [
    "## 4.1.1 Analyse tweets model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfee913",
   "metadata": {},
   "source": [
    "To answer the first research question, we use the trained BERTopic model for tweets from the last section. We load the pretrained model and the resulting data. If the model is retrained, one can skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee677c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "tweets_processed_bert = pickle.load(open( \"../data/processed/tweets_processed_bert.pickle\", \"rb\" ))\n",
    "docs_tweets = tweets_processed_bert.text_preprocessed_sentence.tolist()\n",
    "with open('../data/processed/probabilities_tweets_bert.pickle', 'rb') as handle:\n",
    "    probs_tweets = pickle.load(handle)\n",
    "with open('../data/processed/topics_tweets_bert.pickle', 'rb') as handle:\n",
    "    topics_tweets = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba23168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "topic_model_tweets = BERTopic.load(\"../models/bertopic_tweets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23d26b6",
   "metadata": {},
   "source": [
    "### 4.1.1.1 Overview of topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c757fda",
   "metadata": {},
   "source": [
    "We reduced the model to 100 topics which increased our coherence, but also create one large not very expressive topic, that gives no insights. To avoid this problem we would need to do hyperparamter optimization with the amount of topics and the preprocessing which is out of scope based on computational restrictions. We will now focus on topics that can be interpreted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041a7188",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model_tweets.get_topic_info().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80db2e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model_tweets.get_topic_info().tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7018c673",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topic_model_tweets.visualize_barchart(topics=None, top_n_topics=25, n_words=5, width=250, height=250) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c277dcb6",
   "metadata": {},
   "source": [
    "Even though we already identified some weakness in the model, we can see that there are many interesting topics, that we can use for further analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496b5e6f",
   "metadata": {},
   "source": [
    "### 4.1.1.2 Visualise topic correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1232a38c",
   "metadata": {},
   "source": [
    "Another important quality indicator is the similarity of the topics. If we have very similar topics, they will not be very selective and can lead to skewed topic distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7949d673",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model_tweets.visualize_heatmap(top_n_topics=100, width=800, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13071580",
   "metadata": {},
   "source": [
    "We can see strong similarity between some topics. Interestingly these are topics that seem to be not well defined and do not show high inner topic coherence form a human perspective. With a more sophisticated preprocessing and hyperparameter optimization we should be able to handle this problem. We will focus on topics, that are not highly correlated with other topics and seems to have a coherence meaning in our further analysis to avoid distorted topics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a141d8cf",
   "metadata": {},
   "source": [
    "### 4.1.1.3 Visualise topic hierachy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bfc4d2",
   "metadata": {},
   "source": [
    "To analyse the topic cluster of the resulting BERTopic model we will use the inherent clustering of the model. We use the inherent clustering to identify significant cluster, that we analyse in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a683ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise topic hierarchy\n",
    "topic_model_tweets.visualize_hierarchy(top_n_topics=100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83297707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise topic distance map\n",
    "topic_model_tweets.visualize_topics(top_n_topics=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99969aab",
   "metadata": {},
   "source": [
    "Based on the clustering and our evaluation we identified twelve larger topic clusters. We will analyse three of the cluster in detail, while the other clusters are shortly described and code for deeper analysis is provided. We only selected clusters, that contains topics with political and societal relevance. This limitation exclude topics that only comprise interhuman relationship building and smalltalk. There are a lot more topics and cluster, that we could cover, but this is out of scope for this work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332abfb1",
   "metadata": {},
   "source": [
    "### 4.1.1.4 Analyse topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f293a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare time based visualisation\n",
    "tweets_topics_over_time = topic_model_tweets.topics_over_time(docs_tweets, topics_tweets, \n",
    "                                                       pd.to_datetime(tweets_processed_bert.date).dt.strftime('%Y-%m'), \n",
    "                                                       nr_bins=None, datetime_format=None, evolution_tuning=True,\n",
    "                                                       global_tuning=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9eb38b",
   "metadata": {},
   "source": [
    "#### 4.1.1.4.1 Cluster migration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300b7b1a",
   "metadata": {},
   "source": [
    "The first cluster covering migration contains the topics 15, 19, 53 and 59. It includes the subjects migration, asyl, refugees and family reunion. We deep dive into the analysis of the topic, to get a better understanding of subject area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a3d9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cluster\n",
    "cluster_1_migration = [15, 19, 53, 59]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277dc92e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualise topic hierarchy\n",
    "topic_model_tweets.visualize_hierarchy(top_n_topics=100, topics = cluster_1_migration) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed3d80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse the cluster over time\n",
    "topic_model_tweets.visualize_topics_over_time(tweets_topics_over_time, topics=cluster_1_migration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf817e3",
   "metadata": {},
   "source": [
    "The frequency of tweets concerning the topics migration and asylum peak around the second half of the year 2018 and from then one they are decreasing. One can correlate this peak with the discussion about the [global compact for migration](https://refugeesmigrants.un.org/migration-compact) from the United Nations and other debates about mmigration and asylum in this time span."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbeebaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the party distribution of the cluster\n",
    "tweets_cluster_1_migration =  tweets_processed_bert[tweets_processed_bert.topic_id.isin(cluster_1_migration)]\n",
    "print(tweets_cluster_1_migration.groupby(\"party\").size().sort_values(ascending = False))\n",
    "print(\"\\n\")\n",
    "print(tweets_processed_bert.groupby(\"party\").size().sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c14d493",
   "metadata": {},
   "source": [
    "ADF tweets significantly more about the topic migration asylum compared to the other parties controlling their general tweet frequency. The remaining distribution of tweets seems to be proportional to the amount of tweets of the parties in general. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c78bc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the most prominent politicians of the cluster\n",
    "tweets_cluster_1_migration.groupby(\"full_name\").size().sort_values(ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cc8901",
   "metadata": {},
   "source": [
    "The distribution of the politicians seems to correlate with the identified distribution of the parties. An interesting next step could be to investigate the sentiment of the different politicians and parties for the topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c334a8",
   "metadata": {},
   "source": [
    "#### 4.1.1.4.2 Cluster media"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5254f3f",
   "metadata": {},
   "source": [
    "The topics 0, 44, 78, 79, 88 and 98 from the cluster media. The cluster comprises subjects as social media, press and other communication media."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9e82ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_2_media = [0, 44, 78, 79, 88, 98]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d31d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse the cluster over time\n",
    "# Uncomment if you want to analyse the cluster\n",
    "# topic_model_tweets.visualize_topics_over_time(tweets_topics_over_time, topics=cluster_2_media)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1662daff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the party distribution of the cluster\n",
    "# Uncomment if you want to analyse the cluster\n",
    "# tweets_cluster_2_media =  tweets_processed_bert[tweets_processed_bert.topic_id.isin(cluster_2_media)]\n",
    "# print(tweets_cluster_2_media.groupby(\"party\").size().sort_values(ascending = False))\n",
    "# print(\"\\n\")\n",
    "# print(tweets_processed_bert.groupby(\"party\").size().sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e86e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the most prominent politicians of the cluster\n",
    "# Uncomment if you want to analyse the cluster\n",
    "# tweets_cluster_2_media.groupby(\"full_name\").size().sort_values(ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307c81ab",
   "metadata": {},
   "source": [
    "#### 4.1.1.4.3 Cluster extremism and religion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a2f20d",
   "metadata": {},
   "source": [
    "The next cluster comprises the topics 36, 47, 73, 76 which deal with the subjects extremism and religion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa7d817",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_3_extremism_religion  = [36, 47, 73, 76]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7310e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse the cluster over time\n",
    "# Uncomment if you want to analyse the clruster\n",
    "# topic_model_tweets.visualize_topics_over_time(tweets_topics_over_time, topics=cluster_3_extremism_religion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dae86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the party distribution of the cluster\n",
    "# Uncomment if you want to analyse the cluster\n",
    "# tweets_cluster_3_extremism_religion =  tweets_processed_bert[tweets_processed_bert.topic_id.isin(cluster_3_extremism_religion)]\n",
    "# print(tweets_cluster_3_extremism_religion.groupby(\"party\").size().sort_values(ascending = False))\n",
    "# print(\"\\n\")\n",
    "# print(tweets_processed_bert.groupby(\"party\").size().sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7738a13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the most prominent politicians of the cluster\n",
    "# Uncomment if you want to analyse the cluster\n",
    "# tweets_cluster_3_extremism_religion.groupby(\"full_name\").size().sort_values(ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49ade57",
   "metadata": {},
   "source": [
    "#### 4.1.1.4.4 Cluster foreign politics and armed conflicts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a91bd9",
   "metadata": {},
   "source": [
    "The fourth cluster combines the topics 22, 32, 41, 56, 89 and 93. Main issues of this cluster are armed conflicts and defense topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244a39d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_4_foreign_politics_armed_conflicts = [22, 32, 41, 56, 89, 93]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4588ff12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse the cluster over time\n",
    "# Uncomment if you want to analyse the cluster\n",
    "# topic_model_tweets.visualize_topics_over_time(tweets_topics_over_time, topics=cluster_4_foreign_politics_armed_conflicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc134ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the party distribution of the cluster\n",
    "# Uncomment if you want to analyse the cluster\n",
    "# tweets_cluster_4_foreign_politics_armed_conflicts =  tweets_processed_bert[tweets_processed_bert.topic_id.isin(cluster_4_foreign_politics_armed_conflicts)]\n",
    "# print(tweets_cluster_4_foreign_politics_armed_conflicts.groupby(\"party\").size().sort_values(ascending = False))\n",
    "# print(\"\\n\")\n",
    "# print(tweets_processed_bert.groupby(\"party\").size().sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c55248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the most prominent politicians of the cluster\n",
    "# Uncomment if you want to analyse the cluster\n",
    "# tweets_cluster_4_foreign_politics_armed_conflicts.groupby(\"full_name\").size().sort_values(ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b530919",
   "metadata": {},
   "source": [
    "#### 4.1.1.4.5 Cluster discrimination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77355016",
   "metadata": {},
   "source": [
    "Another prominent topic area is discrimination and racism, that we combined in the fifth cluster with the topics 13, 23, 37, 40 and 72."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95eb92a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_5_discrimination = [13, 23, 37, 40, 72]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7ac6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse the cluster over time\n",
    "# Uncomment if you want to analyse the cluster\n",
    "# topic_model_tweets.visualize_topics_over_time(tweets_topics_over_time, topics=cluster_5_discrimination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae74da74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the party distribution of the cluster\n",
    "# Uncomment if you want to analyse the cluster\n",
    "# tweets_cluster_5_discrimination =  tweets_processed_bert[tweets_processed_bert.topic_id.isin(cluster_5_discrimination)]\n",
    "# print(tweets_cluster_5_discrimination.groupby(\"party\").size().sort_values(ascending = False))\n",
    "# print(\"\\n\")\n",
    "# print(tweets_processed_bert.groupby(\"party\").size().sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e84456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the most prominent politicians of the cluster\n",
    "# Uncomment if you want to analyse the cluster\n",
    "# tweets_cluster_5_discrimination.groupby(\"full_name\").size().sort_values(ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa51f8f",
   "metadata": {},
   "source": [
    "#### 4.1.1.4.6 Cluster Covid-19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b45300",
   "metadata": {},
   "source": [
    "The Covid-19 cluster comprises the topics 8, 9, 29, 54, 65, 71 and 90 and comprises all topics around the global pandemy. We analyse this cluster in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e85feed",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_6_covid = [8, 9, 29, 54, 65, 71, 90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a3b7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse the cluster over time\n",
    "topic_model_tweets.visualize_topics_over_time(tweets_topics_over_time, topics=cluster_6_covid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b481cb7",
   "metadata": {},
   "source": [
    "The time series of the cluster can be easily related to the developmeent of the worldwide pandemic situation. We have an higher frequency of tweets in time of high numbers of infections and restrictions and less tweets in summer when the situation is more relaxed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd6c527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the party distribution of the cluster\n",
    "tweets_cluster_6_covid =  tweets_processed_bert[tweets_processed_bert.topic_id.isin(cluster_6_covid)]\n",
    "print(tweets_cluster_6_covid.groupby(\"party\").size().sort_values(ascending = False))\n",
    "print(\"\\n\")\n",
    "print(tweets_processed_bert.groupby(\"party\").size().sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414c14c4",
   "metadata": {},
   "source": [
    "SPD has an mucher higher number of tweets compared to the other parties concerning this cluster. This can be explained by the amount of tweets of the prominent SPD politican Karl Lauterbach as we can see in the next code cell. It could be interesting to go into a deeper analysis of his tweets, television and other media appearances to better understand his political career."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e37e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the most prominent politicians of the cluster\n",
    "tweets_cluster_6_covid.groupby(\"full_name\").size().sort_values(ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8506834f",
   "metadata": {},
   "source": [
    "#### 4.1.1.4.7 Cluster democratic structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30d7df6",
   "metadata": {},
   "source": [
    "The topics in cluster 16, 24, 27, 34, 38 and 74 seem to focus on general parlamentary and democratic structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df82b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_7_democratic_structure = [16, 24, 27, 34, 38, 74]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72c44ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse the cluster over time\n",
    "# Uncomment if you want to analyse the cluster\n",
    "# topic_model_tweets.visualize_topics_over_time(tweets_topics_over_time, topics=cluster_7_democratic_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f88100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the party distribution of the cluster\n",
    "# Uncomment if you want to analyse the cluster\n",
    "# tweets_cluster_7_democratic_structure =  tweets_processed_bert[tweets_processed_bert.topic_id.isin(cluster_7_democratic_structure)]\n",
    "# print(tweets_cluster_7_democratic_structure.groupby(\"party\").size().sort_values(ascending = False))\n",
    "# print(\"\\n\")\n",
    "# print(tweets_processed_bert.groupby(\"party\").size().sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5176f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the most prominent politicians of the cluster\n",
    "# Uncomment if you want to analyse the cluster\n",
    "# tweets_cluster_7_democratic_structure.groupby(\"full_name\").size().sort_values(ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2d51e9",
   "metadata": {},
   "source": [
    "#### 4.1.1.4.8 Cluster Germany and EU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61385cd",
   "metadata": {},
   "source": [
    "Cluster 8 comprises topics 5, 6, 35, 51 and 70 which focus on europe, the EU and Germany."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8465f982",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_8_germany_in_europe = [5, 6, 35, 51, 70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88660ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse the cluster over time\n",
    "# Uncomment if you want to analyse the cluster\n",
    "# topic_model_tweets.visualize_topics_over_time(tweets_topics_over_time, topics=cluster_8_germany_in_europe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a585d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the party distribution of the cluster\n",
    "# Uncomment if you want to analyse the cluster\n",
    "# tweets_cluster_8_germany_in_europe =  tweets_processed_bert[tweets_processed_bert.topic_id.isin(cluster_8_germany_in_europe)]\n",
    "# print(tweets_cluster_8_germany_in_europe.groupby(\"party\").size().sort_values(ascending = False))\n",
    "# print(\"\\n\")\n",
    "# print(tweets_processed_bert.groupby(\"party\").size().sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6cd459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the most prominent politicians of the cluster\n",
    "# Uncomment if you want to analyse the cluster\n",
    "# tweets_cluster_8_germany_in_europe.groupby(\"full_name\").size().sort_values(ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61e65ae",
   "metadata": {},
   "source": [
    "#### 4.1.1.4.9 Cluster finance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf7852e",
   "metadata": {},
   "source": [
    "Another cluster consisting of topics 14, 39, 67, 91 that cover topic around finance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2739bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_9_finance = [14, 39, 67, 91]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c8f89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse the cluster over time\n",
    "# Uncomment if you want to analyse the cluster\n",
    "# topic_model_tweets.visualize_topics_over_time(tweets_topics_over_time, topics=cluster_9_finance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6573908c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the party distribution of the cluster\n",
    "# Uncomment if you want to analyse the cluster\n",
    "# tweets_cluster_9_finance =  tweets_processed_bert[tweets_processed_bert.topic_id.isin(cluster_9_finance)]\n",
    "# print(tweets_cluster_9_finance.groupby(\"party\").size().sort_values(ascending = False))\n",
    "# print(\"\\n\")\n",
    "# print(tweets_processed_bert.groupby(\"party\").size().sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5108b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the most prominent politicians of the cluster\n",
    "# Uncomment if you want to analyse the cluster\n",
    "# tweets_cluster_9_finance.groupby(\"full_name\").size().sort_values(ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbf0fb5",
   "metadata": {},
   "source": [
    "#### 4.1.1.4.10 Cluster police and safety"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8832e56",
   "metadata": {},
   "source": [
    "The cluster police and safety comprises three the topics  7, 83 and 93 and covers the issues police and safety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b5169a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_10_police_safety = [7, 83, 93]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19453d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse the cluster over time\n",
    "# Uncomment if you want to analyse the cluster\n",
    "# topic_model_tweets.visualize_topics_over_time(tweets_topics_over_time, topics=cluster_10_police_safety)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da562722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the party distribution of the cluster\n",
    "# Uncomment if you want to analyse the cluster\n",
    "# tweets_cluster_10_police_safety =  tweets_processed_bert[tweets_processed_bert.topic_id.isin(cluster_10_police_safety)]\n",
    "# print(tweets_cluster_10_police_safety.groupby(\"party\").size().sort_values(ascending = False))\n",
    "# print(\"\\n\")\n",
    "# print(tweets_processed_bert.groupby(\"party\").size().sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1d66d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the most prominent politicians of the cluster\n",
    "# Uncomment if you want to analyse the cluster\n",
    "# tweets_cluster_10_police_safety.groupby(\"full_name\").size().sort_values(ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f36fa7",
   "metadata": {},
   "source": [
    "#### 4.1.1.4.11 Cluster climate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0928ade5",
   "metadata": {},
   "source": [
    "Another cluster of interest consists of the topics 12 and 99 and covers the area climate and nature. We will analyse the area in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7746eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_11_climate = [12, 99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e775bf0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Analyse the cluster over time\n",
    "# Uncomment if you want to analyse the cluster\n",
    "topic_model_tweets.visualize_topics_over_time(tweets_topics_over_time, topics=cluster_11_climate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ce74f1",
   "metadata": {},
   "source": [
    "An interesting trend we can observe in the data is that there was an sharp increasing frequency of tweets until the beginning of the Covid-19 pandemy. After the beginning of the pandemy, the topic lost importance in the tweet behaviour of the politicians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b220e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the party distribution of the cluster\n",
    "# Uncomment if you want to analyse the cluster\n",
    "tweets_cluster_11_climate =  tweets_processed_bert[tweets_processed_bert.topic_id.isin(cluster_11_climate)]\n",
    "print(tweets_cluster_11_climate.groupby(\"party\").size().sort_values(ascending = False))\n",
    "print(\"\\n\")\n",
    "print(tweets_processed_bert.groupby(\"party\").size().sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48738858",
   "metadata": {},
   "source": [
    "The party Die Grünen has the highest  frequence of tweets concerning enviormental topics. This is in line with the political agenda of the party."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157b3dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the most prominent politicians of the cluster\n",
    "# Uncomment if you want to analyse the cluster\n",
    "tweets_cluster_11_climate.groupby(\"full_name\").size().sort_values(ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69be0d3",
   "metadata": {},
   "source": [
    "When analysing the list of the politicians that tweets about this with a high frequency we can see a lot of politicians of the party Die Grünen and other politicians that generally tweets with a high frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a0f38c",
   "metadata": {},
   "source": [
    "#### 4.1.1.4.12 Cluster infrastructure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cee77b7",
   "metadata": {},
   "source": [
    "The last cluster containing topics 1, 18 and 87 covers digital and anlog infrastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2bd59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster_12_infrastructure = [18, 87, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6afec3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse the cluster over time\n",
    "# Uncomment if you want to analyse the cluster\n",
    "# topic_model_tweets.visualize_topics_over_time(tweets_topics_over_time, topics=cluster_12_infrastructure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b733aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the party distribution of the cluster\n",
    "# Uncomment if you want to analyse the cluster\n",
    "# tweets_cluster_12_infrastructure =  tweets_processed_bert[tweets_processed_bert.topic_id.isin(cluster_12_infrastructure)]\n",
    "# print(tweets_cluster_12_infrastructure.groupby(\"party\").size().sort_values(ascending = False))\n",
    "# print(\"\\n\")\n",
    "# print(tweets_processed_bert.groupby(\"party\").size().sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232fefe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the most prominent politicians of the cluster\n",
    "# Uncomment if you want to analyse the cluster\n",
    "# tweets_cluster_12_infrastructure.groupby(\"full_name\").size().sort_values(ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7037ad84",
   "metadata": {},
   "source": [
    "### 4.1.1.5 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f392c5",
   "metadata": {},
   "source": [
    "In this section we summarize the results concerning the intial research question:\n",
    "\n",
    "**What are the main topics of tweets of prominent politicians of the six parties in the German Parlament in the period of the 19th Bundestag?**\n",
    "\n",
    "We trained a BERTopic model to give us an overview the topics, that are presented in the sections 4.1.1.1 - 4.1.1.3. Based on the identified topics and the inherent modelling clustering we defined 12 overarching cluster of subjects that are presented in section 4.1.1.4. The cluster of the topics, could now we be used to for further analysis. To answers the research question, we identified the following main topics of tweets of the selected politicians:\n",
    "\n",
    "* Migration\n",
    "* Media\n",
    "* Extremism and feligion\n",
    "* Foreign politics and armed conflicts\n",
    "* Discrimination\n",
    "* Covid-19\n",
    "* Democratic structures\n",
    "* Europe, EU and Germany\n",
    "* Finance\n",
    "* Police and safety\n",
    "* Climate\n",
    "* Infrastructure\n",
    "\n",
    "We did not include topics, that were results of interhuman relationship building or smalltalk. We did a deep dive in the clusters migration, Covid-19 and enviroment. The code for deeper analysis of the other clusters is provided and can be used by the interested reader. Based on this analysis we will compare the results with the topics of the speeches in the parlaments in section 4.1.3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ede237f",
   "metadata": {},
   "source": [
    "## 4.1.2 Analyse speeches model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4fc92c",
   "metadata": {},
   "source": [
    "To answer the second research question, we proceed the same as in the process of answering the first research question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa9d668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "speeches_processed_bert = pickle.load(open( \"../data/processed/speeches_processed_bert.pickle\", \"rb\" ))\n",
    "docs_speeches = speeches_processed_bert.text_preprocessed_sentence.tolist()\n",
    "with open('../data/processed/probabilities_speeches_bert.pickle', 'rb') as handle:\n",
    "    probs_speeches = pickle.load(handle)\n",
    "with open('../data/processed/topics_speeches_bert.pickle', 'rb') as handle:\n",
    "    topics_speeches = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0034a00",
   "metadata": {},
   "source": [
    "### 4.1.2.1 Overview of topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71e0c37",
   "metadata": {},
   "source": [
    "We already saw in the modelling section, that we identified less topics for the speeches dataset. This effect correspond to the significantly fewer number of documents in the dataset. We identified 25 topics in the modelling stage, that we now analyse in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a30cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "topic_model_speeches = BERTopic.load(\"../models/bertopic_speeches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7039d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show topic infos\n",
    "topic_model_speeches.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cef3b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topic_model_speeches.visualize_barchart(topics=None, top_n_topics=25, n_words=5, width=250, height=250) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c835133",
   "metadata": {},
   "source": [
    "### 4.1.2.2 Visualise topic correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1237228e",
   "metadata": {},
   "source": [
    "To get an better understanding of the quality of our topic model, we analyse the similarity of the identified topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e45d82",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualise correlation\n",
    "topic_model_speeches.visualize_heatmap(top_n_topics=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa59d065",
   "metadata": {},
   "source": [
    "The first two topics have similaritys score with various other topics. This could skew our results and has to be minded when interpreting the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c56886",
   "metadata": {},
   "source": [
    "### 4.1.2.3 Visualise topic hierachy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d2bb8c",
   "metadata": {},
   "source": [
    "To analyse the topic cluster of the resulting BERTopic model we will use the inherent clustering of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275bb59a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualise clustering\n",
    "topic_model_speeches.visualize_hierarchy(orientation='left', top_n_topics=25, width=1000, height=600) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d7cbb1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualise topic distance\n",
    "topic_model_speeches.visualize_topics(topics=None, top_n_topics=None, width=650, height=650)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8f25ba",
   "metadata": {},
   "source": [
    "### 4.1.2.4 Analyse topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d98d27",
   "metadata": {},
   "source": [
    "We identified 12 cluster and topics that we now analyse in more detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e69d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare time based visualisation\n",
    "speeches_topics_over_time = topic_model_speeches.topics_over_time(docs_speeches, topics_speeches, pd.to_datetime(speeches_processed_bert.date).dt.strftime('%Y-%m'), \n",
    "                                                       nr_bins=None, datetime_format=None, evolution_tuning=True,\n",
    "                                                       global_tuning=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6266a89d",
   "metadata": {},
   "source": [
    "#### 4.1.2.4.1 Cluster europe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05744bb6",
   "metadata": {},
   "source": [
    "The first cluster based on topic 21 and 23 deal with the topic europe and EU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff0e38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_1_europe = [21, 23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09eb6bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse the cluster over time\n",
    "# Uncomment if you want to analyse the cluster\n",
    "# topic_model_speeches.visualize_topics_over_time(speeches_topics_over_time, topics=cluster_1_europe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fcb392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the party distribution of the cluster\n",
    "# Uncomment if you want to analyse the cluster\n",
    "# speeches_cluster_1_europe = speeches_processed_bert[speeches_processed_bert.topic_id.isin([5, 18, 21])]\n",
    "# print(speeches_cluster_1_europe.groupby(\"party\").size().sort_values(ascending = False))\n",
    "# print(\"\\n\")\n",
    "# print(speeches_processed_bert.groupby(\"party\").size().sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75473e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the most prominent politicians of the cluster\n",
    "# Uncomment if you want to analyse the cluster\n",
    "# speeches_cluster_1_europe.groupby(\"full_name\").size().sort_values(ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e12b72",
   "metadata": {},
   "source": [
    "#### 4.1.2.4.2 Cluster democratic structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7f5e86",
   "metadata": {},
   "source": [
    "The second cluster comprises only the topics democratic structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8929a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_2_democratic = [5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758ae4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse the cluster over time\n",
    "# Uncomment if you want to analyse the cluster\n",
    "# topic_model_speeches.visualize_topics_over_time(speeches_topics_over_time, topics=cluster_2_democratic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1201492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the party distribution of the cluster\n",
    "# Uncomment if you want to analyse the cluster\n",
    "# speeches_cluster_2_democratic = speeches_processed_bert[speeches_processed_bert.topic_id.isin(cluster_2_democratic)]\n",
    "# print(speeches_cluster_2_democratic.groupby(\"party\").size().sort_values(ascending = False))\n",
    "# print(\"\\n\")\n",
    "# print(speeches_processed_bert.groupby(\"party\").size().sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84736681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the most prominent politicians of the cluster\n",
    "# Uncomment if you want to analyse the cluster\n",
    "# speeches_cluster_2_democratic.groupby(\"full_name\").size().sort_values(ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9519e477",
   "metadata": {},
   "source": [
    "#### 4.1.2.4.3 Cluster Covid-19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc407950",
   "metadata": {},
   "source": [
    "The third cluster contains the topics 2 and 18 concerning the health and the covid pandemy. We will analyse the prevalence of the topics per time and party."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1957717f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_3_covid = [18, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0a8a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse the cluster over time\n",
    "topic_model_speeches.visualize_topics_over_time(speeches_topics_over_time, topics=cluster_3_covid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa9fcf1",
   "metadata": {},
   "source": [
    "We can identify two peaks of the subject that mirror the development of the pandemic situation. We already saw this trend in the analysis of the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfb1337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the party distribution of the cluster\n",
    "speeches_cluster_3_various = speeches_processed_bert[speeches_processed_bert.topic_id.isin([0,7,8,12])]\n",
    "print(speeches_cluster_3_various.groupby(\"party\").size().sort_values(ascending = False))\n",
    "print(\"\\n\")\n",
    "print(speeches_processed_bert.groupby(\"party\").size().sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a3213b",
   "metadata": {},
   "source": [
    "There are no obvious patterns in the distribution of the speeches per party."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a04f9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the most prominent politicians of the cluster\n",
    "speeches_cluster_3_various.groupby(\"full_name\").size().sort_values(ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd33891a",
   "metadata": {},
   "source": [
    "When analysing the top speaker, we see an suprising pattern as not Jens Spahn nor Karl Lauterbach are in the lsit of most top speaker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa58f9a3",
   "metadata": {},
   "source": [
    "#### 4.1.2.4.4 Cluster foreign politics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8308b53",
   "metadata": {},
   "source": [
    "The largest cluster combines seven topics (6, 7, 11, 12, 15, 19, 20) concenring foreign politics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3f05ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_4_foreign_politics = [6, 7, 11, 12, 15, 19, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455e71b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse the cluster over time\n",
    "# Uncomment if you want to analyse the cluster\n",
    "# topic_model_speeches.visualize_topics_over_time(speeches_topics_over_time, topics=cluster_4_foreign_politics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5552ecfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the party distribution of the cluster\n",
    "# Uncomment if you want to analyse the cluster\n",
    "# speeches_cluster_4_foreign_politics = speeches_processed_bert[speeches_processed_bert.topic_id.isin(cluster_4_foreign_politics)]\n",
    "# print(speeches_cluster_4_foreign_politics.groupby(\"party\").size().sort_values(ascending = False))\n",
    "# print(\"\\n\")\n",
    "# print(speeches_processed_bert.groupby(\"party\").size().sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73312e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the most prominent politicians of the cluster\n",
    "# Uncomment if you want to analyse the cluster\n",
    "# speeches_cluster_4_foreign_politics.groupby(\"full_name\").size().sort_values(ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d39b34",
   "metadata": {},
   "source": [
    "#### 4.1.2.4.5 Cluster occupations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c184aaf",
   "metadata": {},
   "source": [
    "The next cluster contains topic 8 and 9 and deals with the subject occupations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cb33c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_5_occupation = [8, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9beed9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse the cluster over time\n",
    "# Uncomment if you want to analyse the cluster\n",
    "# topic_model_speeches.visualize_topics_over_time(speeches_topics_over_time, topics=cluster_5_occupation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ff3cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the party distribution of the cluster\n",
    "# Uncomment if you want to analyse the cluster\n",
    "# speeches_cluster_5_occupation = speeches_processed_bert[speeches_processed_bert.topic_id.isin(cluster_5_occupation)]\n",
    "# print(speeches_cluster_5_occupation.groupby(\"party\").size().sort_values(ascending = False))\n",
    "# print(\"\\n\")\n",
    "# print(speeches_processed_bert.groupby(\"party\").size().sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66792adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the most prominent politicians of the cluster\n",
    "# Uncomment if you want to analyse the cluster\n",
    "# speeches_cluster_5_occupation.groupby(\"full_name\").size().sort_values(ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f2baac",
   "metadata": {},
   "source": [
    "#### 4.1.2.4.6 Cluster discrimination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c9d92e",
   "metadata": {},
   "source": [
    "The sixth cluster is only the topic 17 that treats the issue migration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbb70a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_6_discrimination = [17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9627f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse the cluster over time\n",
    "# Uncomment if you want to analyse the cluster\n",
    "# topic_model_speeches.visualize_topics_over_time(speeches_topics_over_time, topics=cluster_6_discrimination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60d94b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the party distribution of the cluster\n",
    "# Uncomment if you want to analyse the cluster\n",
    "# speeches_cluster_6_discrimination = speeches_processed_bert[speeches_processed_bert.topic_id.isin(cluster_6_discrimination)]\n",
    "# print(speeches_cluster_6_discrimination.groupby(\"party\").size().sort_values(ascending = False))\n",
    "# print(\"\\n\")\n",
    "# print(speeches_processed_bert.groupby(\"party\").size().sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7ae07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the most prominent politicians of the cluster\n",
    "# Uncomment if you want to analyse the cluster\n",
    "# speeches_cluster_6_discrimination.groupby(\"full_name\").size().sort_values(ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80698847",
   "metadata": {},
   "source": [
    "#### 4.1.2.4.7 Cluster police and safety"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7368f25",
   "metadata": {},
   "source": [
    "Another cluster comprises only one topic (14) and deals with police and safety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05a6753",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_7_police_safety = [14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2110d831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse the cluster over time\n",
    "# Uncomment if you want to analyse the cluster\n",
    "# topic_model_speeches.visualize_topics_over_time(speeches_topics_over_time, topics=cluster_7_police_safety)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffac1cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the party distribution of the cluster\n",
    "# Uncomment if you want to analyse the cluster\n",
    "# speeches_cluster_7_police_safety = speeches_processed_bert[speeches_processed_bert.topic_id.isin(cluster_7_police_safety)]\n",
    "# print(speeches_cluster_7_police_safety.groupby(\"party\").size().sort_values(ascending = False))\n",
    "# print(\"\\n\")\n",
    "# print(speeches_processed_bert.groupby(\"party\").size().sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4295ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the most prominent politicians of the cluster\n",
    "# Uncomment if you want to analyse the cluster\n",
    "# speeches_cluster_7_police_safety.groupby(\"full_name\").size().sort_values(ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba365e93",
   "metadata": {},
   "source": [
    "#### 4.1.2.4.8 Cluster climate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572112ed",
   "metadata": {},
   "source": [
    "Cluster eight (topic 1) includes speeches about climate change and protection. To get an overview of the topic, we analyse it in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2b7cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_8_climate = [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b78b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse the cluster over time\n",
    "topic_model_speeches.visualize_topics_over_time(speeches_topics_over_time, topics=cluster_8_climate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f84eef",
   "metadata": {},
   "source": [
    "There are two peaks for the topics around the end of 2019, 2020 and 2021. The main topics of the peaks renewable energy topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20302ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the party distribution of the cluster\n",
    "speeches_cluster_8_climate = speeches_processed_bert[speeches_processed_bert.topic_id.isin(cluster_8_climate)]\n",
    "print(speeches_cluster_8_climate.groupby(\"party\").size().sort_values(ascending = False))\n",
    "print(\"\\n\")\n",
    "print(speeches_processed_bert.groupby(\"party\").size().sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9e7239",
   "metadata": {},
   "source": [
    "There is a strong difference in the amount of speeches covering the subject. The parties Die Grüne and CDU cover this topics in their speeches more than other parties controlled for their general frequency of speeches. Most of the speeches of the CDU are held by Peter Altmaier, as we see in the next code snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcca740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the most prominent politicians of the cluster\n",
    "speeches_cluster_8_climate.groupby(\"full_name\").size().sort_values(ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e8c8f5",
   "metadata": {},
   "source": [
    "We observe many politicians of the party Die Grünen and the CDU politican Peter Altmaier. He is was the federal minister for energy and economy, which explains his top position in the overview."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1464ba09",
   "metadata": {},
   "source": [
    "#### 4.1.2.4.9 Cluster digitalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e6b56e",
   "metadata": {},
   "source": [
    "In the ninth cluster is topic 4 covering digitalisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20eb2974",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_9_digitalisation = [4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd5a4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse the cluster over time\n",
    "# Uncomment if you want to analyse the cluster\n",
    "# topic_model_speeches.visualize_topics_over_time(speeches_topics_over_time, topics=cluster_9_digitalisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0efeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the party distribution of the cluster\n",
    "# Uncomment if you want to analyse the cluster\n",
    "# speeches_cluster_9_digitalisation = speeches_processed_bert[speeches_processed_bert.topic_id.isin(cluster_9_digitalisation)]\n",
    "# print(speeches_cluster_9_digitalisation.groupby(\"party\").size().sort_values(ascending = False))\n",
    "# print(\"\\n\")\n",
    "# print(speeches_processed_bert.groupby(\"party\").size().sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e78c798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the most prominent politicians of the cluster\n",
    "# Uncomment if you want to analyse the cluster\n",
    "# speeches_cluster_9_digitalisation.groupby(\"full_name\").size().sort_values(ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d490629",
   "metadata": {},
   "source": [
    "#### 4.1.2.4.10 Cluster health"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa262a0",
   "metadata": {},
   "source": [
    "The subject health is present in topic 3 and 22."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8fa8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_10_health = [3,22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001836d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse the cluster over time\n",
    "# Uncomment if you want to analyse the cluster\n",
    "# topic_model_speeches.visualize_topics_over_time(speeches_topics_over_time, topics=cluster_10_health)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b60dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the party distribution of the cluster\n",
    "# Uncomment if you want to analyse the cluster\n",
    "# speeches_cluster_10_health = speeches_processed_bert[speeches_processed_bert.topic_id.isin(cluster_10_health)]\n",
    "# print(speeches_cluster_10_health.groupby(\"party\").size().sort_values(ascending = False))\n",
    "# print(\"\\n\")\n",
    "# print(speeches_processed_bert.groupby(\"party\").size().sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f694f759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the most prominent politicians of the cluster\n",
    "# Uncomment if you want to analyse the cluster\n",
    "# speeches_cluster_10_health.groupby(\"full_name\").size().sort_values(ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b059eb0",
   "metadata": {},
   "source": [
    "#### 4.1.2.4.11 Cluster extremism and religion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9756d8b",
   "metadata": {},
   "source": [
    "Similar to the cluster of the tweets we have a cluster (topic 13 and 16) dealing with extremism and religion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a146d765",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_11_extremism_religion = [13, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b0e148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse the cluster over time\n",
    "# Uncomment if you want to analyse the cluster\n",
    "# topic_model_speeches.visualize_topics_over_time(speeches_topics_over_time, topics=cluster_11_extremism_religion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e642a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the party distribution of the cluster\n",
    "# Uncomment if you want to analyse the cluster\n",
    "# speeches_cluster_11_extremism_religion = speeches_processed_bert[speeches_processed_bert.topic_id.isin(cluster_11_extremism_religion)]\n",
    "# print(speeches_cluster_11_extremism_religion.groupby(\"party\").size().sort_values(ascending = False))\n",
    "# print(\"\\n\")\n",
    "# print(speeches_processed_bert.groupby(\"party\").size().sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33be7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the most prominent politicians of the cluster\n",
    "# Uncomment if you want to analyse the cluster\n",
    "# speeches_cluster_11_extremism_religion.groupby(\"full_name\").size().sort_values(ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a421f83",
   "metadata": {},
   "source": [
    "#### 4.1.2.4.12 Cluster migration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49aaeb46",
   "metadata": {},
   "source": [
    "The last cluster with topic 10 is about migration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e653e925",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_12_migration = [10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695e8d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse the cluster over time\n",
    "topic_model_speeches.visualize_topics_over_time(speeches_topics_over_time, topics=cluster_12_migration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902ac5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the party distribution of the cluster\n",
    "speeches_cluster_12_migration = speeches_processed_bert[speeches_processed_bert.topic_id.isin(cluster_12_migration)]\n",
    "print(speeches_cluster_12_migration.groupby(\"party\").size().sort_values(ascending = False))\n",
    "print(\"\\n\")\n",
    "print(speeches_processed_bert.groupby(\"party\").size().sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e918d4",
   "metadata": {},
   "source": [
    "There are not that many speeches about migration but most of them are held by FDP and AFD. We saw a similar trend in the tweets but the comparable high amounts of tweets of the AFD does not tranfer to the number of speeches hold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96a8101",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# See the most prominent politicians of the cluster\n",
    "speeches_cluster_12_migration.groupby(\"full_name\").size().sort_values(ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dfe326",
   "metadata": {},
   "source": [
    "### 4.1.2.5 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1192c751",
   "metadata": {},
   "source": [
    "We use the results of the last subsections to answer the second questions:\n",
    "\n",
    "**What are the main topics of speeches of prominent politicians of the six parties in the German Bundestag differ in the period of the 19th Bundestag?**\n",
    "\n",
    "We trained a BERTopic model to give us an overview the topics, that are presented in the sections 4.1.2.1 - 4.1.2.3. Based on the identified topics and the inherent modelling clustering we defined 12 overarching cluster of subjects that are presented in section 4.1.2.4. The cluster of the topics, could now we be used to for further analysis. To answers the second research question, we identified the following main topics of speeches of the selected politicians:\n",
    "\n",
    "* Europe\n",
    "* Democratic structures\n",
    "* Covid-19\n",
    "* Foreign politics\n",
    "* Occupation\n",
    "* Discrimination\n",
    "* Police and safety\n",
    "* Climate\n",
    "* Digitalisation\n",
    "* Health\n",
    "* Extremism and religion\n",
    "* Migration\n",
    "\n",
    "We did a deep dive in the clusters migration, Covid-19 and climate. The code for deeper analysis of the other clusters is provided and can be used by the interested reader. Based on this analysis we will compare the results with the topics of the speeches in the parlaments in section 4.1.3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9571e3d4",
   "metadata": {},
   "source": [
    "## 4.1.3 Compare topics of tweets and speeches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a01af8",
   "metadata": {},
   "source": [
    "Based on the results of the last two subsections, we now compare the content of tweets and speeches of the German politicians to answer the third research question:\n",
    "\n",
    "**How do the main topics of tweets and speeches of prominent politicians of the six parties in the German Bundestag differ in the period of the 19th Bundestag?**\n",
    "\n",
    "For this we will compare the differences of the overall topics of the two medias and the difference of the topic distribution broken down to the parties. When the topics of tweets and speeches, we take into account the inherent differences of the two medias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7161a086",
   "metadata": {},
   "source": [
    "### 4.1.3.1 Topics in tweets and speeches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295ee872",
   "metadata": {},
   "source": [
    "There was a significant difference in the amount of topics we identified for the tweets and for speeches. One part of this diference can be explained by the many times larger amount of tweets compared to speeches, while another part can be explained by the nature of tweets and speeches. The amount of topics is quite high for tweets, which can be explained as Twitter is an lower barrier for communication and politicians will express opinions for more different subjects, than they are willing to talk about in the Bundestag. This could also be interpreted as a sign, that politician are willing to express their opinions about topics that they are not experts in on Twitter, while their are more selective in their speeches in the Bundestag. Additionally politicans use Twitter to announce various events and to build connections to voters and other people of public interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a324bc43",
   "metadata": {},
   "source": [
    "When analysing the overall clusters of the topics of speeches and tweets, we found a high amount of matches. There are no obvious signifcant differences in the topics for both medias. However the relative focus of the topics between the medias differs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d9e046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise top topics for tweets\n",
    "tweets_processed_bert.groupby(\"topic\").size().sort_values(ascending = False)[[1, 3, 5, 6, 7, 8, 9, 10, 12, 13]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cea5d14",
   "metadata": {},
   "source": [
    "The first obvious difference in the top topics, is the presence of many non relevant topics in the models for tweets. Therefore we select the subset of the most prominent relevant topics for the tweets model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b0cd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise top topics for speeches\n",
    "speeches_processed_bert.groupby(\"topic\").size().sort_values(ascending = False)[1:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47bae38",
   "metadata": {},
   "source": [
    "One can see striking differences in the top topics between the two medias. The topics digitalisation, climate, occupation and covid pandemy are present in both medias. While the topics concerning the foreign politics and armed conflict is presented with a high frequency in speeches, we do not see it in the top topics of tweets. The topic EU, Europe and Euro has an high presence in the tweets of the politicians, but not that often in the speeches dataset. This is most likely caused by the european parliament election in 2019 and the previous election campaigns. This analysis uses only an except of the topics and therefore has only limited validity. However it still provides an first overview of the differences in the most prominent topics per medium. One could go into deeper analysis but this is out of scope for this work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3a5cfe",
   "metadata": {},
   "source": [
    "### 4.1.3.2 Topics of AFD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8516815",
   "metadata": {},
   "source": [
    "When comparing the most prominent topics of politicans of the party AFD, we can again identify differences in the topic distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dea39b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise top topics for tweets\n",
    "tweets_processed_bert[tweets_processed_bert.party == \"AFD\"].groupby(\"topic\").size().sort_values(ascending = False)[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1117f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise top topics for speeches\n",
    "speeches_processed_bert[speeches_processed_bert.party == \"AFD\"].groupby(\"topic\").size().sort_values(ascending = False)[1:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059943a3",
   "metadata": {},
   "source": [
    "The largest topics of the Twitter presence of politicians of the AFD are police, migration, refugees and muslims. This is in strong contrast to the topics studying, climate, constituition and digitalisation, that are the focus of most speeches in the Bundestag."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a667c9ef",
   "metadata": {},
   "source": [
    "### 4.1.3.3 Topics of CDU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71eed0ad",
   "metadata": {},
   "source": [
    "The most striking difference for the tweets and speeches for the CDU is the focus on climate and energy that is not present in the top topics of the tweets. Another interesting observation is the missing representation of the topic foreign politics in the top tweets topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a51cb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise top topics for tweets\n",
    "tweets_processed_bert[tweets_processed_bert.party == \"CDU\"].groupby(\"topic\").size().sort_values(ascending = False)[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168cdc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise top topics for speeches\n",
    "speeches_processed_bert[speeches_processed_bert.party == \"CDU\"].groupby(\"topic\").size().sort_values(ascending = False)[1:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc22433f",
   "metadata": {},
   "source": [
    "### 4.1.3.4 Topics of FDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b4b31c",
   "metadata": {},
   "source": [
    "The pattern of a missing representation of the topics foreign politcis and armed conflicts is repeating in when analysing the tweets of the FDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f9695f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise top topics for speeches\n",
    "tweets_processed_bert[tweets_processed_bert.party == \"FDP\"].groupby(\"topic\").size().sort_values(ascending = False)[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7164deb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise top topics for speeches\n",
    "speeches_processed_bert[speeches_processed_bert.party == \"FDP\"].groupby(\"topic\").size().sort_values(ascending = False)[1:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3cefd2",
   "metadata": {},
   "source": [
    "### 4.1.3.5 Topics of Grüne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17ea379",
   "metadata": {},
   "source": [
    "The coherence between the topics of the tweets and the topics of the speeches is comparably high for the party Die Grünen. One subject from the speeches that is not highly represented in the tweeets is digitalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582d8523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise top topics for speeches\n",
    "tweets_processed_bert[tweets_processed_bert.party == \"Grüne\"].groupby(\"topic\").size().sort_values(ascending = False)[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3263ab01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise top topics for speeches\n",
    "speeches_processed_bert[speeches_processed_bert.party == \"Grüne\"].groupby(\"topic\").size().sort_values(ascending = False)[1:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd81d53f",
   "metadata": {},
   "source": [
    "The largest topics of the Twitter presence of politicians of the AFD are police, migration, refugees and muslims. This is in strong contrast to the topics studying, climate, constituition and digitalisation, that are the focus of most speeches in the Bundestag."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c704ef",
   "metadata": {},
   "source": [
    "### 4.1.3.6 Topics of Linke"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cff2e5e",
   "metadata": {},
   "source": [
    "The party Die Linken also has many overlapping topics in both medias. But the topic occupation and police, that is quite prevailing in the tweets is nearly not at all represented in the speeches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f624bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise top topics for speeches\n",
    "tweets_processed_bert[tweets_processed_bert.party == \"Linke\"].groupby(\"topic\").size().sort_values(ascending = False)[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca53867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise top topics for speeches\n",
    "speeches_processed_bert[speeches_processed_bert.party == \"Linke\"].groupby(\"topic\").size().sort_values(ascending = False)[1:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05733320",
   "metadata": {},
   "source": [
    "### 4.1.3.7 Topics of SPD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a087f0",
   "metadata": {},
   "source": [
    "The SPD politicans hold speeches about the similar topics as they tweeet, with the common difference, that they not dicsuss forein affair on Twitter often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a03bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise top topics for speeches\n",
    "tweets_processed_bert[tweets_processed_bert.party == \"SPD\"].groupby(\"topic\").size().sort_values(ascending = False)[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c948766c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise top topics for speeches\n",
    "speeches_processed_bert[speeches_processed_bert.party == \"SPD\"].groupby(\"topic\").size().sort_values(ascending = False)[1:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5626b754",
   "metadata": {},
   "source": [
    "The largest topics of the Twitter presence of politicians of the AFD are police, migration, refugees and muslims. This is in strong contrast to the topics studying, climate, constituition and digitalisation, that are the focus of most speeches in the Bundestag."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0740a892",
   "metadata": {},
   "source": [
    "### 4.1.3.8 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f355dd",
   "metadata": {},
   "source": [
    "Based on the last subseciton we answer the third research questions.\n",
    "\n",
    "**How do the main topics of tweets and speeches of prominent politicians of the six parties in the German Bundestag differ in the period of the 19th Bundestag?**\n",
    "\n",
    "There are many similarities between the communicaiton."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e378a9b3",
   "metadata": {},
   "source": [
    "# 4.2 Topic model validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620d82c1",
   "metadata": {},
   "source": [
    "For validating the results in the previous section, we use word and topic intrusion tests based on [Reading Tea Leaves: How Humans Interpret Topic Models](https://proceedings.neurips.cc/paper/2009/file/f92586a25bb3145facd64ab20fd554ff-Paper.pdf). We implement an interface and evaluate the results of humans label by the two authors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc63d4ad",
   "metadata": {},
   "source": [
    "## 4.2.1 Word intrusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd187209",
   "metadata": {},
   "source": [
    "Word intrusion measures the coherence of topics. For this we show annotators 5 high probability keywords of a particular topic and an intruder keyword form another topic and give them the task to identify the intruder keyword. The model precision as measured by the word intrusion score is then defined as the the number of time the intruder keyword was chosen divided by the number of topics shown. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70d6b7b",
   "metadata": {},
   "source": [
    "### 4.2.1.1 Define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd4eb58",
   "metadata": {},
   "source": [
    "Before we can execute the word intrusion task we need to define a set of help functions. We are creating an simple interface for this task to be executed in the Notebooks cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc65d35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a random document searcher\n",
    "def choose_random_document(index, number_documents):\n",
    "    rand_document = random.randrange(-1, number_documents-2)\n",
    "    if rand_document != index:\n",
    "        return rand_document \n",
    "    else:\n",
    "        return choose_random_document(index, number_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a16326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for creating a word intrusion dataset\n",
    "def create_word_intrusion_dataset(topic_model):\n",
    "    number_documents = len(topic_model.get_topics())\n",
    "    records_list = []\n",
    "    for i in range(number_documents): \n",
    "        word_list = []\n",
    "        for j in range(5):\n",
    "            word_list.append(topic_model.get_topic(i-1)[j][0])\n",
    "        intruder_word = topic_model.get_topic(choose_random_document(i-1, number_documents))[0][0]\n",
    "        intruder_position = random.randrange(4)\n",
    "        word_list.insert(intruder_position, intruder_word)\n",
    "        word_list.append(intruder_word)\n",
    "        word_list.append(intruder_position)\n",
    "        records_list.append(word_list)\n",
    "    word_intrusion_df = pd.DataFrame.from_records(records_list)\n",
    "    word_intrusion_df.columns = [\"word_0\", \"word_1\", \"word_2\", \"word_3\", \"word_4\", \"word_5\", \n",
    "                                 \"intruder_word\", \"intruder_index\"]\n",
    "    return word_intrusion_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd00a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that divides the word intrusion dataset into seperate sets for the the annotators\n",
    "def generate_annotator_set(df, number_label, number_iaa, name_1, name_2):\n",
    "    length = df.shape[0]\n",
    "    if 2*number_label + number_iaa > length:\n",
    "        print(\"Too many labels for the size of the dataframe\")\n",
    "    df_shuffeled = df.sample(frac=1).reset_index(drop=True)\n",
    "    df_shuffeled[name_1] = [1] * (number_label+number_iaa) + [0] * (length-number_label-number_iaa)\n",
    "    df_shuffeled[name_2] = [0] * (number_label) + [1] * (number_label+number_iaa) + [0] * (length-2*number_label-number_iaa)\n",
    "    df_shuffeled[\"iaa_flag\"] = [0] * number_label + [1] * number_iaa + [0] * (length-number_label-number_iaa)\n",
    "    df_shuffeled[\"wis_label\"] = [1] * number_label + [0] * number_iaa + [1] * (length-number_label-number_iaa)\n",
    "    return df_shuffeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d8700f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that offers an interface in Jupyter notebook for the word intrusion task\n",
    "def word_intrusion_test(word_df, name, medium):\n",
    "    intrusion_df = word_df[word_df[name] == 1].reset_index(drop = True)\n",
    "    \n",
    "    max_count = intrusion_df.shape[0]\n",
    "    global i\n",
    "    i = 0\n",
    "    \n",
    "    button_0 = widgets.Button(description = intrusion_df.word_0[i])\n",
    "    button_1 = widgets.Button(description = intrusion_df.word_1[i])\n",
    "    button_2 = widgets.Button(description = intrusion_df.word_2[i])\n",
    "    button_3 = widgets.Button(description = intrusion_df.word_3[i])\n",
    "    button_4 = widgets.Button(description = intrusion_df.word_4[i])\n",
    "    button_5 = widgets.Button(description = intrusion_df.word_5[i])\n",
    "\n",
    "\n",
    "    chosen_words = []\n",
    "    chosen_positions= []\n",
    "\n",
    "    display(\"Word Intrusion Test\")\n",
    "\n",
    "    f = IntProgress(min=0, max=max_count)\n",
    "    display(f)\n",
    "\n",
    "    display(button_0)\n",
    "    display(button_1)\n",
    "    display(button_2)\n",
    "    display(button_3)\n",
    "    display(button_4)\n",
    "    display(button_5)\n",
    "\n",
    "\n",
    "    def btn_eventhandler(position, obj):\n",
    "        global i \n",
    "        i += 1\n",
    "        \n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        display(\"Word Intrusion Text\")\n",
    "        display(f)\n",
    "        f.value += 1\n",
    "        \n",
    "        choosen_text = obj.description\n",
    "        chosen_words.append(choosen_text)\n",
    "        \n",
    "        chosen_positions.append(position)\n",
    "        \n",
    "        if i < max_count:\n",
    "\n",
    "            button_0 = widgets.Button(description = intrusion_df.word_0[i])\n",
    "            button_1 = widgets.Button(description = intrusion_df.word_1[i])\n",
    "            button_2 = widgets.Button(description = intrusion_df.word_2[i])\n",
    "            button_3 = widgets.Button(description = intrusion_df.word_3[i])\n",
    "            button_4 = widgets.Button(description = intrusion_df.word_4[i])\n",
    "            button_5 = widgets.Button(description = intrusion_df.word_5[i])\n",
    "            \n",
    "            display(button_0)\n",
    "            display(button_1)\n",
    "            display(button_2)\n",
    "            display(button_3)\n",
    "            display(button_4)\n",
    "            display(button_5)\n",
    "            \n",
    "            button_0.on_click(partial(btn_eventhandler,0))\n",
    "            button_1.on_click(partial(btn_eventhandler,1))\n",
    "            button_2.on_click(partial(btn_eventhandler,2))\n",
    "            button_3.on_click(partial(btn_eventhandler,3))\n",
    "            button_4.on_click(partial(btn_eventhandler,4))\n",
    "            button_5.on_click(partial(btn_eventhandler,5))\n",
    "        else:\n",
    "            print (\"Thanks \" + name + \" you finished all the work!\")\n",
    "            intrusion_df[\"chosen_word\"] = chosen_words\n",
    "            intrusion_df[\"chosen_position\"] = chosen_positions\n",
    "            intrusion_df.to_csv(\"../data/processed/word_intrusion_test_\" + name + \"_\" + medium + \".csv\", index = False)\n",
    "\n",
    "\n",
    "\n",
    "    button_0.on_click(partial(btn_eventhandler,0))\n",
    "    button_1.on_click(partial(btn_eventhandler,1))\n",
    "    button_2.on_click(partial(btn_eventhandler,2))\n",
    "    button_3.on_click(partial(btn_eventhandler,3))\n",
    "    button_4.on_click(partial(btn_eventhandler,4))\n",
    "    button_5.on_click(partial(btn_eventhandler,5))\n",
    "    \n",
    "    return intrusion_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a63c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the word intrusion score for the two annotator sets\n",
    "def calculate_word_intrusion(name_1, name_2, medium):\n",
    "    df_word_intrusion_1 = pd.read_csv(\"../data/processed/word_intrusion_test_\" + name_1 + \"_\" + medium + \".csv\")\n",
    "    df_word_intrusion_2 = pd.read_csv(\"../data/processed/word_intrusion_test_\" + name_2 + \"_\" + medium + \".csv\")\n",
    "    iaa_values_1 = df_word_intrusion_1[df_word_intrusion_1.iaa_flag == 1].chosen_position.values\n",
    "    iaa_values_2 = df_word_intrusion_2[df_word_intrusion_2.iaa_flag == 1].chosen_position.values\n",
    "    kappa = cohen_kappa_score(iaa_values_1, iaa_values_2)\n",
    "    df_word_intrusion = df_word_intrusion_1.append(df_word_intrusion_2)\n",
    "    df_word = df_word_intrusion[df_word_intrusion[\"wis_label\"] == 1]\n",
    "    df_word[\"intruder_chosen\"] = df_word[\"intruder_word\"] == df_word[\"chosen_word\"]\n",
    "    return  df_word[\"intruder_chosen\"].mean(), kappa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cb0655",
   "metadata": {},
   "source": [
    "### 4.2.1.2 Validation of tweets topic model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9583e915",
   "metadata": {},
   "source": [
    "Based on the above defined functions, we are going to execute the word intrusion task for the tweets BERTopic model. The annotation is done by the two authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e65b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "topic_model_tweets = BERTopic.load(\"../models/bertopic_tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a421586b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create candidate dataset\n",
    "word_intrusion_dataset_tweets = create_word_intrusion_dataset(topic_model_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a942353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create label dataset for two annotators\n",
    "word_intrusion_dataset_tweets_label = generate_annotator_set(word_intrusion_dataset_tweets, 45, 11, \"Jakob\",\n",
    "                                                             \"Stjepan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c769caac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Execute annotation for first candidate\n",
    "# Uncomment if annotation is repeated\n",
    "# df_word_intrusion_jakob_tweets = word_intrusion_test(word_intrusion_dataset_tweets_label, \"Jakob\", \"Tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd727a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute annotation for second candidate\n",
    "# Uncomment if annotation is repeated\n",
    "# df_word_intrusion_stjepan_tweets = word_intrusion_test(word_intrusion_dataset_tweets_label, \"Stjepan\", \"Tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23d12ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate intrusion score and cohens kappa\n",
    "word_intrusion_score_tweets, word_kappa_tweets = calculate_word_intrusion(\"Jakob\", \"Stjepan\", \"Tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a708ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cohens kappa\n",
    "print(\"Cohens kappa is: \" + str(round(word_kappa_tweets,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf42315",
   "metadata": {},
   "source": [
    "Our inter annotator agreement is on a satisfactory level and shows a good consensus of our annotations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a40785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intrusion score\n",
    "print(\"The word intrusion score is: \" + str(round(word_intrusion_score_tweets,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648d73f7",
   "metadata": {},
   "source": [
    "We see an a good intrusion score as many of the intruder words were detected. These results could be improved by fixing the identified limitations of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d5072c",
   "metadata": {},
   "source": [
    "### 4.2.1.3 Validation of speeches topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac96098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "topic_model_speeches = BERTopic.load(\"../models/bertopic_speeches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e67a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create candidate dataset\n",
    "word_intrusion_dataset_speeches = create_word_intrusion_dataset(topic_model_speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a85193e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create label dataset for two annotators\n",
    "word_intrusion_dataset_speeches_label = generate_annotator_set(word_intrusion_dataset_speeches, 10, 5, \"Jakob\",\n",
    "                                                             \"Stjepan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0189bb40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Execute annotation for first candidate\n",
    "# df_word_intrusion_jakob_speeches = word_intrusion_test(word_intrusion_dataset_speeches_label, \"Jakob\", \"Speeches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff54591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute annotation for second candidate\n",
    "# df_word_intrusion_stjepan_speeches = word_intrusion_test(word_intrusion_dataset_speeches_label, \"Stjepan\", \"Speeches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867a8dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate intrusion score and cohens kappa\n",
    "word_intrusion_score_speeches, word_kappa_speeches = calculate_word_intrusion(\"Jakob\", \"Stjepan\", \"Speeches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7796f575",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cohens kappa\n",
    "print(\"Cohens kappa is: \" + str(round(word_kappa_speeches,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5de23b8",
   "metadata": {},
   "source": [
    "Our inter annotator agreement is on a satisfactory level and shows a good consensus of our annotations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168fc43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intrusion score\n",
    "print(\"The word intrusion score is: \" + str(round(word_intrusion_score_speeches,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0c52b1",
   "metadata": {},
   "source": [
    "We see an a good intrusion score as many of the intruder words were detected. These results could be improved by fixing the identified limitations of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7921326c",
   "metadata": {},
   "source": [
    "## 4.2.2 Topic Intrusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74a0ef9",
   "metadata": {},
   "source": [
    "By measurng the topic intrusion score we want to test if the algorithms probability distribution of topics for the documents seems to match the human assesment. For this we show an excerpt of the document, the three topics with the highest probability for this topic and a random low probability topic. To calculate the topic intrusion score we take the mean of the differences of the log probabilities of the selected topic and the true topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe225a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that combines key words into a single string\n",
    "def create_topic_string(topic_info):\n",
    "    word_list = []\n",
    "    for i in range(8):\n",
    "        word_list.append(topic_info[i][0])\n",
    "    return \", \".join(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f6a3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that prepares the topic intrusion dataset\n",
    "def create_topic_intrusion_dataset(data, topic_model, topic_probabilities, test_number = 100):\n",
    "    number_documents = data.shape[0]\n",
    "    if number_documents < test_number:\n",
    "        print(\"You can only choose as many test as number of documents!\")\n",
    "    number_topics = len(topic_model.get_topics())\n",
    "    records_list = []\n",
    "    for i in range(test_number): \n",
    "        topic_list = []\n",
    "        high_probability_documents = sorted(zip(topic_probabilities[i].tolist(), list(range(number_topics))), reverse=True)[:3]\n",
    "        low_probability_documents = sorted(zip(topic_probabilities[i].tolist(), list(range(number_topics))), reverse=True)[3:]\n",
    "        for j in range(3):\n",
    "            topic_index = high_probability_documents[j][1]\n",
    "            topic_list.append(create_topic_string(topic_model.get_topic(topic_index)))\n",
    "        intruder_document = low_probability_documents[random.randrange(number_topics-4)]\n",
    "        intruder_topic = create_topic_string(topic_model.get_topic(intruder_document[1]))\n",
    "        intruder_position = random.randrange(4)\n",
    "        topic_list.insert(intruder_position, intruder_topic)\n",
    "        for k in range(3):\n",
    "            topic_index = high_probability_documents[k][1]\n",
    "            topic_list.append(high_probability_documents[k][0])\n",
    "        topic_list.insert(intruder_position + 4, intruder_document[0])\n",
    "        topic_list.append(intruder_topic)\n",
    "        topic_list.append(intruder_document[0])\n",
    "        topic_list.append(intruder_position)\n",
    "        topic_list.append(data[\"text\"][i])\n",
    "        records_list.append(topic_list)\n",
    "    df = pd.DataFrame.from_records(records_list)\n",
    "    df.columns = [\"topic_0\", \"topic_1\", \"topic_2\", \"topic_3\",\"probability_topic_0\",\"probability_topic_1\",\n",
    "                  \"probability_topic_2\",\"probability_topic_3\", \"intruder_topic\", \"intruder_topic_probability\",\n",
    "                  \"intruder_index\", \"text\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a7e085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that generate the interface for the topic intrusion test\n",
    "def topic_intrusion_test(intrusion_df, name, medium):\n",
    "    intrusion_df = intrusion_df[intrusion_df[name] == 1].reset_index(drop = True)\n",
    "    \n",
    "    max_count = intrusion_df.shape[0]\n",
    "    global i\n",
    "    i = 0\n",
    "    \n",
    "    layout = widgets.Layout(width='auto')\n",
    "\n",
    "    button_0 = widgets.Button(description = intrusion_df.topic_0[i], layout = layout)\n",
    "    button_1 = widgets.Button(description = intrusion_df.topic_1[i], layout = layout)\n",
    "    button_2 = widgets.Button(description = intrusion_df.topic_2[i], layout = layout)\n",
    "    button_3 = widgets.Button(description = intrusion_df.topic_3[i], layout = layout)\n",
    "    \n",
    "    chosen_elements = []\n",
    "    chosen_positions = []\n",
    "    chosen_probabilities = []\n",
    "\n",
    "    display(\"Topic Intrusion Test\")\n",
    "\n",
    "    f = IntProgress(min=0, max=max_count)\n",
    "    display(f)\n",
    "    \n",
    "    if len(intrusion_df.text[i]) < 1100:\n",
    "        display(intrusion_df.text[i][0:1100])\n",
    "    else :\n",
    "        display(intrusion_df.text[i][100:1100])\n",
    "\n",
    "    display(button_0)\n",
    "    display(button_1)\n",
    "    display(button_2)\n",
    "    display(button_3)\n",
    "\n",
    "\n",
    "    def btn_eventhandler(position, column, obj):\n",
    "        \n",
    "        global i\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        display(\"Topic Intrusion Text\")\n",
    "        display(f)\n",
    "        f.value += 1\n",
    "                \n",
    "        choosen_text = obj.description\n",
    "        chosen_elements.append(choosen_text)\n",
    "        chosen_positions.append(position)\n",
    "        chosen_probabilities.append(intrusion_df[column][i])\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "        if i < max_count:\n",
    "\n",
    "            button_0 = widgets.Button(description = intrusion_df.topic_0[i], layout = layout)\n",
    "            button_1 = widgets.Button(description = intrusion_df.topic_1[i], layout = layout)\n",
    "            button_2 = widgets.Button(description = intrusion_df.topic_2[i], layout = layout)\n",
    "            button_3 = widgets.Button(description = intrusion_df.topic_3[i], layout = layout)\n",
    "            \n",
    "            if len(intrusion_df.text[i]) < 1100:\n",
    "                display(intrusion_df.text[i][0:1000])\n",
    "            else :\n",
    "                display(intrusion_df.text[i][100:1100])\n",
    "            \n",
    "            display(button_0)\n",
    "            display(button_1)\n",
    "            display(button_2)\n",
    "            display(button_3)\n",
    "            \n",
    "            button_0.on_click(partial(btn_eventhandler,0,\"probability_topic_0\"))\n",
    "            button_1.on_click(partial(btn_eventhandler,1,\"probability_topic_1\"))\n",
    "            button_2.on_click(partial(btn_eventhandler,2,\"probability_topic_2\"))\n",
    "            button_3.on_click(partial(btn_eventhandler,3,\"probability_topic_3\"))\n",
    "        else:\n",
    "            print (\"Thanks \" + name + \" you finished all the work!\")\n",
    "            intrusion_df[\"chosen_topic\"] = chosen_elements\n",
    "            intrusion_df[\"chosen_position\"] = chosen_positions\n",
    "            intrusion_df[\"chosen_topic_probability\"] = chosen_probabilities\n",
    "            intrusion_df.to_csv(\"../data/processed/topic_intrusion_test_\" + name + \"_\" + medium + \".csv\", index = False)\n",
    "\n",
    "\n",
    "\n",
    "    button_0.on_click(partial(btn_eventhandler,0,\"probability_topic_0\"))\n",
    "    button_1.on_click(partial(btn_eventhandler,1,\"probability_topic_1\"))\n",
    "    button_2.on_click(partial(btn_eventhandler,2,\"probability_topic_2\"))\n",
    "    button_3.on_click(partial(btn_eventhandler,3,\"probability_topic_3\"))\n",
    "    \n",
    "    return intrusion_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef76d84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to calulate the topic intrusion score\n",
    "def calculate_topic_intrusion(name_1, name_2, medium):\n",
    "    df_topic_intrusion_1 = pd.read_csv(\"../data/processed/topic_intrusion_test_\" + name_1 + \"_\" + medium + \".csv\")\n",
    "    df_topic_intrusion_2 = pd.read_csv(\"../data/processed/topic_intrusion_test_\" + name_2 + \"_\" + medium + \".csv\")\n",
    "    iaa_values_1 = df_topic_intrusion_1[df_topic_intrusion_1.iaa_flag == 1].chosen_position.values\n",
    "    iaa_values_2 = df_topic_intrusion_2[df_topic_intrusion_2.iaa_flag == 1].chosen_position.values\n",
    "    kappa = cohen_kappa_score(iaa_values_1, iaa_values_2)\n",
    "    df_topic_intrusion = df_topic_intrusion_1.append(df_topic_intrusion_2)\n",
    "    df_topic = df_topic_intrusion[df_topic_intrusion[\"wis_label\"] == 1]\n",
    "    df_topic[\"intruder_score\"] = np.log(df_topic[\"intruder_topic_probability\"]) - np.log(df_topic[\"chosen_topic_probability\"])\n",
    "    return  df_topic[\"intruder_score\"].mean(), kappa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081f988f",
   "metadata": {},
   "source": [
    "### 4.2.2.1 Validation of tweets topic model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fd5a50",
   "metadata": {},
   "source": [
    "In the first step we calculate the validation score for the tweets BERTopic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d708a676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "with open( \"../data/processed/tweets_processed_bert.pickle\", \"rb\" ) as handle:\n",
    "    tweets_processed_bert = pickle.load(handle)\n",
    "with open('../data/processed/probabilities_tweets_bert.pickle', 'rb') as handle:\n",
    "    topic_probabilities_tweets = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de8acc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "# topic_model_tweets = BERTopic.load(\"../models/bertopic_tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346a492b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create candidate dataset\n",
    "topic_intrusion_dataset_tweets = create_topic_intrusion_dataset(tweets_processed_bert, topic_model_tweets,\n",
    "                                                               topic_probabilities_tweets, test_number = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf04d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create label dataset for two annotators\n",
    "topic_intrusion_dataset_tweets_label = generate_annotator_set(topic_intrusion_dataset_tweets, 40, 10, \"Jakob\",\n",
    "                                                             \"Stjepan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0163c42b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Execute annotation for first candidate\n",
    "# df_topic_intrusion_jakob_tweets = topic_intrusion_test(topic_intrusion_dataset_tweets_label, \"Jakob\", \"Tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de620f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute annotation for second candidate\n",
    "# df_topic_intrusion_stjepan_tweets = topic_intrusion_test(topic_intrusion_dataset_tweets_label, \"Stjepan\", \"Tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50205f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate intrusion score and cohens kappa\n",
    "topic_intrusion_score_tweets, topic_kappa_tweets = calculate_topic_intrusion(\"Jakob\", \"Stjepan\", \"Tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced65763",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cohens kappa\n",
    "print(\"Cohens kappa is: \" + str(round(topic_kappa_tweets,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd93a61",
   "metadata": {},
   "source": [
    "Our inter annotator agreement is on a satisfactory level and shows a good consensus of our annotations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbfdd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intrusion score\n",
    "print(\"The topic intrusion score is: \" + str(round(topic_intrusion_score_tweets,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37088cc5",
   "metadata": {},
   "source": [
    "It is difficult to objectively evaluate the resulting topic intrusion score. But comparing with the results from the article, we can infer that this score is at least satisfactory and validates our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b40e1cf",
   "metadata": {},
   "source": [
    "### 4.2.2.2 Validation of speeches topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73da8f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "with open( \"../data/processed/speeches_processed_bert.pickle\", \"rb\" ) as handle:\n",
    "    speeches_processed_bert = pickle.load(handle).reset_index(drop = True)\n",
    "with open('../data/processed/probabilities_speeches_bert.pickle', 'rb') as handle:\n",
    "    topic_probabilities_speeches = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0440ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "# topic_model_speeches = BERTopic.load(\"../models/bertopic_speeches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da139ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create candidate dataset\n",
    "topic_intrusion_dataset_speeches = create_topic_intrusion_dataset(speeches_processed_bert, topic_model_speeches,\n",
    "                                                               topic_probabilities_speeches, test_number = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2227fa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create label dataset for two annotators\n",
    "topic_intrusion_dataset_speeches_label = generate_annotator_set(topic_intrusion_dataset_speeches, 40, 10, \"Jakob\",\n",
    "                                                             \"Stjepan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3326ae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Execute annotation for first candidate\n",
    "# df_topic_intrusion_jakob_speeches = topic_intrusion_test(topic_intrusion_dataset_speeches_label, \"Jakob\", \"Speeches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaa843c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute annotation for second candidate\n",
    "# df_topic_intrusion_stjepan_speeches = topic_intrusion_test(topic_intrusion_dataset_speeches_label, \"Stjepan\", \"Speeches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de41e0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate intrusion score and cohens kappa\n",
    "topic_intrusion_score_speeches, topic_kappa_speeches = calculate_topic_intrusion(\"Jakob\", \"Stjepan\", \"Speeches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8703504",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cohens kappa\n",
    "print(\"Cohens kappa is: \" + str(round(topic_kappa_speeches,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c151ee30",
   "metadata": {},
   "source": [
    "The inter annotator agreement on this task is rather small. We did expect this as it was quite difficult to infer the topics from an excerpt from the speeches, as they are generally quite long and therefore it is not easy to infer the right topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9322437c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intrusion score\n",
    "print(\"The word intrusion score is: \" + str(round(topic_intrusion_score_speeches,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cf4e1d",
   "metadata": {},
   "source": [
    "It is difficult to objectively evaluate the resulting topic intrusion score. But comparing with the results from the article, we can infer that this score is at least satisfactory and validates our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4222f577",
   "metadata": {},
   "source": [
    "### 4.2.3 Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6db035",
   "metadata": {},
   "source": [
    "Based on the topic and word intrusion measures we evaluated in these section, we can infer an satisfactory validity of our models. There are different possibility of improvement and we detected several limtation in the results section, however the model still offers noticeable interesting insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efd2c9a",
   "metadata": {},
   "source": [
    "# 4.3 Result analysis sentiment analysis (Stjepan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f3c34f",
   "metadata": {},
   "source": [
    "**Needs to be added** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb7f040",
   "metadata": {},
   "source": [
    "# 4.4 Validation sentiment analysis (Stjepan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e54794",
   "metadata": {},
   "source": [
    "**Needs to be added** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6f7794",
   "metadata": {},
   "source": [
    "# 5 Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718738f2",
   "metadata": {},
   "source": [
    "**Needs to be added** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc51a948",
   "metadata": {},
   "source": [
    "# 5.1 Discussion topic modelling (Jakob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7acd416",
   "metadata": {},
   "source": [
    "We discuss\n",
    "\n",
    "* Model results\n",
    "* Data quality \n",
    "* Model quality\n",
    "* Model validity\n",
    "\n",
    "* Limitations of our approach\n",
    "* Validity of the results\n",
    "* Next possible steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4e5752",
   "metadata": {},
   "source": [
    "# 5.2 Discussion sentiment analysis (Stjepan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfd7680",
   "metadata": {},
   "source": [
    "**Needs to be added** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5266e8",
   "metadata": {},
   "source": [
    "# 6. Bibliography (Stjepan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f6e09d",
   "metadata": {},
   "source": [
    "**Needs to be added** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
