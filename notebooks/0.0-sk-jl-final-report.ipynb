{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Comparing the communication of German politicians across Twitter and plenary speeches using topic modelling and sentiment analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Introduction (Jakob)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Twitter as a medium of research for understanding politicians' communication is a classical approach ([Zimmer & Proferes, 2014](https://doi.org/10.1108/AJIM-09-2013-0083)) that gained much publicity through the prominent tweets of the 45. president of the United States of America Donald J. Trump ([Ott, 2016](https://doi.org/10.1080/15295036.2016.1266686)). To better understand the medium, we will compare the content and style of politicians' communication with their speeches in the German Bundestag.\n",
    "\n",
    "The importance of understanding how politicians communicate on Twitter and other social media is steadily increasing with the significant influence of their content on societies at large ([McLaughlin & Macafee, 2019](https://doi.org/10.1080/15205436.2019.1614196), [Fatema, Yanbin, & Fugui, 2020](https://doi.org/10.1145/3414752.3414787), [van Hillegersberg & Huibers, 2011](https://doi.org/10.1007/978-3-642-23333-3_3)). Besides, decentralising the reciprocal transfer of information between politicians to citizens ([van Hillegersberg & Huibers, 2011](https://doi.org/10.1007/978-3-642-23333-3_3)), there are also increasing problems with manipulations ([Ferrara, Chang, Chen, Muric, & Patel, 2020](https://doi.org/10.5210/fm.v25i11.11431)) and fake news ([Wright, 2021](https://doi.org/10.1075/jlp.21027.wri)). An improved understanding of the medium can help identify harmful practices and interpret the content and style context-dependent.\n",
    "\n",
    "The presented work aims to help increase the understanding of the communication patterns of politicians on Twitter by comparing the content and sentiment of their tweets to their plenary speeches. We execute this analysis for prominent German politicians of the 19th Bundestag in the time range from 2017 to 2021. For this, we defined the following six research questions:\n",
    "\n",
    "* **RQ 1.1** What are the main topics of tweets of prominent politicians of the six parties in the German Bundestag differ in the period of the 19th Bundestag?\n",
    "\n",
    "* **RQ 1.2** What are the main topics of speeches of prominent politicians of the six parties in the German Bundestag differ in the period of the 19th Bundestag?\n",
    "\n",
    "* **RQ 1.3** How do the main topics of tweets and speeches of prominent politicians of the six parties in the German Bundestag differ in the period of the 19th Bundestag?\n",
    "\n",
    "* **RQ 2.1** What are the differences in sentiment of politician’s speeches in the Bundestag and on their Twitter posts with regard to their parties in the period of the 19th Bundestag?\n",
    "\n",
    "* **RQ 2.2** Are there significant differences in sentiment of male politicians from the Bundestag in speeches and posts compared to female politicians in the period of the 19th Bundestag?\n",
    "\n",
    "* **RQ 2.3** How are the sentiments of prominent politicians of the six parties in the German Bundestag developing over time in the period of the 19th Bundestag?\n",
    "\n",
    "Our approach uses data scraped directly from Twitter and plenary speeches obtained from open discourse ([Richter, Koch, Franke, Kraus, Kuruc, Thiem, Högerl, Heine, & Schöps, 2020](https://github.com/open-discourse/open-discourse)) for creating topic models and sentiment analyses for the tweets and speeches of the politicians. For this, we set up a pipeline in Python that preprocesses the data for the modelling part. Before choosing the final best performing model, we try separate models for topic modelling, including Latent Dirichlet Allocation, Non-Negative Matrix Factorisation and BERTopic. We use an unsupervised dictionary-based approach that we test with two different sentiment dictionaries for the sentiment analysis. We validate our results with the current state of the art evaluation methods.\n",
    "\n",
    "The remaining work is structured into four sections Literature Review, Methodology, Results and Discussion. The literature review will analyse existing research approaches and showcase our contributions. The subsequent section comprises the preprocessing and modelling for our analyses, presented as commented code complemented by explanations. Based on the results from the methodology part, we will analyse and validate the results in the fourth section. Finally, we discuss the obtained results and outlook on further work in the last section."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Literature Review (Stjepan)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In preparation for our research we first look at some similar projects and papers already published. As the field of automated media content analysis is starting to grow exponentially in recent years it is no wonder that there are many researches conducted in the field and new findings shared. For our specific task at hand we looked at some topic modeling and sentiment analysis work done on German texts especially in the context of politics and social media."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Like Fatema et a. (Fatema, S., Yanbin, L., & Fugui, D., 2020) have already discussed in their paper that nowadays social media plays an important role for politicians when they want to communicate and get popular with citizens. Like with every marketing approach it is vital for the politicians to get range and a lot of exposure on their topics and opinions. Therefore, platforms like Twitter serve the politicians as channels to talk to a very specific part of their voters in order to keep them informed and market for themselves and their parties. What is also desirable for them is to be able to communicate with the citizens in discussions and to have a platform to share views outside the boundaries of the Bundestag. For our research that means that we have to take a look at how and what the politicians present on their social medias and how it compares to their appearance in the Bundestag."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the first methodology used by us to analyze the politicians we want to apply topic modeling to the corpora. There exists a very promising initiative in the form of Open Discourse (Richter, F., Koch, P., Franke, O., Kraus, J., Kuruc, F., Thiem, A., Högerl, J., Heine, S., & Schöps, K., 2020) that wants to analyze the texts from the speeches of the Bundestag and make the insights more available for the public. In their work they use data science methods in combination with political research to gain information for the vast amount of data available in form of protocols. As the Bundestag is in the center of German politics and this information is available for everybody it is an initiative that is concerned with the political education and information of the German population. Especially topic modelling in form of LDA topic models can be found in their research. <br>\n",
    "In the paper by Heiberger and Koss (Heiberger, R. H., & Koss, C., 2017) we can also see the LDA model being applied to see the topics and trends in the German Bundestag. The goal of their analysis was get an automated method to identify topics and their use. They showed how this method can be applied to successfully gain insights over a vast corpus that cannot be coded by hand. Nevertheless, they stress the need for manual coding or a validation with human coding as the automated approach can't guarantee valid results on its own. Where this approach end we want to go even further and apply more models in form of NNFK models and BERT topic models to try different and new approaches for automated topic modeling. We also want to consider social media data as before mentioned it is getting more vital for politicians to address citizens over social media."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In a second part, we want to look at the sentiments of politicians in the German Bundestag. As Haselmayer and Jenny (Haselmayer, M., & Jenny, M., 2016) stress in their paper that sentiment for public opinion and political campaigns is eminent. Therefore, the analysis of sentiment and the exploration of important factors in determining sentiment are of great interest in us. As warned in their paper we will have to account for language biases and the limitations of the methods applied. An advantage of automated methods definitely lies in their scalability and that they can be further adhenced with human coding. In their paper they created a dictionary for negative sentiment with the help of crowdcoding an approach that we sadly could not apply in our project scope but which seems to be promising. One takeaway is that we have to be cautious with general purpose dictionaries as they may not cover important topics of sentiment or are biased. <br>\n",
    "For the paper of Lommatzsch et a. (Lommatzsch, A., Bütow, F., Ploch, D., & Albayrak, S., 2017) we also see an approach to create better coverage in German sentiment analysis also using two corpora. They also stress the difficulty of the task at hand as sentiment is very context dependent. In their work to apply machine learning models to annotated corpora to create sentiment models. Furthermore, they are using a SentiWS based classifier in their work which we will apply as only the dictionary due to our unlabeled corpus. As the creation of new corpora for the German language was not our prime target we will focus on the application of dictionary approaches to answer our research questions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Judging from the available research we have entered a field of many options which we can't possibly all try out. Nevertheless, we can profit from the experience and best practices suggested by those papers and hope to achieve some interesting results with our own approaches.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Methodology"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3.1 Technical setup (Jakob)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We present the results of our work in a Jupyter Notebook that contains the commented code and additional explanations, analysis and evaluation. The project was programmed in the programming language Python using various preexisting packages. It is possible to reproduce all results with the provided complimentary files. For this, we recommend setting up a conda environment using Python 3.8. One can then install all the packages using the provided .txt file. Besides these packages, one needs to set up a docker environment in section 3.3 to reproduce the data collection. There are separate instructions in the section."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Uncomment when one setups the environment the first time\n",
    "# ! pip install -r requirements.txt\n",
    "# ! python -m spacy download de_core_news_sm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After installing the packages, one can import them with the following lines of code."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import packages\n",
    "\n",
    "# Import basic Python packages\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import random\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "\n",
    "# Import util packages\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Import data processing packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "\n",
    "# Import visualisation packages\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from scipy.ndimage.filters import gaussian_filter1d\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Import natural language processing packages\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy_langdetect import LanguageDetector\n",
    "from spacy.tokens.doc import Doc\n",
    "from spacy.vocab import Vocab\n",
    "\n",
    "# Import topic modelling packages\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel, LdaMulticore\n",
    "from gensim.models.nmf import Nmf\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "from bertopic import BERTopic\n",
    "\n",
    "# Import metrics packages\n",
    "from sklearn.metrics import cohen_kappa_score, f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "# Import interface widgets\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "# Set package options\n",
    "pd.options.mode.chained_assignment = None\n",
    "tqdm.pandas()\n",
    "pyLDAvis.enable_notebook()\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After loading the packages and installing all dependencies, one can run the entire code. Parts that require longer execution time are commented out, and the results of the code part are imported separately. If one wants to reproduce the analysis, it is required to uncomment the parts and comment out the importing of the results. The necessary steps for this are always described in the code. We do not recommend this step if not necessary as some models have runtimes over eight hours."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3.2 Scrape Twitter data (Stjepan)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### 3.2.1 Import packages"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we want to analyze Twitter data and speeches from German politicians one of our first tasks was to get the tweets. Because we decided to analyze the nineteenth Bundestag as our time span, we had to get data from politicians of the six represented parties in the Bundestag. <br>\n",
    "For our purpose we decided to take seven of the more active and most followed politicians from each respected political party. After deciding on which politicians account need to be scraped, we implemented a scraper. We chose snscrape which is a scraper for social network services. It allowed us to scrape every ever-posted tweet by one of our politicians without problems and restrictions. Apart from Twitter this scraper can be used for many more social networks like Facebook or Instagram but for our project Twitter was sufficient."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2.2 Define variables and functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After installation this package allowed us to scrape tweets from desired accounts and gave us many options on how the retrieved data should look. Besides, the accounts username and tweet texts it allowed us to also get information on the time of the post, the displayed Twitter name of the account, the number of replies, retweets and likes. <br> We created a dictionary to map the usernames of the Twitter accounts to the political party. Afterwards, we defined a function that creates a dataframe with the scraped tweets from snscrape given the Twitter username and party. With the tweets scraped we only get the real tweets and reply from the politician and no retweets what helped in the analysis as we only got media content created by the desired target."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "twitter_name_party_dic = {'rbrinkhaus': 'CDU', 'groehe': 'CDU', 'NadineSchoen': 'CDU', 'n_roettgen': 'CDU',\n",
    "                          'peteraltmaier': 'CDU', 'jensspahn': 'CDU', 'MatthiasHauer': 'CDU', 'c_lindner': 'FDP',\n",
    "                          'MarcoBuschmann': 'FDP', 'starkwatzinger': 'FDP', 'Lambsdorff': 'FDP',\n",
    "                          'johannesvogel': 'FDP', 'KonstantinKuhle': 'FDP', 'MAStrackZi': 'FDP',\n",
    "                          'larsklingbeil': 'SPD', 'EskenSaskia': 'SPD', 'hubertus_heil': 'SPD', 'HeikoMaas': 'SPD',\n",
    "                          'MartinSchulz': 'SPD', 'KarambaDiaby': 'SPD', 'Karl_Lauterbach': 'SPD',\n",
    "                          'SteffiLemke': 'Grüne', 'cem_oezdemir': 'Grüne', 'GoeringEckardt': 'Grüne',\n",
    "                          'KonstantinNotz': 'Grüne', 'BriHasselmann': 'Grüne', 'svenlehmann': 'Grüne',\n",
    "                          'ABaerbock': 'Grüne', 'SWagenknecht': 'Linke', 'b_riexinger': 'Linke',\n",
    "                          'NiemaMovassat': 'Linke', 'jankortemdb': 'Linke', 'DietmarBartsch': 'Linke',\n",
    "                          'GregorGysi': 'Linke', 'SevimDagdelen': 'Linke', 'Alice_Weidel': 'AFD',\n",
    "                          'Beatrix_vStorch': 'AFD', 'JoanaCotar': 'AFD', 'StBrandner': 'AFD',\n",
    "                          'Tino_Chrupalla': 'AFD', 'GtzFrmming': 'AFD', 'Leif_Erik_Holm': 'AFD',\n",
    "                          'ABaerbockArchiv':\"Grüne\"\n",
    "                          }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def retrieve_user_tweets(twitter_name, party):\n",
    "    tweets_list = []\n",
    "    search_string = \"from:\" + twitter_name\n",
    "    for tweet in tqdm((sntwitter.TwitterSearchScraper(search_string).get_items())):\n",
    "        tweets_list.append([tweet.date, tweet.id, tweet.content, tweet.user.username, tweet.user.displayname,\n",
    "                                tweet.replyCount, tweet.retweetCount, tweet.likeCount])\n",
    "    tweets_df = pd.DataFrame(tweets_list, columns=['datetime', 'tweet_id', 'text', 'username', 'name',\n",
    "                                                   'reply_count', 'retweet_count', 'like_count'])\n",
    "    tweets_df[\"party\"] = party\n",
    "    return tweets_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2.3 Scrape twitter data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We define a for loop applying our defined function and appending the generated dataframe to one big corpus for the Twitter data. After scraping the tweets for every politician, we ended up with data frames that contained date, id, text, username, Twitter name, reply count, retweet count and like count of each individual tweet. We combined those data frames to one big corpus containing all the tweets by concatenating the data frames. With this corpus of around 300000 tweets, we could start preprocessing and analyzing the tweets."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# twitter_name_tweets_list = []\n",
    "# for key in twitter_name_party_dic:\n",
    "#     print(key)\n",
    "#     twitter_name_tweets_list.append(retrieve_user_tweets(key, twitter_name_party_dic[key]))\n",
    "# tweets = pd.concat(twitter_name_tweets_list, ignore_index=True)\n",
    "# tweets.to_csv(\"../data/raw/tweets_scraped.csv\", index = False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3.3 Retrieve plenary proceedings data (Jakob)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this section, we retrieve the plenary protocol data from the 19. German Bundestag. As the data is publicly available, they can be downloaded from the official [website](https://www.bundestag.de/services/opendata). Currently, the format of the files is not very convenient for automatic analysis, which is why the researcher from [open discourse](https://opendiscourse.de) published a preprocessed version of the plenary protocols ([Richter et al., 2020](https://github.com/open-discourse/open-discourse)). We use their data, as setting up our preprocessing pipeline would be very time-intensive and out of scope for this work. We use their provided [Docker container](https://github.com/open-discourse/open-discourse) to set up the database. We then query the needed data and export them to a csv file."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.3.1 Setup local database"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To use the database, we set up the Docker container from [open discourse](https://open-discourse.github.io/open-discourse-documentation/1.0.0/run-the-database-locally.html#use-the-database). Before this, we have to download and set up Docker according to these [instructions](https://www.docker.com/products/docker-desktop). After this step, one can launch Docker and proceed."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define connection details\n",
    "con_details = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"database\": \"next\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"postgres\",\n",
    "    \"port\": \"5432\"}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Navigate to Docker container\n",
    "# Uncomment if one wants to set up the Docker container\n",
    "# os.system(\"cd ..\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Login to Github for Docker access\n",
    "# Uncomment if one wants to set up the Docker container\n",
    "# os.system(\"docker login docker.pkg.github.com\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# (Only on the first run) download the Docker container\n",
    "# Uncomment if one wants to set up the Docker container\n",
    "# os.system(\"docker pull docker.pkg.github.com/open-discourse/open-discourse/database:latest\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Start and run the database in the Docker container\n",
    "# Uncomment if one wants to set up the Docker container\n",
    "# os.system(\"docker run --env POSTGRES_USER=postgres --env POSTGRES_DB=postgres --env POSTGRES_PASSWORD=postgres -p 5432:5432 -d docker.pkg.github.com/open-discourse/open-discourse/database\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.3.2 Retrieve plenary proceedings from database"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After setting up the PostgreSQL database, we can now query the required data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define query\n",
    "query = \"\"\"SELECT * from open_discourse.speeches WHERE electoral_term = 19\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create connection\n",
    "# Uncomment if one wants to query the database\n",
    "# con = psycopg2.connect(**con_details) # If this fails, repeat execution of the cell.\n",
    "# cur = con.cursor()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Execute query\n",
    "# Uncomment if one wants to query the database\n",
    "# cur.execute(query)\n",
    "# rows = cur.fetchall()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Transform results in dataframe\n",
    "# Uncomment if one made a new query to the database\n",
    "# speeches_retrieved = pd.DataFrame(rows)\n",
    "# speeches_retrieved.columns = [\"id\", \"session\", \"electoral_term\", \"first_name\", \"last_name\", \"politician_id\", \"text\",\n",
    "#                       \"fraction_id\", \"document_url\", \"position_short\", \"position_long\", \"date\",\n",
    "#                       \"search_speech_content\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Export resullts to csv\n",
    "# Uncomment if one made a new query to the database\n",
    "# speeches_retrieved.to_csv(\"../data/raw/speeches_retrieved.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We save the retrieved data as a CSV file and can use it now for further processing."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3.3 Data Exploration"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.3.1 Tweets exploration (Stjepan)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Import data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load tweets data\n",
    "tweets_scraped = pd.read_csv(\"../data/raw/tweets_scraped.csv\", low_memory=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Check data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tweets_scraped.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tweets_scraped.tail()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tweets_scraped.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tweets_scraped.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Drop missing data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can drop all records with missing data, as we cannot use these records for our analysis."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Drop missing data\n",
    "tweets_scraped.dropna(inplace = True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Clean names"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We harmonise the names in the tweets and speeches data for better comparability."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create twitter username to real name dictionary\n",
    "usernames_to_fullname = {'rbrinkhaus': 'Ralph Brinkhaus', 'groehe': 'Hermann Gröhe',\n",
    "                         'NadineSchoen': 'Nadine Schön', 'n_roettgen': 'Norbert Röttgen',\n",
    "                         'peteraltmaier': 'Peter Altmaier', 'jensspahn': 'Jens Spahn',\n",
    "                         'MatthiasHauer': 'Matthias Hauer', 'c_lindner': 'Christian Lindner',\n",
    "                         'MarcoBuschmann': 'Marco Buschmann', 'starkwatzinger': 'Bettina Stark-Watzinger',\n",
    "                         'Lambsdorff': 'Alexander Graf Lambsdorff', 'johannesvogel': 'Johannes Vogel',\n",
    "                         'KonstantinKuhle': 'Konstantin Kuhle', 'MAStrackZi': 'Marie-Agnes Strack-Zimmermann',\n",
    "                         'larsklingbeil': 'Lars Klingbeil', 'EskenSaskia': 'Saskia Esken',\n",
    "                         'hubertus_heil': 'Hubertus Heil', 'HeikoMaas': 'Heiko Maas',\n",
    "                         'MartinSchulz': 'Martin Schulz', 'KarambaDiaby': 'Karamba Diaby',\n",
    "                         'Karl_Lauterbach': 'Karl Lauterbach', 'SteffiLemke': 'Steffi Lemke',\n",
    "                         'cem_oezdemir': 'Cem Özdemir', 'GoeringEckardt': 'Katrin Göring-Eckardt',\n",
    "                         'KonstantinNotz': 'Konstantin von Notz', '6': 'Konstantin von Notz',\n",
    "                         'BriHasselmann': 'Britta Haßelmann', 'svenlehmann': 'Sven Lehmann',\n",
    "                         'ABaerbock': 'Annalena Baerbock', 'ABaerbockArchiv': 'Annalena Baerbock',\n",
    "                         'SWagenknecht': 'Sahra Wagenknecht', 'b_riexinger': 'Bernd Riexinger',\n",
    "                         'NiemaMovassat': 'Niema Movassat', 'jankortemdb': 'Jan Korte',\n",
    "                         'DietmarBartsch': 'Dietmar Bartsch', 'GregorGysi': 'Gregor Gysi',\n",
    "                         'SevimDagdelen': 'Sevim Dağdelen', 'Alice_Weidel': 'Alice Weidel',\n",
    "                         'Beatrix_vStorch': 'Beatrix von Storch', 'JoanaCotar': 'Joana Cotar',\n",
    "                         'StBrandner': 'Stephan Brandner', 'Tino_Chrupalla': 'Tino Chrupalla',\n",
    "                         'GtzFrmming': 'Götz Frömming', '3': 'Götz Frömming', 'Leif_Erik_Holm': 'Leif-Erik Holm'}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Add full name\n",
    "tweets_scraped[\"full_name\"] = tweets_scraped.username.replace(usernames_to_fullname)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Check time data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Add normalized date\n",
    "tweets_scraped[\"date\"] = pd.to_datetime(tweets_scraped[\"datetime\"], format = \"%Y-%m-%d\").dt.date"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tweets_scraped.date.min()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tweets_scraped.date.max()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Tweet number per time\n",
    "tweets_scraped.groupby('date')['tweet_id'].size().plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now drop all data not represented in the speeches dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Drop unneded data\n",
    "tweets_subset = tweets_scraped[np.logical_and(tweets_scraped.date >= pd.Timestamp(\"24.10.2017\"), tweets_scraped.date <= pd.Timestamp(\"07.05.2021\"))]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Checkt party distribution"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "When checking the distribution of tweets per party, we can see differences, but they do not significantly alter our results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Tweets per party\n",
    "tweets_subset.groupby(\"party\").size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Check politician distribution"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We see significant differences between the number of tweets per politician ranging from nearly 29665 to 658. We have to consider this in our work."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Tweets per politican\n",
    "tweets_scraped.groupby('full_name')['tweet_id'].size().sort_values().plot(kind='bar')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We see an increasing trend of tweets per day. This trend is caused by two new parties entering the Bundestag in 2017."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Check text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We check the texts of the tweets with a word cloud. We can infer the need for data preprocessing from a first visualisation analysis."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a word cloud\n",
    "long_string_tweets = ' '.join(tweets_scraped[\"text\"].tolist())\n",
    "wordcloud_tweets = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue')\n",
    "wordcloud_tweets.generate(long_string_tweets)\n",
    "wordcloud_tweets.to_image()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a counter object\n",
    "counter_tweets = Counter(long_string_tweets.split())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check the most common words\n",
    "counter_tweets.most_common(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can identify the need for a stopword removal."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Drop unneeded columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Drop unneeded columns\n",
    "tweets_subset.drop(['datetime', 'tweet_id', 'username','name', 'reply_count'], axis = 1, inplace = True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Export data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tweets_subset.to_csv(\"../data/interim/tweets_explored.csv\", index = False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.3.2 Explore speeches of politicians (Jakob)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We overview the retrieved data in the first analysis step and do the first simple preprocessing tasks."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3.2.1 Import data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load tweets data\n",
    "# Comment out if one retrieves the data from scratch\n",
    "speeches_retrieved = pd.read_csv(\"../data/raw/speeches_retrieved.csv\", low_memory=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3.2.2 Check data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We use standard steps of data exploration to get an overview of the retrieved data, including data types and missing values."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "speeches_retrieved.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "speeches_retrieved.tail()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "speeches_retrieved.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Based on the first overview of the data, we can identify different variables that we have to deep dive into to understand the data quality better and prepare the first processing steps."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3.2.3 Drop missing data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can drop all records with missing speech content, as we cannot use these records for our analysis."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Drop missing data\n",
    "speeches_retrieved.dropna(subset = [\"text\"], inplace = True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3.2.4 Clean names"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We harmonise the politicians' names in the tweets and speeches data for better comparability."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Add full name of politicians\n",
    "speeches_retrieved[\"full_name\"] = speeches_retrieved[\"first_name\"] + \" \" + speeches_retrieved[\"last_name\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Subset to the selected politicians\n",
    "speeches_subset = speeches_retrieved[speeches_retrieved.full_name.isin(tweets_subset.full_name.unique())]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Speeches per politican\n",
    "speeches_subset.groupby('full_name')['id'].size().sort_values().plot(kind='bar')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are significant differences between the number of speeches per politician ranging from 252 to 5. We have to consider this in the interpretation of our results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3.2.5 Check time data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To analyse the topic per time, we need to have the data in a pandas dateformat. Additionally, we control the time with retrieved speeches."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Add normalized date\n",
    "speeches_subset[\"date\"] = pd.to_datetime(speeches_subset[\"date\"], format = \"%Y-%m-%d\").dt.date"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Find first day with speeches\n",
    "speeches_subset.date.min()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Find last day with speeches\n",
    "speeches_subset.date.max()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Speech number per time\n",
    "speeches_subset.groupby('date')['id'].size().plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We see some patterns in the time series. However, there are no significant gaps in the observed time frame."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3.2.6 Check party distribution"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To control the distributions of tweets per party, we assign the author's party to each speech."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Dictionary assigning parties to the politicians\n",
    "fullname_to_party = {'Ralph Brinkhaus': 'CDU', 'Hermann Gröhe': 'CDU', 'Nadine Schön': 'CDU',\n",
    "                     'Norbert Röttgen': 'CDU', 'Peter Altmaier': 'CDU', 'Jens Spahn': 'CDU',\n",
    "                     'Matthias Hauer': 'CDU', 'Christian Lindner': 'FDP', 'Marco Buschmann': 'FDP',\n",
    "                     'Bettina Stark-Watzinger': 'FDP', 'Alexander Graf Lambsdorff': 'FDP', 'Johannes Vogel': 'FDP',\n",
    "                     'Konstantin Kuhle': 'FDP', 'Marie-Agnes Strack-Zimmermann': 'FDP', 'Lars Klingbeil': 'SPD',\n",
    "                     'Saskia Esken': 'SPD', 'Hubertus Heil': 'SPD', 'Heiko Maas': 'SPD', 'Martin Schulz': 'SPD',\n",
    "                     'Karamba Diaby': 'SPD', 'Karl Lauterbach': 'SPD', 'Steffi Lemke': 'Grüne',\n",
    "                     'Cem Özdemir': 'Grüne', 'Katrin Göring-Eckardt': 'Grüne', 'Konstantin von Notz': 'Grüne',\n",
    "                     'Britta Haßelmann': 'Grüne', 'Sven Lehmann': 'Grüne', 'Annalena Baerbock': 'Grüne',\n",
    "                     'Sahra Wagenknecht': 'Linke', 'Bernd Riexinger': 'Linke', 'Niema Movassat': 'Linke',\n",
    "                     'Jan Korte': 'Linke', 'Dietmar Bartsch': 'Linke', 'Gregor Gysi': 'Linke',\n",
    "                     'Sevim Dağdelen': 'Linke', 'Alice Weidel': 'AFD', 'Beatrix von Storch': 'AFD',\n",
    "                     'Joana Cotar': 'AFD', 'Stephan Brandner': 'AFD', 'Tino Chrupalla': 'AFD',\n",
    "                     'Götz Frömming': 'AFD', 'Leif-Erik Holm': 'AFD'}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Assign party\n",
    "speeches_subset[\"party\"] = speeches_subset.full_name.replace(fullname_to_party)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Speeches per party\n",
    "speeches_subset.groupby(\"party\").size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "When checking the distribution of speeches per party, we see differences, but we do not expect them to alter our results significantly."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3.2.7 Check text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We check the texts of the tweets with a word cloud. We can infer the need for data preprocessing from a first visualisation analysis."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a word cloud\n",
    "long_string_speeches = ' '.join(speeches_subset[\"text\"].tolist())\n",
    "wordcloud_speeches = WordCloud(background_color=\"white\", max_words=5000, contour_width=3,\n",
    "                               contour_color='steelblue')\n",
    "wordcloud_speeches.generate(long_string_speeches)\n",
    "wordcloud_speeches.to_image()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a counter object\n",
    "speeches_counter = Counter(long_string_speeches.split())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check the most common words\n",
    "speeches_counter.most_common(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There is a clear need for extensive stopword removal to reduce noise in the topic and sentiment analysis."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3.2.8 Drop unneeded columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Drop unneeded columns\n",
    "speeches_subset.drop(['id', 'session', 'electoral_term', 'first_name', 'last_name', 'politician_id',\n",
    "                      'fraction_id', 'document_url', 'position_short', 'position_long', 'search_speech_content'],\n",
    "                     axis = 1, inplace = True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3.2.9 Export data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "speeches_subset.to_csv(\"../data/interim/speeches_explored.csv\", index = False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This section explored the speeches dataset and controlled the data quality of different essential variables. The data quality is satisfactory, except for a highly skewed distribution of politicians' speeches."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3.4 Data preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.4.1 Prepare spacy pipelines (Jakob)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the last section, we identified the need for extensive preprocessing. We build a flexible spacy pipeline structure that quickly adds or removes different preprocessing steps. We base our model on the pretrained [spacy pipeline](https://spacy.io/models/de)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@Language.component(\"Remove non alphabetic words\")\n",
    "def remove_non_alpha(doc):\n",
    "    return [token for token in doc if token.is_alpha]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We identified the need to remove non-German text, as they reduce the quality of our topic and sentiment models. We use a language detector and an additional component that only removes sentences of other languages."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@Language.factory(\"Detect languages\")\n",
    "def create_language_detector(nlp, name):\n",
    "    return LanguageDetector(language_detection_function=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@Language.component(\"Keep only German documents\")\n",
    "def remove_non_german(doc):\n",
    "    res = [sent for sent in doc.sents if sent._.language[\"language\"] == \"de\"]\n",
    "    if res:\n",
    "        return [token for sent in res for token in sent]\n",
    "    else:\n",
    "        return Doc(Vocab([]), words=[], spaces=[])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@Language.component(\"Remove stopwords\")\n",
    "def remove_stopwords(doc):\n",
    "    return [token for token in doc if not token.is_stop]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We lemmatise the resulting tokens to keep the semantic meaning of the resulting words."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@Language.component(\"Lemmatize text\")\n",
    "def lemmatize_text(doc):\n",
    "    return [token.lemma_ for token in doc]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@Language.component(\"Lowercase Text\")\n",
    "def lowercase(doc):\n",
    "    return [token.lower() for token in doc]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.4.2 Topic modeling preprocessing (Jakob)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We do not use all pre-trained pipeline elements and therefore exclude them. In the next step, we will add additional needed previous defined components."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Exclude not needed pipeline elements\n",
    "pipeline_exclude = ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'ner', 'morphologizer']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###  3.4.2.1 Tweets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This subsection defines a pipeline to preprocess the Twitter data and execute the pipeline."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import data\n",
    "tweets_explored = pd.read_csv(\"../data/interim/tweets_explored.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create spacy pipeline\n",
    "nlp_tweets = spacy.load('de_core_news_sm', exclude=pipeline_exclude)\n",
    "nlp_tweets.Defaults.stop_words |= {\"amp\", \"rt\"}\n",
    "\n",
    "# Add needed pipeline components\n",
    "nlp_tweets.add_pipe(\"sentencizer\", last=True)\n",
    "nlp_tweets.add_pipe(\"Detect languages\", name='Detect languages', last=True)\n",
    "nlp_tweets.add_pipe(\"Keep only German documents\", name='Keep only German documents', last=True)\n",
    "nlp_tweets.add_pipe(\"Remove non alphabetic words\", name=\"Remove non alphabetic words\", last=True)\n",
    "nlp_tweets.add_pipe(\"Remove stopwords\", name=\"Remove stopwords\", last=True)\n",
    "nlp_tweets.add_pipe(\"Lemmatize text\", name=\"Lemmatize text\", last=True)\n",
    "nlp_tweets.add_pipe(\"Lowercase Text\", name=\"Lowercase Text\", last=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Apply pipeline to text\n",
    "# Uncomment if one wants to update the preprocessing of the data\n",
    "# tweets_explored[\"text_preprocessed\"] = tweets_explored.text.progress_apply(nlp_tweets)\n",
    "# This takes approximately one hour"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Add sentence structure\n",
    "# Uncomment if one wants to update the preprocessing of the data\n",
    "# tweets_explored[\"text_preprocessed_sentence\"] = tweets_explored[\"text_preprocessed\"].progress_apply(\n",
    "#    lambda x: \" \".join(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Subset needed data\n",
    "# Uncomment if one wants to update the preprocessing of the data\n",
    "# tweets_preprocessed = tweets_explored[[\"full_name\", \"date\", \"party\", \"text\", \"text_preprocessed\",\n",
    "#                                       \"text_preprocessed_sentence\", 'retweet_count', 'like_count']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Drop empty texts\n",
    "# Uncomment if one wants to update the preprocessing of the data\n",
    "# tweets_preprocessed.replace('', np.NaN, inplace=True)\n",
    "# tweets_preprocessed.dropna(inplace=True)\n",
    "# tweets_preprocessed.reset_index(drop = True, inplace = True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save data as pickle file\n",
    "# Uncomment if one wants to update the preprocessing of the data\n",
    "# pickle.dump(tweets_preprocessed, open(\"../data/processed/tweets_processed.p\", \"wb\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now can use the resulting file to train a topic model for the tweets dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4.2.2 Speeches"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This subsection defines a pipeline for preprocessing the speeches data and executing the pipeline."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import data\n",
    "speeches_explored = pd.read_csv(\"../data/interim/speeches_explored.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create spacy pipeline\n",
    "nlp_speeches = spacy.load('de_core_news_sm', exclude=pipeline_exclude)\n",
    "\n",
    "# Add needed pipeline components\n",
    "nlp_speeches.add_pipe('sentencizer', last=True)\n",
    "nlp_speeches.add_pipe(\"Detect languages\", name='Detect languages', last=True)\n",
    "nlp_speeches.add_pipe(\"Keep only German documents\", name='Keep only German documents', last=True)\n",
    "nlp_speeches.add_pipe(\"Remove non alphabetic words\", name=\"Remove non alphabetic words\", last=True)\n",
    "nlp_speeches.add_pipe(\"Remove stopwords\", name=\"Remove stopwords\", last=True)\n",
    "nlp_speeches.add_pipe(\"Lemmatize text\", name=\"Lemmatize text\", last=True)\n",
    "nlp_speeches.add_pipe(\"Lowercase Text\", name=\"Lowercase Text\", last=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Apply pipeline to text\n",
    "# Uncomment if one wants to update the preprocessing of the data\n",
    "# speeches_explored[\"text_preprocessed\"] = speeches_explored.text.progress_apply(nlp_speeches)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Add sentence structure\n",
    "# Uncomment if one wants to update the preprocessing of the data\n",
    "# speeches_explored[\"text_preprocessed_sentence\"] = speeches_explored[\"text_preprocessed\"].progress_apply(\n",
    "#    lambda x: \" \".join(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Subset needed data\n",
    "# Uncomment if one wants to update the preprocessing of the data\n",
    "# speeches_preprocessed = speeches_explored[[\"full_name\", \"date\", \"party\", \"text\",\n",
    "#                                           \"text_preprocessed\", \"text_preprocessed_sentence\"]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We identified the need to remove frequent words for topic modelling. Many words come from greeting phrases (Sehr, geehrte, Frauen, Herren) that do not have semantic relevance for our analyses but interfere with the model quality based on their frequency."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define function for removing frequent words\n",
    "def remove_frequent_words(words_list, most_frequent_words):\n",
    "    return [word for word in words_list if word not in most_frequent_words]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Additional preprocessing for Bertopic model\n",
    "# Uncomment if one wants to update the preprocessing of the data\n",
    "# long_string_speeches= ' '.join(speeches_preprocessed.text_preprocessed_sentence.tolist())\n",
    "# counter_speeches = Counter(long_string_speeches.split())\n",
    "# most_frequent_words = []\n",
    "# for item in counter_speeches.most_common(200):\n",
    "#    most_frequent_words.append(item[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Add columns with preprocessed text and removed frequent words\n",
    "# Uncomment if one wants to update the preprocessing of the data\n",
    "# speeches_preprocessed[\"text_preprocessed_infrequent\"] = speeches_preprocessed.text_preprocessed.progress_apply(remove_frequent_words,most_frequent_words = most_frequent_words)\n",
    "# speeches_preprocessed[\"text_preprocessed_infrequent_sentence\"] = speeches_preprocessed[\"text_preprocessed_infrequent\"].progress_apply(lambda x: \" \".join(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Drop empty texts\n",
    "# Uncomment if one wants to update the preprocessing of the data\n",
    "# speeches_preprocessed.replace('', np.NaN, inplace=True)\n",
    "# speeches_preprocessed.dropna(inplace=True)\n",
    "# speeches_preprocessed.reset_index(drop = True, inplace = True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save data as pickle file\n",
    "# Uncomment if one wants to update the preprocessing of the data\n",
    "# pickle.dump(speeches_preprocessed, open(\"../data/processed/speeches_processed.p\", \"wb\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now can use the resulting file to train a topic model for the speeches dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.4.3 Sentiment analysis preprocessing (Stjepan)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Needs to be added**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3.5 Topic Modeling (Jakob)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We perform a topic modelling to understand better the differences in politicians' communication on Twitter and the Bundestag. For this, we test three different approaches before we choose the best performing as our final model. We apply hyperparameter tuning if applicable but omit classic train test split validation. We are analysing the validity of the topic model in the Results section."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.5.1 Latent Dirichlet Allocation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Latent Dirichlet Allocation (LDA) constitutes a state of the art approach ([Asmussen & Møller, 2019](https://doi.org/10.1186/s40537-019-0255-7)) for topic modelling. LDA is an unsupervised machine learning technique that uses generative statistical models to extract topics from a collection of documents ([Blei, Ng, & Jordan, 2003](https://dl.acm.org/doi/10.5555/944919.944937)). The underlying model assigns a probability distribution over the vocabulary of the documents to topics that can be used for topic detection. We will base our choice of the optimal hyperparameter combination on the coherence of the resulting topic model. This decision is based on the discussion in [Dietz (2016)](http://topicmodels.info/ckling/tmt/part4.pdf) and [Röder, Both, & Hinneburg (2015)](https://dl.acm.org/doi/abs/10.1145/2684822.2685324)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.5.1.1 Define hyperparameters for optimization."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We optimise the hyperparameters of the LDA model based on a grid search with the variables topic number (k), the a-priory belief of document-topic distribution (alpha) and the a-priory belief of topic-word distribution (eta) ([Rehurek & Sojka, 2010](https://radimrehurek.com/gensim/models/ldamodel.html)). This hyperparameter optimisation is loosely based on [Kapadia (2019)](https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Topics range\n",
    "min_topics = 10\n",
    "max_topics = 150\n",
    "step_size = 10\n",
    "topics_range = range(min_topics, max_topics, step_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Alpha parameter\n",
    "alpha = list(np.arange(0.01, 1, 0.3)) #\n",
    "alpha.append('symmetric')\n",
    "alpha.append('asymmetric')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Beta parameter\n",
    "beta = list(np.arange(0.01, 1, 0.3))\n",
    "beta.append('symmetric')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function for calculating coherence values of specific hyperparamter combinations\n",
    "def compute_lda_coherence_values(corpus, text, id2word, k, a, b):\n",
    "    lda_model = LdaMulticore(corpus=corpus,\n",
    "                             id2word=id2word,\n",
    "                             num_topics=k,\n",
    "                             random_state=42,\n",
    "                             alpha=a,\n",
    "                             eta=b)\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=text, dictionary=id2word, coherence='c_v')\n",
    "    return coherence_model_lda.get_coherence()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function for executing a hyperparameter optimization\n",
    "def hyperparameter_lda(data_preprocessed, title, topics_range, alpha, beta):\n",
    "    id2word = corpora.Dictionary(data_preprocessed.text_preprocessed.to_list())\n",
    "    # These hyperparameter could also be trialed in an extend scope\n",
    "    id2word.filter_extremes(no_below=10, no_above=0.1)\n",
    "    texts = data_preprocessed.text_preprocessed.to_list()\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "    model_results = {'Topics': [],\n",
    "                     'Alpha': [],\n",
    "                     'Beta': [],\n",
    "                     'Coherence': []\n",
    "                    }\n",
    "    for k in tqdm(topics_range):\n",
    "        print(\"Number of topics:\" + str(k))\n",
    "        for a in tqdm(alpha):\n",
    "            print(\"Alpha value:\" + str(a))\n",
    "            for b in tqdm(beta):\n",
    "                print(\"Beta value:\" + str(b))\n",
    "                cv = compute_lda_coherence_values(corpus=corpus,text = texts,\n",
    "                                              id2word=id2word, k=k, a=a, b=b)\n",
    "                model_results['Topics'].append(k)\n",
    "                model_results['Alpha'].append(a)\n",
    "                model_results['Beta'].append(b)\n",
    "                model_results['Coherence'].append(cv)\n",
    "    results_df = pd.DataFrame(model_results)\n",
    "    results_df.to_csv('../data/processed/lda_tuning_results_' + title + '.csv', index=False)\n",
    "    return results_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.5.1.1 Hyperparameter optimization LDA for tweets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load data\n",
    "tweets_processed_lda = pickle.load(open(\"../data/processed/tweets_processed.p\", \"rb\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Hyperparameter optimization\n",
    "# Uncomment if one wants to repeat the hyperparameter optimization\n",
    "# hyperparameter_lda_tweets = hyperparameter_lda(tweets_processed_lda, \"tweets\", topics_range, alpha, beta)\n",
    "# Takes approximately one hour of runtime"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save hyperparameter\n",
    "# Uncomment if one wants to repeat the hyperparameter optimization\n",
    "# hyperparameter_lda_tweets.to_csv('../data/processed/lda_tuning_results_tweets.csv', index = False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.5.1.2 Calculate best model LDA for tweets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We compute the best LDA model for the tweets dataset based on the hyperparameter optimisation from the last subsection."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load data\n",
    "lda_tuning_results_tweets = pd.read_csv('../data/processed/lda_tuning_results_tweets.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Prepare corpus\n",
    "id2word_tweets_lda = corpora.Dictionary(tweets_processed_lda.text_preprocessed.to_list())\n",
    "id2word_tweets_lda.filter_extremes(no_below=5, no_above=0.1)\n",
    "texts_tweets_lda = tweets_processed_lda.text_preprocessed.to_list()\n",
    "corpus_tweets_lda = [id2word_tweets_lda.doc2bow(text) for text in texts_tweets_lda]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Retrieve optimal hyperparameter\n",
    "k_optimal_lda_tweets = int(lda_tuning_results_tweets.sort_values(\"Coherence\", ascending = False).reset_index(drop = True).Topics[0])\n",
    "try:\n",
    "    a_optimal_lda_tweets = float(lda_tuning_results_tweets.sort_values(\"Coherence\", ascending = False).reset_index(drop = True).Alpha[0])\n",
    "except ValueError:\n",
    "    a_optimal_lda_tweets = lda_tuning_results_tweets.sort_values(\"Coherence\", ascending = False).reset_index(drop = True).Alpha[0]\n",
    "try:\n",
    "    b_optimal_lda_tweets = float(lda_tuning_results_tweets.sort_values(\"Coherence\", ascending = False).reset_index(drop = True).Beta[0])\n",
    "except ValueError:\n",
    "    b_optimal_lda_tweets = lda_tuning_results_tweets.sort_values(\"Coherence\", ascending = False).reset_index(drop = True).Beta[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train model\n",
    "lda_model_tweets = LdaMulticore(corpus=corpus_tweets_lda,\n",
    "                                 id2word=id2word_tweets_lda,\n",
    "                                 num_topics=k_optimal_lda_tweets,\n",
    "                                 random_state=42,\n",
    "                                 alpha=a_optimal_lda_tweets,\n",
    "                                 eta=b_optimal_lda_tweets)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate final coherence value\n",
    "coherence_model_lda_tweets = CoherenceModel(model=lda_model_tweets, texts=texts_tweets_lda, dictionary=id2word_tweets_lda, coherence='c_v')\n",
    "coherence_lda_tweets = coherence_model_lda_tweets.get_coherence()\n",
    "print(\"The final model coherence of the LDA for Tweets is: \" + str(round(coherence_lda_tweets,2)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visually inspect result\n",
    "lda_vis_tweets = pyLDAvis.gensim_models.prepare(lda_model_tweets, corpus_tweets_lda, id2word_tweets_lda)\n",
    "lda_vis_tweets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We use the coherence value and topic visualisation to evaluate the model. The model has a good coherence score, but the visual inspection shows not easily interpretable topics. Based on this, we cannot infer a high model quality."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.5.1.3 Hyperparameter optimization LDA for speeches"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load data\n",
    "speeches_processed_lda = pickle.load(open( \"../data/processed/speeches_processed.p\", \"rb\" ))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Hyperparameter optimization\n",
    "# Uncomment if one wants to repeat the hyperparameter optimization\n",
    "# hyperparameter_lda_speeches = hyperparameter_lda(speeches_processed_lda, \"tweets\", topics_range, alpha, beta)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save hyperparameter\n",
    "# Uncomment if one wants to repeat the hyperparameter optimization\n",
    "# hyperparameter_lda_speeches.to_csv('../data/processed/lda_tuning_results_speeches.csv', index = False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.5.1.4 Calculate best model LDA for speeches"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We compute the best LDA model for the speeches dataset based on the hyperparameter optimisation from the last subsection."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load data\n",
    "lda_tuning_results_speeches = pd.read_csv('../data/processed/lda_tuning_results_speeches.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Prepare corpus\n",
    "id2word_speeches_lda = corpora.Dictionary(tweets_processed_lda.text_preprocessed.to_list())\n",
    "id2word_speeches_lda.filter_extremes(no_below=5, no_above=0.1)\n",
    "texts_speeches_lda = tweets_processed_lda.text_preprocessed.to_list()\n",
    "corpus_speeches_lda = [id2word_speeches_lda.doc2bow(text) for text in texts_speeches_lda]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Retrieve optimal hyperparameter\n",
    "k_optimal_lda_speeches = int(lda_tuning_results_speeches.sort_values(\"Coherence\", ascending = False).reset_index(drop = True).Topics[0])\n",
    "try:\n",
    "    a_optimal_lda_speeches = float(lda_tuning_results_speeches.sort_values(\"Coherence\", ascending = False).reset_index(drop = True).Alpha[0])\n",
    "except ValueError:\n",
    "    a_optimal_lda_speeches = lda_tuning_results_speeches.sort_values(\"Coherence\", ascending = False).reset_index(drop = True).Alpha[0]\n",
    "try:\n",
    "    b_optimal_lda_speeches = float(lda_tuning_results_speeches.sort_values(\"Coherence\", ascending = False).reset_index(drop = True).Beta[0])\n",
    "except ValueError:\n",
    "    b_optimal_lda_speeches = lda_tuning_results_speeches.sort_values(\"Coherence\", ascending = False).reset_index(drop = True).Beta[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train model\n",
    "lda_model_speeches = LdaMulticore(corpus=corpus_speeches_lda,\n",
    "                                 id2word=id2word_speeches_lda,\n",
    "                                 num_topics=k_optimal_lda_speeches,\n",
    "                                 random_state=42,\n",
    "                                 alpha=a_optimal_lda_speeches,\n",
    "                                 eta=b_optimal_lda_speeches)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate final coherence value\n",
    "coherence_model_lda_speeches = CoherenceModel(model=lda_model_speeches, texts=texts_speeches_lda, dictionary=id2word_speeches_lda,\n",
    "                                                    coherence='c_v')\n",
    "coherence_lda_speeches = coherence_model_lda_speeches.get_coherence()\n",
    "print(\"The final model coherence of the LDA for Speeches is: \" + str(round(coherence_lda_speeches,2)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visually inspect result\n",
    "lda_vis_speeches = pyLDAvis.gensim_models.prepare(lda_model_speeches, corpus_speeches_lda, id2word_speeches_lda)\n",
    "lda_vis_speeches"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Neither the coherence score nor the visual inspection indicates a high model quality."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.5.2 Non Negative Matrix Factorisation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Another approach for topic modelling we are testing is Non-Negative Matrix Factorisation (NNMF). This technique is another unsupervised machine learning method that factorises a matrix into two matrices that give a less complex representation of the original data ([Wang & Zhang, 2012](https://doi.org/10.1109/TKDE.2012.51)). In our case, we use it to create a document term matrix that helps identify topics of the considered documents."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Define hyperparameters for optimization."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We optimise the hyperparameters of the NNMF model based on a grid search with the variables topic number (k)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function for calculating coherence values of specific hyperparamter combinations\n",
    "def compute_nnmf_coherence_values(corpus, text, id2word, k):\n",
    "    nmf_model = Nmf(\n",
    "        corpus=corpus,\n",
    "        id2word=id2word,\n",
    "        num_topics=k,\n",
    "        random_state=42\n",
    "    )\n",
    "    coherence_model_lda = CoherenceModel(model=nmf_model, texts=text, dictionary=id2word, coherence='c_v')\n",
    "    return coherence_model_lda.get_coherence()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function for executing a hyperparameter optimization\n",
    "def hyperparameter_nnmf(data_preprocessed, title, topics_range):\n",
    "    id2word = corpora.Dictionary(data_preprocessed.text_preprocessed.to_list())\n",
    "    id2word.filter_extremes(no_below=10, no_above=0.1)\n",
    "    texts = data_preprocessed.text_preprocessed.to_list()\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "    model_results = {'Topics': [],\n",
    "                     'Coherence': []\n",
    "                    }\n",
    "    for k in tqdm(topics_range):\n",
    "        print(\"Number of topics:\" + str(k))\n",
    "        cv = compute_nnmf_coherence_values(corpus=corpus,text = texts,\n",
    "                                      id2word=id2word, k=k)\n",
    "        model_results['Topics'].append(k)\n",
    "        model_results['Coherence'].append(cv)\n",
    "    results_df = pd.DataFrame(model_results)\n",
    "    results_df.to_csv('../data/processed/nnmf_tuning_results_' + title + '.csv', index=False)\n",
    "    return results_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.5.2.1 Hyperparameter optimization NNMF for tweets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load data\n",
    "tweets_processed_nnmf = pickle.load(open(\"../data/processed/tweets_processed.p\", \"rb\" ))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Hyperparameter optimization\n",
    "# Uncomment if one wants to repeat the hyperparameter optimization\n",
    "# hyperparameter_nnmf_tweets = hyperparameter_nnmf(tweets_processed_nnmf, \"tweets\", topics_range)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save hyperparameter\n",
    "# Uncomment if one wants to repeat the hyperparameter optimization\n",
    "# hyperparameter_nnmf_tweets.to_csv('../data/processed/nnmf_tuning_results_tweets.csv', index = False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.5.2.2 Calculate best model NNMF for tweets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We compute the best NNMF model for the tweets dataset based on the hyperparameter optimisation from the last subsection."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load data\n",
    "tweets_processed_nnmf = pickle.load(open( \"../data/processed/tweets_processed.p\", \"rb\" ))\n",
    "nnmf_tuning_results_tweets = pd.read_csv('../data/processed/nnmf_tuning_results_tweets.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Prepare corpus\n",
    "id2word_tweets_nnmf = corpora.Dictionary(tweets_processed_nnmf.text_preprocessed.to_list())\n",
    "id2word_tweets_nnmf.filter_extremes(no_below=5, no_above=0.1)\n",
    "texts_tweets_nnmf = tweets_processed_nnmf.text_preprocessed.to_list()\n",
    "corpus_tweets_nnmf = [id2word_tweets_nnmf.doc2bow(text) for text in texts_tweets_nnmf]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "k_optimal_nnmf_tweets = int(lda_tuning_results_tweets.sort_values(\"Coherence\", ascending = False).reset_index(drop = True).Topics[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train model\n",
    "nnmf_model_tweets = Nmf(corpus=corpus_tweets_nnmf,\n",
    "                                 id2word=id2word_tweets_nnmf,\n",
    "                                 num_topics=k_optimal_nnmf_tweets,\n",
    "                                 random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate final coherence value\n",
    "coherence_model_nnmf_tweets = CoherenceModel(model=nnmf_model_tweets, texts=texts_tweets_nnmf, dictionary=id2word_tweets_nnmf,\n",
    "                                                    coherence='c_v')\n",
    "coherence_nnmf_tweets = coherence_model_nnmf_tweets.get_coherence()\n",
    "print(\"The final model coherence of the NNMF for tweets is: \" + str(round(coherence_nnmf_tweets,2)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visually inspect result\n",
    "nnmf_model_tweets.show_topics()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "When analysing the top topic words, we cannot identify comprehensible subjects. Combined with the low coherence score, we can conclude a low model quality."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.5.2.3 Hyperparameter optimization NNMF for speeches"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load data\n",
    "speeches_processed_nnmf = pickle.load(open( \"../data/processed/speeches_processed.p\", \"rb\" ))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Hyperparameter optimization\n",
    "# Uncomment if one wants to repeat the hyperparameter optimization\n",
    "# hyperparameter_nnmf_speeches = hyperparameter_lda(speeches_processed_nnmf, \"tweets\", topics_range, alpha, beta)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save hyperparameter\n",
    "# Uncomment if one wants to repeat the hyperparameter optimization\n",
    "# hyperparameter_nnmf_speeches.to_csv('../data/processed/nnmf_tuning_results_speeches.csv', index = False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.5.2.4 Calculate best model NNMF for speeches"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We compute the best NNMF model for the speeches dataset based on the hyperparameter optimisation from the last subsection."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load data\n",
    "speeches_processed_nnmf = pickle.load(open( \"../data/processed/speeches_processed.p\", \"rb\" ))\n",
    "nnmf_tuning_results_speeches = pd.read_csv('../data/processed/nnmf_tuning_results_speeches.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Prepare corpus\n",
    "id2word_speeches_nnmf = corpora.Dictionary(tweets_processed_nnmf.text_preprocessed.to_list())\n",
    "id2word_speeches_nnmf.filter_extremes(no_below=5, no_above=0.1)\n",
    "texts_speeches_nnmf = tweets_processed_nnmf.text_preprocessed.to_list()\n",
    "corpus_speeches_nnmf = [id2word_speeches_nnmf.doc2bow(text) for text in texts_speeches_nnmf]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "k_optimal_nnmf_speeches = int(lda_tuning_results_speeches.sort_values(\"Coherence\", ascending = False).reset_index(drop = True).Topics[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train model\n",
    "nnmf_model_speeches = Nmf(corpus=corpus_speeches_nnmf,\n",
    "                                 id2word=id2word_speeches_nnmf,\n",
    "                                 num_topics=k_optimal_nnmf_speeches,\n",
    "                                 random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate final coherence value\n",
    "coherence_model_nnmf_speeches = CoherenceModel(model=nnmf_model_speeches, texts=texts_speeches_nnmf, dictionary=id2word_speeches_nnmf,\n",
    "                                                    coherence='c_v')\n",
    "coherence_nnmf_speeches = coherence_model_nnmf_speeches.get_coherence()\n",
    "print(\"The final model coherence of the NNMF for Speeches is: \" + str(round(coherence_nnmf_speeches,2)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visually inspect result\n",
    "nnmf_model_speeches.show_topics()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The coherence of the model is relatively low, and the resulting topics show no consistent them resulting in low model usability."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.5.3 Bertopic"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The last model we apply is BERTopic ([Grootendorst & Reimers, 2021](https://doi.org/10.5281/zenodo.4381785)), which employs the BERT transformers model for creating topic models. BERTopic uses pre-trained BERT models and UMAP and HDBSCAN clustering with a c-TF-IDF embedding and Maximal Marginal Relevance selection. This model architecture is pretty new, and there is not much existing research on the topic. However first results seem promising. Based on the architecture, the model can identify relevant topics in the text and cluster them according to semantic similarity. The model architecture is quite complex, and therefore the runtime of training BERTopic is high. We do not perform hyperparameter optimisation for the BERTopic models, as we have only limited computational power."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_coherence_bert(topic_model, docs, topics):\n",
    "    cleaned_docs = topic_model._preprocess_text(docs)\n",
    "\n",
    "    # Extract vectorizer and tokenizer from BERTopic\n",
    "    vectorizer = topic_model.vectorizer_model\n",
    "    tokenizer = vectorizer.build_tokenizer()\n",
    "\n",
    "    # Extract features for Topic Coherence evaluation\n",
    "    tokens = [tokenizer(doc) for doc in cleaned_docs]\n",
    "    dictionary = corpora.Dictionary(tokens)\n",
    "    corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "    topic_words = [[words for words, _ in topic_model.get_topic(topic)]\n",
    "                   for topic in range(len(set(topics))-1)]\n",
    "\n",
    "    # Evaluate\n",
    "    coherence_model = CoherenceModel(topics=topic_words,\n",
    "                                     texts=tokens,\n",
    "                                     corpus=corpus,\n",
    "                                     dictionary=dictionary,\n",
    "                                     coherence='c_v')\n",
    "    coherence = coherence_model.get_coherence()\n",
    "    return coherence"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def assign_topic(topic_id, topic_model):\n",
    "    return topic_model.get_topic_info(topic_id).Name.values[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.5.3.1 Compute BERTopic model Tweets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load data\n",
    "tweets_processed_bert = pickle.load(open( \"../data/processed/tweets_processed.p\", \"rb\" ))\n",
    "docs_tweets_bert = tweets_processed_bert.text_preprocessed_sentence.tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Prepare topic model\n",
    "topic_model_tweets = BERTopic(language=\"german\", nr_topics=\"auto\", calculate_probabilities = True, verbose = True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compute Bertopic model\n",
    "# Uncomment if one wants to retrain the network\n",
    "# start_time_bert_tweets = datetime.now()\n",
    "# topics_tweets_bert, probs_tweets_bert = topic_model_tweets.fit_transform(docs_tweets_bert)\n",
    "# end_time_bert_tweets = datetime.now()\n",
    "# print('Duration: {}'.format(end_time_bert_tweets - start_time_bert_tweets))\n",
    "# Takes approximately eight hours of runtime"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate coherence\n",
    "# Uncomment if one wants to retrain the network\n",
    "# coherence_bert_tweets = calculate_coherence_bert(topic_model_tweets,docs_tweets_bert, topics_tweets_bert)\n",
    "# coherence_bert_tweets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualise results\n",
    "# Uncomment if one wants to retrain the network\n",
    "# topic_model_tweets.visualize_topics()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We saw too many topics based on the first analysis, so we reduced the number of topics with the inherent reduction logic."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Reduce topics\n",
    "# Uncomment if one wants to retrain the network\n",
    "# topics_tweets_bert_reduced, probs_tweets_bert_reduced = topic_model_tweets.reduce_topics(docs_tweets_bert,\n",
    "#                                                                                         topics_tweets_bert,\n",
    "#                                                                                         probs_tweets_bert,\n",
    "#                                                                                         nr_topics=25)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load model\n",
    "# Comment out if one retrains the model\n",
    "with open('../data/processed/topics_tweets_bert.pickle', 'rb') as handle:\n",
    "    topics_tweets_bert_reduced = pickle.load(handle)\n",
    "topic_model_tweets = BERTopic.load(\"../models/bertopic_tweets\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate coherence reduced\n",
    "coherence_bert_tweets_reduced = calculate_coherence_bert(topic_model_tweets, docs_tweets_bert,\n",
    "                                                         topics_tweets_bert_reduced)\n",
    "print(\"The final model coherence of the BERTopic for Tweets is: \" + str(round(coherence_bert_tweets_reduced,2)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualise results\n",
    "topic_model_tweets.visualize_topics()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The coherence of the model is on a satisfactory level, and the identified topics are interpretable for a human observer. We can infer a high model quality."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Assign results to dataframe\n",
    "tweets_processed_bert[\"topic_id\"] = topics_tweets_bert_reduced\n",
    "tweets_processed_bert[\"topic\"] = tweets_processed_bert.topic_id.progress_apply(assign_topic,                                                                                    topic_model = topic_model_tweets)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save model and results\n",
    "# Uncomment if one wants to retrain the network\n",
    "# topic_model_tweets.save(\"../models/bertopic_tweets\")\n",
    "# with open( \"../data/processed/tweets_processed_bert.pickle\", \"wb\" ) as handle:\n",
    "#    pickle.dump(tweets_processed_bert, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with open('../data/processed/probabilities_tweets_bert.pickle', 'wb') as handle:\n",
    "#     pickle.dump(probs_tweets_bert_reduced, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with open('../data/processed/topics_tweets_bert.pickle', 'wb') as handle:\n",
    "#    pickle.dump(topics_tweets_bert_reduced, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.5.3.2 Compute BERTopic model Speeches"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load data\n",
    "speeches_processed_bert = pickle.load(open( \"../data/processed/speeches_processed.p\", \"rb\" ))\n",
    "docs_speeches_bert = speeches_processed_bert.text_preprocessed_infrequent_sentence.tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Prepare topic model\n",
    "topic_model_speeches = BERTopic(language=\"german\", nr_topics=\"auto\", calculate_probabilities = True,\n",
    "                                verbose = True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compute Bertopic mode\n",
    "# Uncomment if one wants to retrain the network\n",
    "# start_time_bert_speeches = datetime.now()\n",
    "# topics_speeches_bert, probs_speeches_bert = topic_model_speeches.fit_transform(docs_speeches_bert)\n",
    "# end_time_bert_speeches = datetime.now()\n",
    "# print('Duration: {}'.format(end_time_bert_speeches - start_time_bert_speeches))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load model\n",
    "# Comment out if one retrains the model\n",
    "with open('../data/processed/topics_speeches_bert.pickle', 'rb') as handle:\n",
    "    topics_speeches_bert = pickle.load(handle)\n",
    "topic_model_speeches = BERTopic.load(\"../models/bertopic_speeches\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate coherence reduced\n",
    "coherence_bert_speeches = calculate_coherence_bert(topic_model_speeches, docs_speeches_bert,\n",
    "                                                         topics_speeches_bert)\n",
    "print(\"The final model coherence of the BERTopic for speeches is: \" + str(round(coherence_bert_speeches,2)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualise results\n",
    "# Uncomment if one wants to retrain the network\n",
    "topic_model_speeches.visualize_topics()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The model has a comparatively high coherence and also interpretable topics. Therefore we conclude a high model quality."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Assign results to dataframe\n",
    "# Uncomment if one wants to retrain the network\n",
    "# speeches_processed_bert[\"topic_id\"] = topics_speeches_bert\n",
    "# speeches_processed_bert[\"topic\"] = speeches_processed_bert.topic_id.progress_apply(assign_topic,\n",
    "#                                                                                   topic_model = topic_model_speeches)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save model and results\n",
    "# Uncomment if one wants to retrain the network\n",
    "# topic_model_speeches.save(\"../models/bertopic_speeches\")\n",
    "# with open( \"../data/processed/speeches_processed_bert.pickle\", \"wb\" ) as handle:\n",
    "#    pickle.dump(speeches_processed_bert, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with open('../data/processed/probabilities_speeches_bert.pickle', 'wb') as handle:\n",
    "#    pickle.dump(probs_speeches_bert, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with open('../data/processed/topics_speeches_bert.pickle', 'wb') as handle:\n",
    "#    pickle.dump(topics_speeches_bert, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.5.4 Model selection"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the final model selection, we evaluate the model based on the coherence and the visual inspection of the results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"The final model coherence of the LDA for tweets is: \" + str(round(coherence_lda_tweets,2)))\n",
    "print(\"The final model coherence of the NNMF for tweets is: \" + str(round(coherence_nnmf_tweets,2)))\n",
    "print(\"The final model coherence of the BERTopic for tweets is: \" + str(round(coherence_bert_tweets_reduced,2)))\n",
    "print(\"The final model coherence of the LDA for speeches is: \" + str(round(coherence_lda_speeches,2)))\n",
    "print(\"The final model coherence of the NNMF for speeches is: \" + str(round(coherence_nnmf_speeches,2)))\n",
    "print(\"The final model coherence of the BERTopic for speeches is: \" + str(round(coherence_bert_speeches,2)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " NNMF models did not perform very well in terms of coherence, while the LDA model only showed good coherence values for the tweets dataset. BERTopic could perform well for both datasets measured by cohesion. Based on the visual inspection, we saw excellent results for BERTopic and medium results for the other two model types. We decided on the BERTopic model for both datasets to create the final topic model based on these criteria. In the next section, we will analyse the results of BERTopic and validate the selected models based on word and topic intrusion metrics."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3.6 Sentiment Analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With the sentiment analysis we want to look at the question of how the sentiment of politicians from different parties varies from social media to the Bundestag as an audience and make a comparison between female and male politicians in the way of used sentiment. As we used Python for our programming language, we start by importing some useful and commonly used packages. After loading in our preprocessed corpus we were ready to analyze the data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#import packages\n",
    "\n",
    "import pandas as pd\n",
    "from textblob_de import TextBlobDE as TextBlob\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import re\n",
    "import pickle\n",
    "pd.options.mode.chained_assignment = None  # default='warn' based on false positives\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy_langdetect import LanguageDetector\n",
    "from spacy.tokens.doc import Doc\n",
    "from spacy.vocab import Vocab\n",
    "from spacy_sentiws import spaCySentiWS\n",
    "from spacy_sentiws import spaCySentiWS\n",
    "\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "#load in the preprocessed data\n",
    "unpre_data_twitter= pd.read_csv(\"../data/raw/tweets_explored.csv\")\n",
    "unpre_data_speeches= pd.read_csv(\"../data/raw/speeches_explored.csv\")\n",
    "pre_data_twitter= pickle.load(open('../data/processed/tweets_processed.p','rb'))\n",
    "pre_data_speeches= pickle.load(open('../data/processed/speeches_processed.p','rb'))\n",
    "pre_data_twitter.head()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.6.1 Sentiment Analysis with TextBlob"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For our first approach at sentiment analysis, we use the package TextBlob which can be used for preprocessing textual data and provides an API for natural language processing tasks like sentiment analysis. As our corpus was in German language, we needed to use the German version TextBlobDE which has fewer functionalities than its English counterpart but was sufficient for our first sentiment approach. For sentiment analysis it returns the polarity of a given sentence where polarity -1 means very negative and 1 very positive. The scores are generated based on a dictionary approach using a polarity lexicon for German from Clematide and Klenner (Clematide, S; Klenner, M, 2010)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.6.1.1 Sentiment Analysis for Twitter Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First we start of with the analysis of the Twitter data. As we want to look at the different politicians from our corpus individually, we define a for loop going through each politician. To apply TextBlob we first need to take the preprocessed tweets in sentence format. After applying TextBlob we use the function sentiment to generate the polarity scores for the individual tweets. We ignore the second output subjectivity as it has no meaning in this German version of this package. Then we calculate the mean of the polarity for each politician. Furthermore, we counted the number of positive, negative, and neutral tweets for every politician without accounting for how positive or negative they were."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#loop through all the politicians we want to analyze\n",
    "# data=[]\n",
    "# for name in tqdm(['Ralph Brinkhaus','Hermann Gröhe', 'Nadine Schön' ,'Norbert Röttgen' , 'Peter Altmaier' , 'Jens Spahn' , 'Matthias Hauer',\n",
    "#             'Christian Lindner' , 'Marco Buschmann' , 'Bettina Stark-Watzinger', 'Alexander Graf Lambsdorff' , 'Johannes Vogel' , 'Konstantin Kuhle' , 'Marie-Agnes Strack-Zimmermann',\n",
    "#             'Lars Klingbeil' , 'Saskia Esken' , 'Hubertus Heil' , 'Heiko Maas' , 'Martin Schulz' , 'Karamba Diaby' , 'Karl Lauterbach',\n",
    "#             'Steffi Lemke' , 'Cem Özdemir' , 'Katrin Göring-Eckardt' , 'Konstantin von Notz' , 'Britta Haßelmann' , 'Sven Lehmann' , 'Annalena Baerbock',\n",
    "#             'Sahra Wagenknecht' , 'Bernd Riexinger' , 'Niema Movassat' , 'Jan Korte' , 'Dietmar Bartsch' , 'Gregor Gysi' , 'Sevim Dağdelen',\n",
    "#             'Alice Weidel' , 'Beatrix von Storch' , 'Joana Cotar' , 'Stephan Brandner' , 'Tino Chrupalla' , 'Götz Frömming' , 'Leif-Erik Holm']):\n",
    "#     #get tweets from the specific politician\n",
    "#     tweets_analyzing =pre_data_twitter.loc[pre_data_twitter['full_name']==name]\n",
    "#     #create sentiment scores\n",
    "#     blobs=tweets_analyzing['text_preprocessed_sentence'].apply(TextBlob)\n",
    "#     sentiment=[]\n",
    "#     for blob in blobs:\n",
    "#         sentiment.append(blob.sentiment)\n",
    "#     #get the polarity scores\n",
    "#     polarity=[]\n",
    "#     for egg in sentiment:\n",
    "#         polarity.append(egg.polarity)\n",
    "#     #get the mean of the scores\n",
    "#     p_mean = np.mean(polarity)\n",
    "#     #get the number of positive, neutral and negative tweets\n",
    "#     positive_p=0\n",
    "#     neutral_p=0\n",
    "#     negative_p=0\n",
    "#     for item_p in polarity:\n",
    "#         if item_p>0:\n",
    "#             positive_p += 1\n",
    "#         elif item_p<0:\n",
    "#             negative_p += 1\n",
    "#         else:\n",
    "#             neutral_p += 1\n",
    "#     #set up list to secure the values generated\n",
    "#     data.append([name,p_mean,positive_p,neutral_p,negative_p])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ending up with a data frame containing the polarity means and tweet counts for every politician, we had a first overview of the sentiments of their social media presence."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#set up dataframe with all values and save it into a csv file\n",
    "# dataf = pd.DataFrame(data, columns=['Name','Polarity_mean','Num_pos_tweets','Num_neutral_tweets','Num_neg_tweets'])\n",
    "# dataf.to_csv('../data/processed/sentiment_scores_twitter_01.csv')\n",
    "# dataf.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now expand our dataframe with a column containing the polarity score generated by TextBlob. By simply applying the code from our for loop to the whole corpus and appending the generated scores."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#create a polarity column for our dataset\n",
    "# blobs=pre_data_twitter['text_preprocessed_sentence'].progress_apply(TextBlob)\n",
    "# sentiment=[]\n",
    "# for blob in blobs:\n",
    "#     sentiment.append(blob.sentiment)\n",
    "# #get the scores\n",
    "# polarity=[]\n",
    "# for egg in sentiment:\n",
    "#     polarity.append(egg.polarity)\n",
    "# pre_data_twitter['polarity_textblob'] = polarity\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# pickle.dump(pre_data_twitter, open(\"../data/processed/tweets_processed.p\", \"wb\"))\n",
    "#\n",
    "# pre_data_twitter.head()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.6.1.2 Sentiment Analysis Bundestag Speeches"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next up are the Bundestag speeches from the same politicians we analyzed in the step before. Here we take our preprocessed speeches and apply TextBlob in a similar fashion as on the tweets also looping through the politicians individually."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#loop through all the politicians we want to analyze\n",
    "# data=[]\n",
    "# for name in tqdm(['Ralph Brinkhaus','Hermann Gröhe', 'Nadine Schön' ,'Norbert Röttgen' , 'Peter Altmaier' , 'Jens Spahn' , 'Matthias Hauer',\n",
    "#             'Christian Lindner' , 'Marco Buschmann' , 'Bettina Stark-Watzinger', 'Alexander Graf Lambsdorff' , 'Johannes Vogel' , 'Konstantin Kuhle' , 'Marie-Agnes Strack-Zimmermann',\n",
    "#             'Lars Klingbeil' , 'Saskia Esken' , 'Hubertus Heil' , 'Heiko Maas' , 'Martin Schulz' , 'Karamba Diaby' , 'Karl Lauterbach',\n",
    "#             'Steffi Lemke' , 'Cem Özdemir' , 'Katrin Göring-Eckardt' , 'Konstantin von Notz' , 'Britta Haßelmann' , 'Sven Lehmann' , 'Annalena Baerbock',\n",
    "#             'Sahra Wagenknecht' , 'Bernd Riexinger' , 'Niema Movassat' , 'Jan Korte' , 'Dietmar Bartsch' , 'Gregor Gysi' , 'Sevim Dağdelen',\n",
    "#             'Alice Weidel' , 'Beatrix von Storch' , 'Joana Cotar' , 'Stephan Brandner' , 'Tino Chrupalla' , 'Götz Frömming' , 'Leif-Erik Holm']):\n",
    "#     #get speeches from the specific politician\n",
    "#     speeches_analyzing =pre_data_speeches.loc[pre_data_speeches['full_name']==name]\n",
    "#     #create sentiment scores\n",
    "#     blobs=speeches_analyzing['text_preprocessed_sentence'].apply(TextBlob)\n",
    "#     sentiment=[]\n",
    "#     for blob in blobs:\n",
    "#         sentiment.append(blob.sentiment)\n",
    "#     #get the polarity scores\n",
    "#     polarity=[]\n",
    "#     for egg in sentiment:\n",
    "#         polarity.append(egg.polarity)\n",
    "#     #get the mean and of the polarity values\n",
    "#     p_mean = np.mean(polarity)\n",
    "#     #get the number of positive, neutral and negative tweets\n",
    "#     positive_p=0\n",
    "#     neutral_p=0\n",
    "#     negative_p=0\n",
    "#     for item_p in polarity:\n",
    "#         if item_p>0:\n",
    "#             positive_p += 1\n",
    "#         elif item_p<0:\n",
    "#             negative_p += 1\n",
    "#         else:\n",
    "#             neutral_p += 1\n",
    "#     #set up list to secure the values generated\n",
    "#     data.append([name,p_mean,positive_p,neutral_p,negative_p])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Again, we end up with a list containing the sentiment score means and counts of positive, negative, and neutral speeches which we transform into a dataset we can analyze further."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#set up dataframe with all values\n",
    "# dataf = pd.DataFrame(data, columns=['Name','Polarity_mean','Num_pos_speeches','Num_neutral_speeches','Num_neg_speeches'])\n",
    "# dataf.to_csv('../data/processed/sentiment_scores_speeches_01.csv')\n",
    "# dataf.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we also add a column for the sentiment scores to have an overview."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# blobs=pre_data_speeches['text_preprocessed_sentence'].apply(TextBlob)\n",
    "# sentiment=[]\n",
    "# for blob in blobs:\n",
    "#     sentiment.append(blob.sentiment)\n",
    "# #get the scores\n",
    "# polarity=[]\n",
    "# for egg in sentiment:\n",
    "#     polarity.append(egg.polarity)\n",
    "# pre_data_speeches['polarity_textblob'] = polarity\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# pickle.dump(pre_data_speeches, open(\"../data/processed/speeches_processed.p\", \"wb\"))\n",
    "# pre_data_speeches.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.6.2 Sentiment Analysis with SentiWS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As a second approach for sentiment analysis we tried using SentiWS an often used German sentiment dictionary. It also calculates the sentiment of a given sentence with a polarity score from -1 to 1 and has over 3000 base words and over 30000 word forms in its dictionary. Not only does it use adjectives and adverbs but also nouns and verbs to calculate the sentiment score. For the code implementation we could use an extension from the spacy pipeline used in preprocessing. With this spaCySentiWS we can add the application of the dictionary directly into the preprocessing pipeline. Therefore, we write a new preprocessing pipeline which is changed a little from original pipeline to get the sentiment scores of a sentence."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#insert pipeline to add sentiws preprocessing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pre_data_twitter= pd.read_csv(\"../data/raw/tweets_explored.csv\")\n",
    "pre_data_speeches= pd.read_csv(\"../data/raw/speeches_explored.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@Language.component(\"Remove non alphabetic words\")\n",
    "def remove_non_alpha(doc):\n",
    "    return [token for token in doc if token.is_alpha]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@Language.factory(\"Detect languages\")\n",
    "def create_language_detector(nlp, name):\n",
    "    return LanguageDetector(language_detection_function=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@Language.factory(\"Sentiment Appplication\")\n",
    "def create_sentiment_dictionary(nlp, name):\n",
    "    return spaCySentiWS(sentiws_path = \"../data/raw/Sentiment/\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@Language.component(\"Keep only German documents\")\n",
    "def remove_non_german(doc):\n",
    "    res = [sent for sent in doc.sents if sent._.language[\"language\"] == \"de\"]\n",
    "    if res:\n",
    "        return [token for sent in res for token in sent]\n",
    "    else:\n",
    "        return Doc(Vocab([]), words=[], spaces=[])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@Language.component(\"Remove stopwords\")\n",
    "def remove_stopwords(doc):\n",
    "    return [token for token in doc if not token.is_stop]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@Language.component(\"Lemmatize text\")\n",
    "def lemmatize_text(doc):\n",
    "    return [token.lemma_ for token in doc]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@Language.component(\"Lowercase Text\")\n",
    "def lowercase(doc):\n",
    "    return [token.lower() for token in doc]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "emoji_codes = re.compile(\"[\"\n",
    "                         u\"\\U0001F600-\\U0001F64F\"\n",
    "                         u\"\\U0001F300-\\U0001F5FF\"\n",
    "                         u\"\\U0001F680-\\U0001F6FF\"\n",
    "                         u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "                         u\"\\U00002500-\\U00002BEF\"\n",
    "                         u\"\\U00002702-\\U000027B0\"\n",
    "                         u\"\\U00002702-\\U000027B0\"\n",
    "                         u\"\\U000024C2-\\U0001F251\"\n",
    "                         u\"\\U0001f926-\\U0001f937\"\n",
    "                         u\"\\U00010000-\\U0010ffff\"\n",
    "                         u\"\\u2640-\\u2642\"\n",
    "                         u\"\\u2600-\\u2B55\"\n",
    "                         u\"\\u200d\"\n",
    "                         u\"\\u23cf\"\n",
    "                         u\"\\u23e9\"\n",
    "                         u\"\\u231a\"\n",
    "                         u\"\\ufe0f\"\n",
    "                         u\"\\u3030\"\n",
    "                         \"]+\", re.UNICODE)\n",
    "\n",
    "@Language.component(\"Remove emojis\")\n",
    "def remove_emojis(doc):\n",
    "    doc = [token.text for token in doc if not re.match(emoji, token.text)]\n",
    "    doc = ' '.join(doc)\n",
    "    return nlp_twitter.make_doc(doc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@Language.component(\"Remove URLs\")\n",
    "def remove_urls(doc):\n",
    "    doc = [token.text for token in doc if not token.like_url]\n",
    "    doc = ' '.join(doc)\n",
    "    return nlp_twitter.make_doc(doc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@Language.component(\"Remove mentions\")\n",
    "def remove_mentions(doc):\n",
    "    doc = [token.text for token in doc if not re.match(\"@.*\", token.text)]\n",
    "    doc = ' '.join(doc)\n",
    "    return nlp_twitter.make_doc(doc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@Language.component(\"Remove stopwords and punctuation\")\n",
    "def remove_stopwords(doc):\n",
    "    doc = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
    "    return doc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create spacy pipeline\n",
    "nlp_tweets_sentiws = spacy.load('de_core_news_sm')\n",
    "nlp_tweets_sentiws.Defaults.stop_words |= {\"amp\", \"rt\"}\n",
    "\n",
    "# The add_pipe function appends our functions to the default pipeline.\n",
    "nlp_tweets_sentiws.add_pipe(\"sentencizer\", last=True)\n",
    "nlp_tweets_sentiws.add_pipe(\"Detect languages\", name='Detect languages', last=True)\n",
    "nlp_tweets_sentiws.add_pipe(\"Keep only German documents\", name='Keep only German documents', last=True)\n",
    "nlp_tweets_sentiws.add_pipe(\"Remove non alphabetic words\", name=\"Remove non alphabetic words\", last=True)\n",
    "nlp_tweets_sentiws.add_pipe(\"Remove stopwords\", name=\"Remove stopwords\", last=True)\n",
    "# nlp_tweets.add_pipe(\"Lemmatize text\", name=\"Lemmatize text\", last=True)\n",
    "# nlp_tweets.add_pipe(\"Lowercase Text\", name=\"Lowercase Text\", last=True)\n",
    "nlp_tweets_sentiws.add_pipe(\"Sentiment Appplication\", name=\"Sentiment Appplication\", last=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.6.2.1 Sentiment Analysis for Twitter Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, we want to have a look at our Twitter data again. As with the TextBlob analysis we want to go through all the individual politicians and therefore create a loop. In difference to the first approach we used the raw data here as we want to apply our new pipeline to the dataset. After the application of the pipeline with sentiment functionality we go through the preprocessed tweets and take the calculated sentiment of each token. Next we add the scores together and calculate the means for each tweet and then for the individual politician. Again we count the number of positive, negative, and neutral tweets as well."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Apply the sentiment anaylsis to the Twitter accounts of the politicians\n",
    "# data=[]\n",
    "# for name in tqdm(['Ralph Brinkhaus','Hermann Gröhe', 'Nadine Schön' ,'Norbert Röttgen' , 'Peter Altmaier' , 'Jens Spahn' , 'Matthias Hauer',\n",
    "#             'Christian Lindner' , 'Marco Buschmann' , 'Bettina Stark-Watzinger', 'Alexander Graf Lambsdorff' , 'Johannes Vogel' , 'Konstantin Kuhle' , 'Marie-Agnes Strack-Zimmermann',\n",
    "#             'Lars Klingbeil' , 'Saskia Esken' , 'Hubertus Heil' , 'Heiko Maas' , 'Martin Schulz' , 'Karamba Diaby' , 'Karl Lauterbach',\n",
    "#             'Steffi Lemke' , 'Cem Özdemir' , 'Katrin Göring-Eckardt' , 'Konstantin von Notz' , 'Britta Haßelmann' , 'Sven Lehmann' , 'Annalena Baerbock',\n",
    "#             'Sahra Wagenknecht' , 'Bernd Riexinger' , 'Niema Movassat' , 'Jan Korte' , 'Dietmar Bartsch' , 'Gregor Gysi' , 'Sevim Dağdelen',\n",
    "#             'Alice Weidel' , 'Beatrix von Storch' , 'Joana Cotar' , 'Stephan Brandner' , 'Tino Chrupalla' , 'Götz Frömming' , 'Leif-Erik Holm']):\n",
    "#     #get tweets from the specific politician\n",
    "#     tweets_analyzing = pre_data_twitter.loc[pre_data_twitter['full_name']==name]\n",
    "#     tweets_analyzing1 = tweets_analyzing.text.progress_apply(nlp_tweets_sentiws)\n",
    "#     #get the sentiment of the tweets\n",
    "#     politician_sum=[]\n",
    "#     for sentence in tweets_analyzing1:\n",
    "#         sentence_sum=[]\n",
    "#         for token in sentence:\n",
    "#             if token._.sentiws == None:\n",
    "#                 a=0\n",
    "#             elif token._.sentiws == 'nan':\n",
    "#                 a=0\n",
    "#             else:\n",
    "#                 sentence_sum.append(token._.sentiws)\n",
    "#         sentence_score=np.nanmean(sentence_sum)\n",
    "#         politician_sum.append(sentence_score)\n",
    "#     politician_score=np.nanmean(politician_sum)\n",
    "#     #get the number of positive, neutral and negative tweets\n",
    "#     positive_p=0\n",
    "#     neutral_p=0\n",
    "#     negative_p=0\n",
    "#     for item_p in politician_sum:\n",
    "#         if item_p>0:\n",
    "#             positive_p += 1\n",
    "#         elif item_p<0:\n",
    "#             negative_p += 1\n",
    "#         elif item_p == 'nan':\n",
    "#             neutral_p += 1\n",
    "#         else:\n",
    "#             neutral_p += 1\n",
    "#     #set up list to secure the values generated\n",
    "#     data.append([name,politician_score,positive_p,neutral_p,negative_p])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We transform the list into a dataframe that we can again analyze further."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#set up dataframe with all values\n",
    "# dataf = pd.DataFrame(data, columns=['Name','Polarity_mean','Num_pos_tweets','Num_neutral_tweets','Num_neg_tweets'])\n",
    "# dataf.to_csv('../data/processed/sentiment_scores_tweets_sentiws_01.csv')\n",
    "# dataf.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.6.2.2 Sentiment Analysis for Bundestag Speeches"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Again we want to have a look at the Bundestag speeches and see how the SentiWS dictionary classifies them in terms of sentiment. We use the same procedure as with the tweets before to calculate the scores and the counts."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Apply the sentiment analysis to the speeches accounts of the politicians\n",
    "# data=[]\n",
    "# for name in tqdm(['Ralph Brinkhaus','Hermann Gröhe', 'Nadine Schön' ,'Norbert Röttgen' , 'Peter Altmaier' , 'Jens Spahn' , 'Matthias Hauer',\n",
    "#             'Christian Lindner' , 'Marco Buschmann' , 'Bettina Stark-Watzinger', 'Alexander Graf Lambsdorff' , 'Johannes Vogel' , 'Konstantin Kuhle' , 'Marie-Agnes Strack-Zimmermann',\n",
    "#             'Lars Klingbeil' , 'Saskia Esken' , 'Hubertus Heil' , 'Heiko Maas' , 'Martin Schulz' , 'Karamba Diaby' , 'Karl Lauterbach',\n",
    "#             'Steffi Lemke' , 'Cem Özdemir' , 'Katrin Göring-Eckardt' , 'Konstantin von Notz' , 'Britta Haßelmann' , 'Sven Lehmann' , 'Annalena Baerbock',\n",
    "#             'Sahra Wagenknecht' , 'Bernd Riexinger' , 'Niema Movassat' , 'Jan Korte' , 'Dietmar Bartsch' , 'Gregor Gysi' , 'Sevim Dağdelen',\n",
    "#             'Alice Weidel' , 'Beatrix von Storch' , 'Joana Cotar' , 'Stephan Brandner' , 'Tino Chrupalla' , 'Götz Frömming' , 'Leif-Erik Holm']):\n",
    "#     #get speeches from the specific politician\n",
    "#     speeches_analyzing = pre_data_speeches.loc[pre_data_speeches['full_name']==name]\n",
    "#     speeches_analyzing1 = speeches_analyzing.text.progress_apply(nlp_tweets_sentiws)\n",
    "#     #get the sentiment of the tweets\n",
    "#     politician_sum=[]\n",
    "#     for sentence in speeches_analyzing1:\n",
    "#         sentence_sum=[]\n",
    "#         for token in sentence:\n",
    "#             if token._.sentiws == None:\n",
    "#                 a=0\n",
    "#             elif token._.sentiws == 'nan':\n",
    "#                 a=0\n",
    "#             else:\n",
    "#                 sentence_sum.append(token._.sentiws)\n",
    "#         sentence_score=np.nanmean(sentence_sum)\n",
    "#         politician_sum.append(sentence_score)\n",
    "#     politician_score=np.nanmean(politician_sum)\n",
    "#     #get the number of positive, neutral and negative tweets\n",
    "#     positive_p=0\n",
    "#     neutral_p=0\n",
    "#     negative_p=0\n",
    "#     for item_p in politician_sum:\n",
    "#         if item_p>0:\n",
    "#             positive_p += 1\n",
    "#         elif item_p<0:\n",
    "#             negative_p += 1\n",
    "#         elif item_p == 'nan':\n",
    "#             neutral_p += 1\n",
    "#         else:\n",
    "#             neutral_p += 1\n",
    "#     #set up list to secure the values generated\n",
    "#     data.append([name,politician_score,positive_p,neutral_p,negative_p])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And afterwards create a dataframe from the data for analysis."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#set up dataframe with all values\n",
    "# dataf = pd.DataFrame(data, columns=['Name','Polarity_mean','Num_pos_speeches','Num_neutral_speeches','Num_neg_speeches'])\n",
    "# dataf.to_csv('../data/processed/sentiment_scores_speeches_sentiws_01.csv')\n",
    "# dataf.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As the polarity scores for the SentiWS dictionary seem to be less significant due to their absolute values being smaller, we decided to conduct the further in depth analysis of the sentiment with the TextBlob model. These smaller values with the SentiWS dictionary could be a result from our loop used because the mean values could be too unrobust to mean neutral tweets. Another possible explanation could be that there are no great outliers for the tweet or speech sentiments as the value range for polarity is only from -1 to 1."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}