{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import and view data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_raw = pd.read_csv(\"../data/interim/Bundestag_Tweets.csv\", index_col = \"Unnamed: 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_raw.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usernames_to_fullname = {'rbrinkhaus': 'Ralph Brinkhaus',\n",
    "                         'groehe': 'Hermann Gröhe',\n",
    "                         'NadineSchoen': 'Nadine Schön',\n",
    "                         'n_roettgen': 'Norbert Röttgen',\n",
    "                         'peteraltmaier': 'Peter Altmaier',\n",
    "                         'jensspahn': 'Jens Spahn',\n",
    "                         'MatthiasHauer': 'Matthias Hauer',\n",
    "                         'c_lindner': 'Christian Lindner',\n",
    "                         'MarcoBuschmann': 'Marco Buschmann',\n",
    "                         'starkwatzinger': 'Bettina Stark-Watzinger',\n",
    "                         'Lambsdorff': 'Alexander Graf Lambsdorff',\n",
    "                         'johannesvogel': 'Johannes Vogel',\n",
    "                         'KonstantinKuhle': 'Konstantin Kuhle',\n",
    "                         'MAStrackZi': 'Marie-Agnes Strack-Zimmermann',\n",
    "                         'larsklingbeil': 'Lars Klingbeil',\n",
    "                         'EskenSaskia': 'Saskia Esken',\n",
    "                         'hubertus_heil': 'Hubertus Heil',\n",
    "                         'HeikoMaas': 'Heiko Maas',\n",
    "                         'MartinSchulz': 'Martin Schulz',\n",
    "                         'KarambaDiaby': 'Karamba Diaby',\n",
    "                         'Karl_Lauterbach': 'Karl Lauterbach',\n",
    "                         'SteffiLemke': 'Steffi Lemke',\n",
    "                         'cem_oezdemir': 'Cem Özdemir',\n",
    "                         'GoeringEckardt': 'Katrin Göring-Eckardt',\n",
    "                         'KonstantinNotz': 'Konstantin von Notz',\n",
    "                         '22': 'Konstantin von Notz',\n",
    "                         'BriHasselmann': 'Britta Haßelmann',\n",
    "                         'svenlehmann': 'Sven Lehmann',\n",
    "                         'ABaerbock': 'Annalena Baerbock',\n",
    "                         'SWagenknecht': 'Sahra Wagenknecht',\n",
    "                         'b_riexinger': 'Bernd Riexinger',\n",
    "                         'NiemaMovassat': 'Niema Movassat',\n",
    "                         'jankortemdb': 'Jan Korte',\n",
    "                         'DietmarBartsch': 'Dietmar Bartsch',\n",
    "                         'GregorGysi': 'Gregor Gysi',\n",
    "                         'SevimDagdelen': 'Sevim Dağdelen',\n",
    "                         'Alice_Weidel': 'Alice Weidel',\n",
    "                         'Beatrix_vStorch': 'Beatrix von Storch',\n",
    "                         'JoanaCotar': 'Joana Cotar',\n",
    "                         'StBrandner': 'Stephan Brandner',\n",
    "                         'Tino_Chrupalla': 'Tino Chrupalla',\n",
    "                         'GtzFrmming': 'Götz Frömming',\n",
    "                         '5': 'Götz Frömming',\n",
    "                         'Leif_Erik_Holm': 'Leif-Erik Holm'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_raw[\"full_name\"] = tweets_raw.Username.replace(usernames_to_fullname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plenar protocolls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_raw = pd.read_csv(\"../data/interim/bundestag_speeches_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_raw.dropna(subset = [\"speech_content\"], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_raw[\"full_name\"] = speeches_raw[\"first_name\"] + \" \" + speeches_raw[\"last_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(speeches_raw[\"full_name\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_subset = speeches_raw[speeches_raw.full_name.isin(tweets_raw.full_name.unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(speeches_subset[\"full_name\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "# python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spacy pipeline\n",
    "pipeline_exclude = ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'ner', 'morphologizer']\n",
    "nlp_speeches = spacy.load('de_core_news_sm', exclude=pipeline_exclude)\n",
    "nlp_speeches.Defaults.stop_words |= {\"\\n  \",\"\\n\\n  \"}\n",
    "\n",
    "@Language.component(\"Lemmatize text\")\n",
    "def lemmatize_text(doc):\n",
    "    doc = [token.lemma_ for token in doc]\n",
    "    doc = ' '.join(doc)\n",
    "    return nlp_speeches.make_doc(doc)\n",
    "\n",
    "@Language.component(\"Lowercase Text\")\n",
    "def lowercase(doc):\n",
    "    doc = [token.lower_ for token in doc]\n",
    "    doc = ' '.join(doc)\n",
    "    return nlp_speeches.make_doc(doc)\n",
    "\n",
    "@Language.component(\"Remove numbers that mark no year\")\n",
    "def remove_number_not_year(doc):\n",
    "    return [token for token in doc if not token.is_digit or len(token.text) > 3]\n",
    "\n",
    "@Language.component(\"Remove stopwords and punctuation\")\n",
    "def remove_stopwords(doc):\n",
    "    doc = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
    "    return doc\n",
    "\n",
    "# The add_pipe function appends our functions to the default pipeline.\n",
    "nlp_speeches.add_pipe(\"Lemmatize text\", name=\"Lemmatize text\", last=True)\n",
    "nlp_speeches.add_pipe(\"Lowercase Text\", name=\"Lowercase Text\", last=True)\n",
    "nlp_speeches.add_pipe(\"Remove numbers that mark no year\", name=\"Remove numbers that mark no year\", last=True)\n",
    "nlp_speeches.add_pipe(\"Remove stopwords and punctuation\", name=\"Remove stopwords and punctuation\", last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_subset[\"text_preprocessed\"] = speeches_subset.speech_content.progress_apply(nlp_speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_subset[\"text_preprocessed_sentence\"] = speeches_subset[\"text_preprocessed\"].progress_apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_subset.to_csv(\"../data/processed/speeches_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create emoji matcher\n",
    "import re\n",
    "emoji = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spacy pipeline\n",
    "pipeline_exclude = ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'ner', 'morphologizer']\n",
    "nlp_twitter = spacy.load('de_core_news_sm', exclude=pipeline_exclude)\n",
    "nlp_twitter.Defaults.stop_words |= {\"\\n    \", \"amp\", \"rt\"}\n",
    "\n",
    "@Language.component(\"Lemmatize text\")\n",
    "def lemmatize_text(doc):\n",
    "    doc = [token.lemma_ for token in doc]\n",
    "    doc = ' '.join(doc)\n",
    "    return nlp_twitter.make_doc(doc)\n",
    "\n",
    "@Language.component(\"Lowercase Text\")\n",
    "def lowercase(doc):\n",
    "    doc = [token.lower_ for token in doc]\n",
    "    doc = ' '.join(doc)\n",
    "    return nlp_twitter.make_doc(doc)\n",
    "\n",
    "@Language.component(\"Remove URLs\")\n",
    "def remove_urls(doc):\n",
    "    doc = [token.text for token in doc if not token.like_url]\n",
    "    doc = ' '.join(doc)\n",
    "    return nlp_twitter.make_doc(doc)\n",
    "\n",
    "@Language.component(\"Remove emojis\")\n",
    "def remove_emojis(doc):\n",
    "    doc = [token.text for token in doc if not re.match(emoji, token.text)]\n",
    "    doc = ' '.join(doc)\n",
    "    return nlp_twitter.make_doc(doc)\n",
    "\n",
    "@Language.component(\"Remove mentions\")\n",
    "def remove_mentions(doc):\n",
    "    doc = [token.text for token in doc if not re.match(\"@.*\", token.text)]\n",
    "    doc = ' '.join(doc)\n",
    "    return nlp_twitter.make_doc(doc)\n",
    "\n",
    "@Language.component(\"Remove stopwords and punctuation\")\n",
    "def remove_stopwords(doc):\n",
    "    doc = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
    "    return doc\n",
    "\n",
    "# The add_pipe function appends our functions to the default pipeline.\n",
    "nlp_twitter.add_pipe(\"Lemmatize text\", name=\"Lemmatize text\", last=True)\n",
    "nlp_twitter.add_pipe(\"Lowercase Text\", name=\"Lowercase Text\", last=True)\n",
    "nlp_twitter.add_pipe(\"Remove URLs\", name=\"Remove URLs\", last=True)\n",
    "nlp_twitter.add_pipe(\"Remove emojis\", name=\"Remove emojis\", last=True)\n",
    "nlp_twitter.add_pipe(\"Remove mentions\", name=\"Remove mentions\", last=True)\n",
    "nlp_twitter.add_pipe(\"Remove stopwords and punctuation\", name=\"Remove stopwords and punctuation\", last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_raw[\"text_preprocessed\"] = tweets_raw.Text.progress_apply(nlp_twitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_raw[\"text_preprocessed_sentence\"] = tweets_raw[\"text_preprocessed\"].progress_apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_raw.to_csv(\"../data/processed/tweets_preprocessed.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
