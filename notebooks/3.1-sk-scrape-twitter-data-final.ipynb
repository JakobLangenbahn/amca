{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f372fe9",
   "metadata": {},
   "source": [
    "## 3.1 Scrape tweets from selected politicians"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75290b2f",
   "metadata": {},
   "source": [
    "### 1. Import packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e73d2f6",
   "metadata": {},
   "source": [
    "As we want to analyze Twitter data and speeches from German politicians one of our first tasks was to get the tweets. Because we decided to analyze the nineteenth Bundestag as our time span, we had to get data from politicians of the six represented parties in the Bundestag. <br>\n",
    "For our purpose we decided to take seven of the more active and most followed politicians from each respected political party. After deciding on which politicians accounts need to be scraped, we implemented a scraper. We chose snscrape which is a scraper for social network services. It allowed us to scrape every ever-posted tweet by one of our politicians without problems and restrictions. Apart from Twitter this scraper can be used for many more social networks like Facebook or Instagram but for our project Twitter was sufficient.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be49374c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795ab320",
   "metadata": {},
   "source": [
    "### 2. Define variables and functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2db6d7",
   "metadata": {},
   "source": [
    "After installation this package allowed us to scrape tweets from desired accounts and gave us many options on how the retrieved data should look. Besides the accounts username and tweet texts it allowed us to also get information on the time of the post, the displayed Twitter name of the account, the number of replies, retweets and likes. <br> We created a dictionary to map the usernames of the Twitter accounts to the political party. Afterwards, we defined a function that creates a dataframe with the scraped tweets from snscrape given the Twitter username and party. With the tweets scraped we only get the real tweets and reply from the politician and no retweets what helped in the analysis as we only got media content created by the desired target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0467576",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_name_party_dic = {'rbrinkhaus': 'CDU', 'groehe': 'CDU', 'NadineSchoen': 'CDU', 'n_roettgen': 'CDU',\n",
    "                          'peteraltmaier': 'CDU', 'jensspahn': 'CDU', 'MatthiasHauer': 'CDU', 'c_lindner': 'FDP',\n",
    "                          'MarcoBuschmann': 'FDP', 'starkwatzinger': 'FDP', 'Lambsdorff': 'FDP',\n",
    "                          'johannesvogel': 'FDP', 'KonstantinKuhle': 'FDP', 'MAStrackZi': 'FDP',\n",
    "                          'larsklingbeil': 'SPD', 'EskenSaskia': 'SPD', 'hubertus_heil': 'SPD', 'HeikoMaas': 'SPD',\n",
    "                          'MartinSchulz': 'SPD', 'KarambaDiaby': 'SPD', 'Karl_Lauterbach': 'SPD',\n",
    "                          'SteffiLemke': 'Grüne', 'cem_oezdemir': 'Grüne', 'GoeringEckardt': 'Grüne',\n",
    "                          'KonstantinNotz': 'Grüne', 'BriHasselmann': 'Grüne', 'svenlehmann': 'Grüne',\n",
    "                          'ABaerbock': 'Grüne', 'SWagenknecht': 'Linke', 'b_riexinger': 'Linke',\n",
    "                          'NiemaMovassat': 'Linke', 'jankortemdb': 'Linke', 'DietmarBartsch': 'Linke',\n",
    "                          'GregorGysi': 'Linke', 'SevimDagdelen': 'Linke', 'Alice_Weidel': 'AFD',\n",
    "                          'Beatrix_vStorch': 'AFD', 'JoanaCotar': 'AFD', 'StBrandner': 'AFD',\n",
    "                          'Tino_Chrupalla': 'AFD', 'GtzFrmming': 'AFD', 'Leif_Erik_Holm': 'AFD',\n",
    "                          'ABaerbockArchiv':\"Grüne\"\n",
    "                          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bcf79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_user_tweets(twitter_name, party):\n",
    "    tweets_list = []\n",
    "    search_string = \"from:\" + twitter_name\n",
    "    for tweet in tqdm((sntwitter.TwitterSearchScraper(search_string).get_items())):\n",
    "        tweets_list.append([tweet.date, tweet.id, tweet.content, tweet.user.username, tweet.user.displayname,\n",
    "                                tweet.replyCount, tweet.retweetCount, tweet.likeCount])\n",
    "    tweets_df = pd.DataFrame(tweets_list, columns=['datetime', 'tweet_id', 'text', 'username', 'name',\n",
    "                                                   'reply_count', 'retweet_count', 'like_count'])\n",
    "    tweets_df[\"party\"] = party\n",
    "    return tweets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bd71bc",
   "metadata": {},
   "source": [
    "### 3. Scrape twitter data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b70aaa",
   "metadata": {},
   "source": [
    "We define a for loop applying our defined function and appending the generated dataframe to one big corpus for the Twitter data. After scraping the tweets for every politician, we ended up with data frames that contained date, id, text, username, Twitter name, reply count, retweet count and like count of each individual tweet. We combined those data frames to one big corpus containing all the tweets by concatenating the data frames. With this corpus of around 300000 tweets, we could start preprocessing and analyzing the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162dd97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_name_tweets_list = []\n",
    "for key in twitter_name_party_dic:\n",
    "    print(key)\n",
    "    twitter_name_tweets_list.append(retrieve_user_tweets(key, twitter_name_party_dic[key]))\n",
    "tweets = pd.concat(twitter_name_tweets_list, ignore_index=True)\n",
    "tweets.to_csv(\"../data/raw/tweets_scraped.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}