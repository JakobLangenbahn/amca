{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eff23b2",
   "metadata": {},
   "source": [
    "### 4.2.2 Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f531552",
   "metadata": {},
   "source": [
    "As often stressed in literature (cite) we need to revalidate the dictionary used to see if it fits with the application we are trying to analyse. Therefore, we will use gold standard validation to see how well the dictionaries perform in comparison to human coders. We will also check if the inter coder reliability is granted with our two authors coded. <br>\n",
    "Due to the fact that context plays a major role in deciding how emotions are expressed within a text we need to be especially careful when using a non-specific dictionary to detect sentiment. Working with this limitation we addressed the issue in our validation approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8897a78f",
   "metadata": {},
   "source": [
    "First, we load in the needed packages to perform the validation step. We will look at the accuracy, recall, precision and f1 score for the comparison to the gold standard of human coded sentiment. In addition we will use Cohen's Kappa to get a score for the intercoder reliability for the human coded data. These measures should proof useful in determining whether the dictionary performed well for our research questions and topic. <br>\n",
    "In addition, we also implemented a little interface to perform the gold standard validation which is why we need the ipywidgets library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "140dc25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import pickle\n",
    "\n",
    "#get the scores\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output\n",
    "from ipywidgets import IntProgress\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de452a6",
   "metadata": {},
   "source": [
    "### Validation Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73a9319",
   "metadata": {},
   "source": [
    "Again we start of our validation with the sentiment scores for the Twitter data. First, we need to transform our dataset in a way that we can apply our gold standard validation. As until now, we got the sentiment in form of a polarity score which is given as a number between -1 and 1, we needed a way to make this scoring system manageable for human coders. (How should they distinguish between a sentiment of 0.0001 and 0.0002?) To make our lifes easier we decided to take a simple scoring system of distributing the tweets into positive, negative and neutral tweets. That way there was a less subjective classification as we can for the most part agree on what positive and negative conotated messages are. To generate the corresponding score generated by the dictionary approach we classified the sentiment as positive if the polarity was positive and negative if the polarity was negative. This only left tweets and speeches with a polarity of 0 as neutral which again needs to critically viewed as the polarity score can be biased in one direction. So we should consider the neutral assignments made by the dictionary with care. Nevertheless, for our gold standard coding we can use this scoring system. <br>\n",
    "After creating the new score values via a loop we add them as a new column to our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bfc079f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up Twitter dataset for sentiment coding\n",
    "pre_data_twitter= pickle.load(open('../data/processed/tweets_processed.p','rb'))\n",
    "sentiment=[]\n",
    "for polarity in pre_data_twitter['polarity_textblob']:\n",
    "    if polarity>0:\n",
    "        sentiment.append('Positive')\n",
    "    elif polarity<0:\n",
    "        sentiment.append('Negative')\n",
    "    else:\n",
    "        sentiment.append('Neutral')\n",
    "pre_data_twitter['sentiment']=sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fa16a3",
   "metadata": {},
   "source": [
    "In preparation for the validation step we will need to define a function that let's us randomly select a certain number of tweets from our corpus. We do this by simply suffeling the data and afterwards selecting the first tweets until the desired number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d36eff87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function to chose random tweets for manual coding\n",
    "def create_sentiment_dataset(data, number):\n",
    "    data= data.sample(frac=1)\n",
    "    data_test= data[0:number]\n",
    "    data_test.reset_index(drop=True, inplace= True)\n",
    "    return data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbb5526",
   "metadata": {},
   "source": [
    "The next step is to create the interface for the validity testing. Herefore, we also define a function that lets us display buttons we can press to select the wanted sentiment of the coder while going through the randomly selected tweets. After the coder has labeled all the given tweets we save his labels as a new column for the given dataframe and create a file were we save the coded corpus for the coder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcca91eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the test interface\n",
    "def sentiment_gold_dictionary_tweets(sentiment_df, name):\n",
    "    max_count = sentiment_df.shape[0]\n",
    "    global i\n",
    "    i = 0\n",
    "\n",
    "    button_0 = widgets.Button(description = \"Positive\")\n",
    "    button_1 = widgets.Button(description = \"Neutral\")\n",
    "    button_2 = widgets.Button(description = \"Negative\")\n",
    "    \n",
    "    chosen_elements = []\n",
    "\n",
    "    display(\"Sentiment Gold Standard\")\n",
    "\n",
    "    f = IntProgress(min=0, max=max_count)\n",
    "    display(f)\n",
    "    \n",
    "    display(sentiment_df.text_preprocessed_sentence[i])\n",
    "\n",
    "    display(button_0)\n",
    "    display(button_1)\n",
    "    display(button_2)\n",
    "\n",
    "\n",
    "    def btn_eventhandler(obj):\n",
    "        global i \n",
    "        i += 1\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        display(\"Sentiment Gold Standard\")\n",
    "        display(f)\n",
    "        f.value += 1\n",
    "                \n",
    "        choosen_text = obj.description\n",
    "        chosen_elements.append(choosen_text)\n",
    "        \n",
    "        if i < max_count:\n",
    "            \n",
    "            display(sentiment_df.text_preprocessed_sentence[i])\n",
    "            \n",
    "            display(button_0)\n",
    "            display(button_1)\n",
    "            display(button_2)\n",
    "            \n",
    "            button_0.on_click(btn_eventhandler)\n",
    "            button_1.on_click(btn_eventhandler)\n",
    "            button_2.on_click(btn_eventhandler)\n",
    "            \n",
    "        else:\n",
    "            print (\"Thanks \" + name + \" you finished all the work!\")\n",
    "            sentiment_df[\"choosen_sentiment\"] = chosen_elements\n",
    "            sentiment_df.to_csv(\"../data/processed/sentiment_gold_standard_tweets_\" + name + \".csv\", index = False)\n",
    "\n",
    "    button_0.on_click(btn_eventhandler)\n",
    "    button_1.on_click(btn_eventhandler)\n",
    "    button_2.on_click(btn_eventhandler)\n",
    "    \n",
    "    return sentiment_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491315c9",
   "metadata": {},
   "source": [
    "In our validation step, we use 40 randomly selected tweets to label manually. We are well aware that the usual suggestion is to label at least 1% of the corpus manually when revalidating. As this would have meant to label over 1000 tweets we settled for fewer but in the same range as for the speeches later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c8143b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=create_sentiment_dataset(pre_data_twitter, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ad97c0",
   "metadata": {},
   "source": [
    "After selection, we labeled the tweets with our defined function. While labeling we of course get a glimps of the tweets in the corpus. While most of the tweets seem to contain useful messages there were also some that were rather short or even just one word as there also seem to be replys to tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18a6d141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sentiment Gold Standard'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6981617845b49f7975753a236dccda8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=39, max=40)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thanks Stjepan you finished all the work!\n"
     ]
    }
   ],
   "source": [
    "#test_sentiment=sentiment_gold_dictionary_tweets(test_data,'Stjepan')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6580999",
   "metadata": {},
   "source": [
    "When labeling is finished we save the results in a csv file so we can analyze them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "37cfa4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentiment1=pd.read_csv('../data/processed/sentiment_gold_standard_tweets_Stjepan.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5105c42a",
   "metadata": {},
   "source": [
    "The same labeling with 40 newly selected tweets is done by the second coder so we have a labeled corpus of 80 tweets in total. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "321d9f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=create_sentiment_dataset(pre_data_twitter, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe104073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sentiment Gold Standard'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b54a795e9dcb4e09bebd32f2fb76c2be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=39, max=40)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thanks Jakob you finished all the work!\n"
     ]
    }
   ],
   "source": [
    "#test_sentiment=sentiment_gold_dictionary_tweets(test_data,'Jakob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0662f690",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentiment2=pd.read_csv('../data/processed/sentiment_gold_standard_tweets_Jakob.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938cdc64",
   "metadata": {},
   "source": [
    "Afterwards the labeled tweets are combined into one file so we can analyze them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "94b1548c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentiment_both=pd.concat([test_sentiment1,test_sentiment2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ea8fcf",
   "metadata": {},
   "source": [
    "As mentioned, we want to use different metric to see how the dictionary performed with the sentiment analysis. We first look at F1 and Accuracy to get a feeling for the performance and then at Recall and Precision to get further insides on how the performance was achieved. <br>\n",
    "For the whole corpus of labeled tweets we can see that Accuracy and F1 where rather simliar with around 54%. On a first look this seems like a bad result but when thinking about the performance of dictionary approaches in general a score in this region is really close to the best you can hope for considering we did not tune the dictionary to fit our problem particularly well. When looking at Recall and Precision we can see that those metrics also don't differ to much from the F1 score. <br>\n",
    "We can plot the confusion matrices for the different labels from our validation corpus to see what kind of errors were made by the dictionary while classifying. We can see some quiet different pictures there for the different labels but overall the dictionary classifier seems to be making both kinds of errors. All in all the performance seems to be fine for the dictionary in TextBlob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a6700d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.5494074201305393\n",
      "Accuracy Score: 0.5375\n",
      "Precision Score: 0.6026515151515152\n",
      "Recall Score: 0.5375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[48,  2],\n",
       "        [14, 16]],\n",
       "\n",
       "       [[29, 20],\n",
       "        [11, 20]],\n",
       "\n",
       "       [[46, 15],\n",
       "        [12,  7]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f2=f1_score(test_sentiment_both['choosen_sentiment'], test_sentiment_both['sentiment'], average='weighted')\n",
    "print('F1 Score:',f2)\n",
    "accuracy2=accuracy_score(test_sentiment_both['choosen_sentiment'], test_sentiment_both['sentiment'])\n",
    "print('Accuracy Score:',accuracy2)\n",
    "precision2=precision_score(test_sentiment_both['choosen_sentiment'], test_sentiment_both['sentiment'], average='weighted',zero_division=1)\n",
    "print('Precision Score:',precision2)\n",
    "recall2=recall_score(test_sentiment_both['choosen_sentiment'], test_sentiment_both['sentiment'], average='weighted',zero_division=1)\n",
    "print('Recall Score:',recall2)\n",
    "cm = multilabel_confusion_matrix(test_sentiment_both['choosen_sentiment'], test_sentiment_both['sentiment'])\n",
    "display(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499e061c",
   "metadata": {},
   "source": [
    "For the next step we want to look at the intercoder reliability of our manually coded tweets. This will give us a measure on how reliable the previous result are. Only with a relatively good score here can we be sure to have so good gold standard. We choose 10 randomly selected tweets that will be labeled by both coders on which basis we perform this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08a6df08",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=create_sentiment_dataset(pre_data_twitter, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e286bd",
   "metadata": {},
   "source": [
    "Then, we again perform the manual coding, this time with both coders having the same tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93d79ac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sentiment Gold Standard'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5da7b9e4ea4c49e4a8b08660f8024e88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=9, max=10)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thanks Stjepan_inter you finished all the work!\n"
     ]
    }
   ],
   "source": [
    "#test_sentiment=sentiment_gold_dictionary_tweets(test_data,'Stjepan_inter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "384dd62e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sentiment Gold Standard'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1fc46c51275450eaf73e7e2331b4556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=9, max=10)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thanks Jakob_inter you finished all the work!\n"
     ]
    }
   ],
   "source": [
    "#test_sentiment=sentiment_gold_dictionary_tweets(test_data,'Jakob_inter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c4f2ce",
   "metadata": {},
   "source": [
    "Afterwards, we save the results if the manual coding in csv files so we can access them at will."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c956ffe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentiment1=pd.read_csv('../data/processed/sentiment_gold_standard_tweets_Stjepan_inter.csv')\n",
    "test_sentiment2=pd.read_csv('../data/processed/sentiment_gold_standard_tweets_Jakob_inter.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6f81d5",
   "metadata": {},
   "source": [
    "In the end, we calculate Cohen's Kappa as our reliability measure for which we use a implemented function from the sklearn library. We can see that our score of roughly 68% is an acceptable level of intercoder reliability. There sure is room for improvement but considering that we had no big coding manual for the coders and just relied on simple sentiment impression this seems like a satisfying result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32c7c06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6825396825396826\n"
     ]
    }
   ],
   "source": [
    "#kappa\n",
    "kappa= cohen_kappa_score(test_sentiment1['choosen_sentiment'],test_sentiment2['choosen_sentiment'])\n",
    "print(kappa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c22085",
   "metadata": {},
   "source": [
    "### Validation Speeches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b00412f",
   "metadata": {},
   "source": [
    "Next up are the speeches for which we of course also have revalidate the dictionary approach. Again we load in the data and apply our code to generate a new column which contains the new sentiment scoring system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a876d5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up Speeches dataset for sentiment coding\n",
    "pre_data_speeches= pickle.load(open('../data/processed/speeches_processed.p','rb'))\n",
    "sentiment=[]\n",
    "for polarity in pre_data_speeches['polarity_textblob']:\n",
    "    if polarity>0:\n",
    "        sentiment.append('Positive')\n",
    "    elif polarity<0:\n",
    "        sentiment.append('Negative')\n",
    "    else:\n",
    "        sentiment.append('Neutral')\n",
    "pre_data_speeches['sentiment']=sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733592cc",
   "metadata": {},
   "source": [
    "Then we again set up afunction to randomly sample a given amount of tweets that we want to code. (Notice it is the same function as above.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a6624c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function to chose random speeches for manual coding\n",
    "def create_sentiment_dataset(data, number):\n",
    "    data= data.sample(frac=1)\n",
    "    data_test= data[0:number]\n",
    "    data_test.reset_index(drop=True, inplace= True)\n",
    "    return data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9111b6cc",
   "metadata": {},
   "source": [
    "Afterwards we create the interface for the manual coding of the speeches by implementing widget that let us select the sentiment for the sampled tweets as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4e7e74bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the test interface\n",
    "def sentiment_gold_dictionary_speeches(sentiment_df, name):\n",
    "    max_count = sentiment_df.shape[0]\n",
    "    global i\n",
    "    i = 0\n",
    "\n",
    "    button_0 = widgets.Button(description = \"Positive\")\n",
    "    button_1 = widgets.Button(description = \"Neutral\")\n",
    "    button_2 = widgets.Button(description = \"Negative\")\n",
    "    \n",
    "    chosen_elements = []\n",
    "\n",
    "    display(\"Sentiment Gold Standard\")\n",
    "\n",
    "    f = IntProgress(min=0, max=max_count)\n",
    "    display(f)\n",
    "    \n",
    "    display(sentiment_df.text_preprocessed_sentence[i])\n",
    "\n",
    "    display(button_0)\n",
    "    display(button_1)\n",
    "    display(button_2)\n",
    "\n",
    "\n",
    "    def btn_eventhandler(obj):\n",
    "        global i \n",
    "        i += 1\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        display(\"Sentiment Gold Standard\")\n",
    "        display(f)\n",
    "        f.value += 1\n",
    "                \n",
    "        choosen_text = obj.description\n",
    "        chosen_elements.append(choosen_text)\n",
    "        \n",
    "        if i < max_count:\n",
    "            \n",
    "            display(sentiment_df.text_preprocessed_sentence[i])\n",
    "            \n",
    "            display(button_0)\n",
    "            display(button_1)\n",
    "            display(button_2)\n",
    "            \n",
    "            button_0.on_click(btn_eventhandler)\n",
    "            button_1.on_click(btn_eventhandler)\n",
    "            button_2.on_click(btn_eventhandler)\n",
    "            \n",
    "        else:\n",
    "            print (\"Thanks \" + name + \" you finished all the work!\")\n",
    "            sentiment_df[\"choosen_sentiment\"] = chosen_elements\n",
    "            sentiment_df.to_csv(\"../data/processed/sentiment_gold_standard_speeches_\" + name + \".csv\", index = False)\n",
    "\n",
    "    button_0.on_click(btn_eventhandler)\n",
    "    button_1.on_click(btn_eventhandler)\n",
    "    button_2.on_click(btn_eventhandler)\n",
    "    \n",
    "    return sentiment_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce71769b",
   "metadata": {},
   "source": [
    "Again we choose 40 speeches randomly to be coded by hand. This time around this is around 1% of our speech corpus as we have significantly less speeches from the 19. Bundestag. Now every coder has to choose the sentiment of 40 different tweets. The results are again saved as a csv file so they can be accessed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9428b107",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=create_sentiment_dataset(pre_data_speeches, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3224868a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_sentiment=sentiment_gold_dictionary_speeches(test_data,'Stjepan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0662e561",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentiment1=pd.read_csv('../data/processed/sentiment_gold_standard_speeches_Stjepan.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ca4b0753",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=create_sentiment_dataset(pre_data_speeches, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3028c9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_sentiment=sentiment_gold_dictionary_speeches(test_data,'Jakob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "23de0b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentiment2=pd.read_csv('../data/processed/sentiment_gold_standard_speeches_Jakob.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a88026",
   "metadata": {},
   "source": [
    "After combining the two labeled corpi we can again apply our evaluation metrics. This time around the score are overall lower which lets us believe that the dictionary performed worse on the Bundestag speeches. When looking at the F1 score we see a drop of more than 10% which is a significantly lower result than before. The only score which doesn't seem to have dropped that much is Precision. Reasons for this drop in performance could lie in the different lengths of the speeches in comparison to the tweets and the higher complexity of texts. With this we can cautiously assume that the dictionary struggles to perform well with longer texts as there are more words influencing sentiment and in longer text sentiments could also be changing in different parts of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "72f2f389",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentiment_both=pd.concat([test_sentiment1,test_sentiment2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "19c02927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.3975587794617645\n",
      "Accuracy Score: 0.3875\n",
      "Precision Score: 0.5754140866873065\n",
      "Recall Score: 0.3875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[36,  7],\n",
       "        [25, 12]],\n",
       "\n",
       "       [[50,  3],\n",
       "        [20,  7]],\n",
       "\n",
       "       [[25, 39],\n",
       "        [ 4, 12]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f2=f1_score(test_sentiment_both['choosen_sentiment'], test_sentiment_both['sentiment'], average='weighted')\n",
    "print('F1 Score:',f2)\n",
    "accuracy2=accuracy_score(test_sentiment_both['choosen_sentiment'], test_sentiment_both['sentiment'])\n",
    "print('Accuracy Score:',accuracy2)\n",
    "precision2=precision_score(test_sentiment_both['choosen_sentiment'], test_sentiment_both['sentiment'], average='weighted',zero_division=1)\n",
    "print('Precision Score:',precision2)\n",
    "recall2=recall_score(test_sentiment_both['choosen_sentiment'], test_sentiment_both['sentiment'], average='weighted',zero_division=1)\n",
    "print('Recall Score:',recall2)\n",
    "cm = multilabel_confusion_matrix(test_sentiment_both['choosen_sentiment'], test_sentiment_both['sentiment'])\n",
    "display(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab7b3c9",
   "metadata": {},
   "source": [
    "After these measures we also want to have a look at the intercoder reliability of the human coders. For this we again take 10 speeches which are coded by both authors to determine the reliabilty of the results. We also save these results for future analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2f6ab8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=create_sentiment_dataset(pre_data_speeches, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "166c494d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sentiment Gold Standard'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3d43035d8bd455aa828a087224f876b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=9, max=10)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thanks Stjepan_inter you finished all the work!\n"
     ]
    }
   ],
   "source": [
    "test_sentiment=sentiment_gold_dictionary_speeches(test_data,'Stjepan_inter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1e31cba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sentiment Gold Standard'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1efe08d7259644f2bc4a84af6af9ee3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=9, max=10)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thanks Jakob_inter you finished all the work!\n"
     ]
    }
   ],
   "source": [
    "test_sentiment=sentiment_gold_dictionary_speeches(test_data,'Jakob_inter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "206c5fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentiment1=pd.read_csv('../data/processed/sentiment_gold_standard_speeches_Stjepan_inter.csv')\n",
    "test_sentiment2=pd.read_csv('../data/processed/sentiment_gold_standard_speeches_Jakob_inter.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223ed4d2",
   "metadata": {},
   "source": [
    "In the end we again compute Cohen's Kappa with help of the sklearn library. We get a value of 53% which is around 15% worse than for the tweets. Again this doesn't seem surprising as these speeches are more complex in their structure and can address multiple issues or issues from different perspectives. With regards to that the Kappa score seems resonable but again there is definitely room for improvement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f5654496",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### kappa\n",
    "kappa= cohen_kappa_score(test_sentiment1['choosen_sentiment'],test_sentiment2['choosen_sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d964489e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.53125\n"
     ]
    }
   ],
   "source": [
    "print(kappa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f88d10",
   "metadata": {},
   "source": [
    "In conclusion, we can say that our revalidation showed that the dictionary used to answer our research questions in regard to the sentiment analysis has performed good for a dictionary. As already suggested in (cite) dictionaries seem to struggle to give brilliant results in automated media content analysis due to their limited capacities and harsh assumptions. Nevertheless, for our research they offered a nice approach to gain first insights into the corpus and make reproducable analysis. As mentioned before, in a next step one should definitely view dictionaries as a first approach and try out different technics as machine learning and semi-supervised approaches next. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
