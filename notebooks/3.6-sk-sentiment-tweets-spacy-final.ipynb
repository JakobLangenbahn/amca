{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.6 Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the sentiment analysis we want to look at the question how the sentiment of politicians from different parties varies from social media to the Bundestag as an audience and make a comparison between female and male politicians in the way of used sentiment. As we used Python for our programming language, we start by importing some useful and commonly used packages. After loading in our preprocessed corpus we were ready to analyze the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_name</th>\n",
       "      <th>date</th>\n",
       "      <th>party</th>\n",
       "      <th>text_preprocessed</th>\n",
       "      <th>text_preprocessed_sentence</th>\n",
       "      <th>like_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ralph Brinkhaus</td>\n",
       "      <td>2021-06-15</td>\n",
       "      <td>CDU</td>\n",
       "      <td>[fußballfans, freuen, spiel, nationalmannschaf...</td>\n",
       "      <td>fußballfans freuen spiel nationalmannschaft dr...</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ralph Brinkhaus</td>\n",
       "      <td>2021-06-11</td>\n",
       "      <td>CDU</td>\n",
       "      <td>[außenpolitik, wirtschaftlich, souveränität, d...</td>\n",
       "      <td>außenpolitik wirtschaftlich souveränität digit...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ralph Brinkhaus</td>\n",
       "      <td>2021-06-11</td>\n",
       "      <td>CDU</td>\n",
       "      <td>[nachhaltig, klimawandel, kämpfen, brauchen, a...</td>\n",
       "      <td>nachhaltig klimawandel kämpfen brauchen akzept...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ralph Brinkhaus</td>\n",
       "      <td>2021-06-11</td>\n",
       "      <td>CDU</td>\n",
       "      <td>[brauchen, pandemie, bezahlen, arbeitsplätze, ...</td>\n",
       "      <td>brauchen pandemie bezahlen arbeitsplätze digit...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ralph Brinkhaus</td>\n",
       "      <td>2021-06-11</td>\n",
       "      <td>CDU</td>\n",
       "      <td>[wahldebatte, thema, zukunft, passieren, coron...</td>\n",
       "      <td>wahldebatte thema zukunft passieren corona sta...</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         full_name        date party  \\\n",
       "0  Ralph Brinkhaus  2021-06-15   CDU   \n",
       "1  Ralph Brinkhaus  2021-06-11   CDU   \n",
       "2  Ralph Brinkhaus  2021-06-11   CDU   \n",
       "3  Ralph Brinkhaus  2021-06-11   CDU   \n",
       "4  Ralph Brinkhaus  2021-06-11   CDU   \n",
       "\n",
       "                                   text_preprocessed  \\\n",
       "0  [fußballfans, freuen, spiel, nationalmannschaf...   \n",
       "1  [außenpolitik, wirtschaftlich, souveränität, d...   \n",
       "2  [nachhaltig, klimawandel, kämpfen, brauchen, a...   \n",
       "3  [brauchen, pandemie, bezahlen, arbeitsplätze, ...   \n",
       "4  [wahldebatte, thema, zukunft, passieren, coron...   \n",
       "\n",
       "                          text_preprocessed_sentence  like_count  \n",
       "0  fußballfans freuen spiel nationalmannschaft dr...        32.0  \n",
       "1  außenpolitik wirtschaftlich souveränität digit...         5.0  \n",
       "2  nachhaltig klimawandel kämpfen brauchen akzept...         4.0  \n",
       "3  brauchen pandemie bezahlen arbeitsplätze digit...         2.0  \n",
       "4  wahldebatte thema zukunft passieren corona sta...        24.0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import packages\n",
    "\n",
    "import pandas as pd\n",
    "from textblob_de import TextBlobDE as TextBlob\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import re\n",
    "import pickle\n",
    "pd.options.mode.chained_assignment = None  # default='warn' based on false positives\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy_langdetect import LanguageDetector\n",
    "from spacy.tokens.doc import Doc\n",
    "from spacy.vocab import Vocab\n",
    "from spacy_sentiws import spaCySentiWS\n",
    "from spacy_sentiws import spaCySentiWS\n",
    "\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "#load in the preprocessed data\n",
    "\n",
    "pre_data_twitter= pickle.load(open('../data/processed/tweets_processed.p','rb'))[0:100]\n",
    "pre_data_speeches= pickle.load(open('../data/processed/speeches_processed.p','rb'))[0:100]\n",
    "pre_data_twitter.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6.1 Sentiment Analysis with TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our first approach at sentiment analysis, we use the package TextBlob which can be used for preprocessing textual data and provides an API for natural language processing tasks like sentiment analysis. As our corpus was in German language, we needed to use the German version TextBlobDE which has fewer functionalities than its English counterpart but was sufficient for our first sentiment approach. For sentiment analysis it returns the polarity of a given sentence where polarity -1 means very negative and 1 very positive. The scores are generated based on a dictionary approach using a polarity lexicon for German from Clematide and Klenner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.1.1 Sentiment Analysis for Twitter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we start of with the analysis of the Twitter data. As we want to look at the different politicians from our corpus individually, we define a for loop going through each politician. To apply TextBlob we first need to take the preprocessed tweets in sentence format. After applying TextBlob we use the function sentiment to generate the polarity scores for the individual tweets. We ignore the second output subjectivity as it has no meaning in this German version of this package. Then we calculate the mean of the polarity for each politician. Furthermore, we counted the number of positive, negative, and neutral tweets for every politician without accounting for how positive or negative they were. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d79670f843334cfab94a185f5d50493b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stjepankusenic/.conda/envs/amca/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/stjepankusenic/.conda/envs/amca/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "#loop through all the politicians we want to analyze\n",
    "data=[]\n",
    "for name in tqdm(['Ralph Brinkhaus','Hermann Gröhe', 'Nadine Schön' ,'Norbert Röttgen' , 'Peter Altmaier' , 'Jens Spahn' , 'Matthias Hauer',\n",
    "            'Christian Lindner' , 'Marco Buschmann' , 'Bettina Stark-Watzinger', 'Alexander Graf Lambsdorff' , 'Johannes Vogel' , 'Konstantin Kuhle' , 'Marie-Agnes Strack-Zimmermann',\n",
    "            'Lars Klingbeil' , 'Saskia Esken' , 'Hubertus Heil' , 'Heiko Maas' , 'Martin Schulz' , 'Karamba Diaby' , 'Karl Lauterbach',\n",
    "            'Steffi Lemke' , 'Cem Özdemir' , 'Katrin Göring-Eckardt' , 'Konstantin von Notz' , 'Britta Haßelmann' , 'Sven Lehmann' , 'Annalena Baerbock',\n",
    "            'Sahra Wagenknecht' , 'Bernd Riexinger' , 'Niema Movassat' , 'Jan Korte' , 'Dietmar Bartsch' , 'Gregor Gysi' , 'Sevim Dağdelen',\n",
    "            'Alice Weidel' , 'Beatrix von Storch' , 'Joana Cotar' , 'Stephan Brandner' , 'Tino Chrupalla' , 'Götz Frömming' , 'Leif-Erik Holm']):\n",
    "    #get tweets from the specific politician \n",
    "    tweets_analyzing =pre_data_twitter.loc[pre_data_twitter['full_name']==name]\n",
    "    #create sentiment scores\n",
    "    blobs=tweets_analyzing['text_preprocessed_sentence'].apply(TextBlob)\n",
    "    sentiment=[]\n",
    "    for blob in blobs:\n",
    "        sentiment.append(blob.sentiment)\n",
    "    #get the polarity scores\n",
    "    polarity=[]\n",
    "    for egg in sentiment:\n",
    "        polarity.append(egg.polarity)\n",
    "    #get the mean of the scores \n",
    "    p_mean = np.mean(polarity)\n",
    "    #get the number of positive, neutral and negative tweets\n",
    "    positive_p=0\n",
    "    neutral_p=0\n",
    "    negative_p=0\n",
    "    for item_p in polarity:\n",
    "        if item_p>0:\n",
    "            positive_p += 1\n",
    "        elif item_p<0:\n",
    "            negative_p += 1\n",
    "        else:\n",
    "            neutral_p += 1\n",
    "    #set up list to secure the values generated\n",
    "    data.append([name,p_mean,positive_p,neutral_p,negative_p]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ending up with a data frame containing the polarity means and tweet counts for every politician, we had a first overview of the sentiments of their social media presence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Polarity_mean</th>\n",
       "      <th>Num_pos_tweets</th>\n",
       "      <th>Num_neutral_tweets</th>\n",
       "      <th>Num_neg_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ralph Brinkhaus</td>\n",
       "      <td>0.265083</td>\n",
       "      <td>42</td>\n",
       "      <td>48</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hermann Gröhe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nadine Schön</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Norbert Röttgen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Peter Altmaier</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Name  Polarity_mean  Num_pos_tweets  Num_neutral_tweets  \\\n",
       "0  Ralph Brinkhaus       0.265083              42                  48   \n",
       "1    Hermann Gröhe            NaN               0                   0   \n",
       "2     Nadine Schön            NaN               0                   0   \n",
       "3  Norbert Röttgen            NaN               0                   0   \n",
       "4   Peter Altmaier            NaN               0                   0   \n",
       "\n",
       "   Num_neg_tweets  \n",
       "0              10  \n",
       "1               0  \n",
       "2               0  \n",
       "3               0  \n",
       "4               0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set up dataframe with all values and save it into a csv file\n",
    "dataf = pd.DataFrame(data, columns=['Name','Polarity_mean','Num_pos_tweets','Num_neutral_tweets','Num_neg_tweets'])\n",
    "dataf.to_csv('../data/processed/sentiment_scores_twitter_01.csv')\n",
    "dataf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now expand our dataframe with a column containing the polarity score generated by TextBlob. By simply applying the code from our for loop to the whole corpus and appending the generated scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d742d7b44f441a3b41b814add007117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#create a polarity column for our dataset\n",
    "blobs=pre_data_twitter['text_preprocessed_sentence'].progress_apply(TextBlob)\n",
    "sentiment=[]\n",
    "for blob in blobs:\n",
    "    sentiment.append(blob.sentiment)\n",
    "#get the scores\n",
    "polarity=[]\n",
    "for egg in sentiment:\n",
    "    polarity.append(egg.polarity)\n",
    "pre_data_twitter['polarity_textblob'] = polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_name</th>\n",
       "      <th>date</th>\n",
       "      <th>party</th>\n",
       "      <th>text_preprocessed</th>\n",
       "      <th>text_preprocessed_sentence</th>\n",
       "      <th>like_count</th>\n",
       "      <th>polarity_textblob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ralph Brinkhaus</td>\n",
       "      <td>2021-06-15</td>\n",
       "      <td>CDU</td>\n",
       "      <td>[fußballfans, freuen, spiel, nationalmannschaf...</td>\n",
       "      <td>fußballfans freuen spiel nationalmannschaft dr...</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ralph Brinkhaus</td>\n",
       "      <td>2021-06-11</td>\n",
       "      <td>CDU</td>\n",
       "      <td>[außenpolitik, wirtschaftlich, souveränität, d...</td>\n",
       "      <td>außenpolitik wirtschaftlich souveränität digit...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ralph Brinkhaus</td>\n",
       "      <td>2021-06-11</td>\n",
       "      <td>CDU</td>\n",
       "      <td>[nachhaltig, klimawandel, kämpfen, brauchen, a...</td>\n",
       "      <td>nachhaltig klimawandel kämpfen brauchen akzept...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ralph Brinkhaus</td>\n",
       "      <td>2021-06-11</td>\n",
       "      <td>CDU</td>\n",
       "      <td>[brauchen, pandemie, bezahlen, arbeitsplätze, ...</td>\n",
       "      <td>brauchen pandemie bezahlen arbeitsplätze digit...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ralph Brinkhaus</td>\n",
       "      <td>2021-06-11</td>\n",
       "      <td>CDU</td>\n",
       "      <td>[wahldebatte, thema, zukunft, passieren, coron...</td>\n",
       "      <td>wahldebatte thema zukunft passieren corona sta...</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         full_name        date party  \\\n",
       "0  Ralph Brinkhaus  2021-06-15   CDU   \n",
       "1  Ralph Brinkhaus  2021-06-11   CDU   \n",
       "2  Ralph Brinkhaus  2021-06-11   CDU   \n",
       "3  Ralph Brinkhaus  2021-06-11   CDU   \n",
       "4  Ralph Brinkhaus  2021-06-11   CDU   \n",
       "\n",
       "                                   text_preprocessed  \\\n",
       "0  [fußballfans, freuen, spiel, nationalmannschaf...   \n",
       "1  [außenpolitik, wirtschaftlich, souveränität, d...   \n",
       "2  [nachhaltig, klimawandel, kämpfen, brauchen, a...   \n",
       "3  [brauchen, pandemie, bezahlen, arbeitsplätze, ...   \n",
       "4  [wahldebatte, thema, zukunft, passieren, coron...   \n",
       "\n",
       "                          text_preprocessed_sentence  like_count  \\\n",
       "0  fußballfans freuen spiel nationalmannschaft dr...        32.0   \n",
       "1  außenpolitik wirtschaftlich souveränität digit...         5.0   \n",
       "2  nachhaltig klimawandel kämpfen brauchen akzept...         4.0   \n",
       "3  brauchen pandemie bezahlen arbeitsplätze digit...         2.0   \n",
       "4  wahldebatte thema zukunft passieren corona sta...        24.0   \n",
       "\n",
       "   polarity_textblob  \n",
       "0                0.0  \n",
       "1                0.0  \n",
       "2                1.0  \n",
       "3               -1.0  \n",
       "4                0.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_data_twitter.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.1.2 Sentiment Analysis Bundestag Speeches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up are the Bundestag speeches from the same politicians we analyzed in the step before. Here we take our preprocessed speeches and apply TextBlob in a similar fashion as on the tweets also looping through the politicians individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a12c47b50e384b3db26da33c9f10bd6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stjepankusenic/.conda/envs/amca/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/stjepankusenic/.conda/envs/amca/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "#loop through all the politicians we want to analyze\n",
    "data=[]\n",
    "for name in tqdm(['Ralph Brinkhaus','Hermann Gröhe', 'Nadine Schön' ,'Norbert Röttgen' , 'Peter Altmaier' , 'Jens Spahn' , 'Matthias Hauer',\n",
    "            'Christian Lindner' , 'Marco Buschmann' , 'Bettina Stark-Watzinger', 'Alexander Graf Lambsdorff' , 'Johannes Vogel' , 'Konstantin Kuhle' , 'Marie-Agnes Strack-Zimmermann',\n",
    "            'Lars Klingbeil' , 'Saskia Esken' , 'Hubertus Heil' , 'Heiko Maas' , 'Martin Schulz' , 'Karamba Diaby' , 'Karl Lauterbach',\n",
    "            'Steffi Lemke' , 'Cem Özdemir' , 'Katrin Göring-Eckardt' , 'Konstantin von Notz' , 'Britta Haßelmann' , 'Sven Lehmann' , 'Annalena Baerbock',\n",
    "            'Sahra Wagenknecht' , 'Bernd Riexinger' , 'Niema Movassat' , 'Jan Korte' , 'Dietmar Bartsch' , 'Gregor Gysi' , 'Sevim Dağdelen',\n",
    "            'Alice Weidel' , 'Beatrix von Storch' , 'Joana Cotar' , 'Stephan Brandner' , 'Tino Chrupalla' , 'Götz Frömming' , 'Leif-Erik Holm']):\n",
    "    #get speeches from the specific politician\n",
    "    speeches_analyzing =pre_data_speeches.loc[pre_data_speeches['full_name']==name]\n",
    "    #create sentiment scores\n",
    "    blobs=speeches_analyzing['text_preprocessed_sentence'].apply(TextBlob)\n",
    "    sentiment=[]\n",
    "    for blob in blobs:\n",
    "        sentiment.append(blob.sentiment)\n",
    "    #get the polarity scores\n",
    "    polarity=[]\n",
    "    for egg in sentiment:\n",
    "        polarity.append(egg.polarity)\n",
    "    #get the mean and of the polarity values \n",
    "    p_mean = np.mean(polarity)\n",
    "    #get the number of positive, neutral and negative tweets\n",
    "    positive_p=0\n",
    "    neutral_p=0\n",
    "    negative_p=0\n",
    "    for item_p in polarity:\n",
    "        if item_p>0:\n",
    "            positive_p += 1\n",
    "        elif item_p<0:\n",
    "            negative_p += 1\n",
    "        else:\n",
    "            neutral_p += 1\n",
    "    #set up list to secure the values generated\n",
    "    data.append([name,p_mean,positive_p,neutral_p,negative_p]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we end up with a list containing the sentiment score means and counts of positive, negative, and neutral speeches which we transform into a dataset we can analyze further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Polarity_mean</th>\n",
       "      <th>Num_pos_speeches</th>\n",
       "      <th>Num_neutral_speeches</th>\n",
       "      <th>Num_neg_speeches</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ralph Brinkhaus</td>\n",
       "      <td>0.435714</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hermann Gröhe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nadine Schön</td>\n",
       "      <td>0.207771</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Norbert Röttgen</td>\n",
       "      <td>-0.421739</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Peter Altmaier</td>\n",
       "      <td>0.532353</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Name  Polarity_mean  Num_pos_speeches  Num_neutral_speeches  \\\n",
       "0  Ralph Brinkhaus       0.435714                 1                     0   \n",
       "1    Hermann Gröhe            NaN                 0                     0   \n",
       "2     Nadine Schön       0.207771                 3                     1   \n",
       "3  Norbert Röttgen      -0.421739                 0                     1   \n",
       "4   Peter Altmaier       0.532353                 1                     0   \n",
       "\n",
       "   Num_neg_speeches  \n",
       "0                 0  \n",
       "1                 0  \n",
       "2                 1  \n",
       "3                 2  \n",
       "4                 0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set up dataframe with all values\n",
    "dataf = pd.DataFrame(data, columns=['Name','Polarity_mean','Num_pos_speeches','Num_neutral_speeches','Num_neg_speeches'])\n",
    "dataf.to_csv('../data/processed/sentiment_scores_speeches_01.csv')\n",
    "dataf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we also add a column for the sentiment scores to have a overview. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs=pre_data_speeches['text_preprocessed_sentence'].apply(TextBlob)\n",
    "sentiment=[]\n",
    "for blob in blobs:\n",
    "    sentiment.append(blob.sentiment)\n",
    "#get the scores\n",
    "polarity=[]\n",
    "for egg in sentiment:\n",
    "    polarity.append(egg.polarity)\n",
    "pre_data_speeches['polarity_textblob'] = polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_name</th>\n",
       "      <th>date</th>\n",
       "      <th>party</th>\n",
       "      <th>text_preprocessed</th>\n",
       "      <th>text_preprocessed_sentence</th>\n",
       "      <th>polarity_textblob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1114</th>\n",
       "      <td>Jan Korte</td>\n",
       "      <td>2017-10-24</td>\n",
       "      <td>Linke</td>\n",
       "      <td>[herr, präsident, lieben, kollegin, kollege, g...</td>\n",
       "      <td>herr präsident lieben kollegin kollege geehrt ...</td>\n",
       "      <td>0.512121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1115</th>\n",
       "      <td>Marco Buschmann</td>\n",
       "      <td>2017-10-24</td>\n",
       "      <td>FDP</td>\n",
       "      <td>[herr, präsident, lieb, kollegin, kollege, kon...</td>\n",
       "      <td>herr präsident lieb kollegin kollege konstitui...</td>\n",
       "      <td>0.061111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1116</th>\n",
       "      <td>Britta Haßelmann</td>\n",
       "      <td>2017-10-24</td>\n",
       "      <td>Grüne</td>\n",
       "      <td>[geehrt, herr, präsident, dame, herr, kern, de...</td>\n",
       "      <td>geehrt herr präsident dame herr kern debatte t...</td>\n",
       "      <td>0.231579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1117</th>\n",
       "      <td>Marco Buschmann</td>\n",
       "      <td>2017-11-21</td>\n",
       "      <td>FDP</td>\n",
       "      <td>[herr, präsident, geehrt, kollegin, kollege, f...</td>\n",
       "      <td>herr präsident geehrt kollegin kollege fraktio...</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1118</th>\n",
       "      <td>Jan Korte</td>\n",
       "      <td>2017-11-21</td>\n",
       "      <td>Linke</td>\n",
       "      <td>[geehrt, herr, präsident, dame, herr, ernst, z...</td>\n",
       "      <td>geehrt herr präsident dame herr ernst zeit hum...</td>\n",
       "      <td>0.293333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             full_name        date  party  \\\n",
       "1114         Jan Korte  2017-10-24  Linke   \n",
       "1115   Marco Buschmann  2017-10-24    FDP   \n",
       "1116  Britta Haßelmann  2017-10-24  Grüne   \n",
       "1117   Marco Buschmann  2017-11-21    FDP   \n",
       "1118         Jan Korte  2017-11-21  Linke   \n",
       "\n",
       "                                      text_preprocessed  \\\n",
       "1114  [herr, präsident, lieben, kollegin, kollege, g...   \n",
       "1115  [herr, präsident, lieb, kollegin, kollege, kon...   \n",
       "1116  [geehrt, herr, präsident, dame, herr, kern, de...   \n",
       "1117  [herr, präsident, geehrt, kollegin, kollege, f...   \n",
       "1118  [geehrt, herr, präsident, dame, herr, ernst, z...   \n",
       "\n",
       "                             text_preprocessed_sentence  polarity_textblob  \n",
       "1114  herr präsident lieben kollegin kollege geehrt ...           0.512121  \n",
       "1115  herr präsident lieb kollegin kollege konstitui...           0.061111  \n",
       "1116  geehrt herr präsident dame herr kern debatte t...           0.231579  \n",
       "1117  herr präsident geehrt kollegin kollege fraktio...           0.750000  \n",
       "1118  geehrt herr präsident dame herr ernst zeit hum...           0.293333  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_data_speeches.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6.2 Sentiment Analysis with SentiWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a second approach for sentiment analysis we tried using SentiWS a often used German sentiment dictionary. It also calculates the sentiment of a given sentence with a polarity score from -1 to 1 and has over 3000 base words and over 30000 word forms in its dictionary. Not only does it use adjectives and adverbs but also nouns and verbs to calculate the sentiment score. For the code implementation we could use a extension from the spacy pipeline used in preprocessing. With this spaCySentiWS we can add the application of the dictionary directly into the preprocessing pipeline. Therefore, we write a new preprocessing pipeline which is changed a little from original pipeline to get the sentiment scores of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert pipeline to add sentiws preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_data_twitter= pd.read_csv(\"../data/raw/tweets_explored.csv\")[0:100]\n",
    "pre_data_speeches= pd.read_csv(\"../data/raw/speeches_explored.csv\")[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"Remove non alphabetic words\")\n",
    "def remove_non_alpha(doc):\n",
    "    return [token for token in doc if token.is_alpha]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.factory(\"Detect languages\")\n",
    "def create_language_detector(nlp, name):\n",
    "    return LanguageDetector(language_detection_function=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.factory(\"Sentiment Appplication\")\n",
    "def create_sentiment_dictionary(nlp, name):\n",
    "    return spaCySentiWS(sentiws_path = \"../data/raw/Sentiment/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"Keep only German documents\")\n",
    "def remove_non_german(doc):\n",
    "    res = [sent for sent in doc.sents if sent._.language[\"language\"] == \"de\"]\n",
    "    if res:\n",
    "        return [token for sent in res for token in sent]\n",
    "    else:\n",
    "        return Doc(Vocab([]), words=[], spaces=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"Remove stopwords\")\n",
    "def remove_stopwords(doc): \n",
    "    return [token for token in doc if not token.is_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"Lemmatize text\")\n",
    "def lemmatize_text(doc):\n",
    "    return [token.lemma_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"Lowercase Text\")\n",
    "def lowercase(doc):\n",
    "    return [token.lower() for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_codes = re.compile(\"[\"\n",
    "                         u\"\\U0001F600-\\U0001F64F\"\n",
    "                         u\"\\U0001F300-\\U0001F5FF\"\n",
    "                         u\"\\U0001F680-\\U0001F6FF\"\n",
    "                         u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "                         u\"\\U00002500-\\U00002BEF\"\n",
    "                         u\"\\U00002702-\\U000027B0\"\n",
    "                         u\"\\U00002702-\\U000027B0\"\n",
    "                         u\"\\U000024C2-\\U0001F251\"\n",
    "                         u\"\\U0001f926-\\U0001f937\"\n",
    "                         u\"\\U00010000-\\U0010ffff\"\n",
    "                         u\"\\u2640-\\u2642\"\n",
    "                         u\"\\u2600-\\u2B55\"\n",
    "                         u\"\\u200d\"\n",
    "                         u\"\\u23cf\"\n",
    "                         u\"\\u23e9\"\n",
    "                         u\"\\u231a\"\n",
    "                         u\"\\ufe0f\"\n",
    "                         u\"\\u3030\"\n",
    "                         \"]+\", re.UNICODE)\n",
    "\n",
    "@Language.component(\"Remove emojis\")\n",
    "def remove_emojis(doc):\n",
    "    doc = [token.text for token in doc if not re.match(emoji, token.text)]\n",
    "    doc = ' '.join(doc)\n",
    "    return nlp_twitter.make_doc(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"Remove URLs\")\n",
    "def remove_urls(doc):\n",
    "    doc = [token.text for token in doc if not token.like_url]\n",
    "    doc = ' '.join(doc)\n",
    "    return nlp_twitter.make_doc(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"Remove mentions\")\n",
    "def remove_mentions(doc):\n",
    "    doc = [token.text for token in doc if not re.match(\"@.*\", token.text)]\n",
    "    doc = ' '.join(doc)\n",
    "    return nlp_twitter.make_doc(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"Remove stopwords and punctuation\")\n",
    "def remove_stopwords(doc):\n",
    "    doc = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy_sentiws.spaCySentiWS at 0x7fd841d4fc40>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create spacy pipeline\n",
    "nlp_tweets_sentiws = spacy.load('de_core_news_sm')\n",
    "nlp_tweets_sentiws.Defaults.stop_words |= {\"amp\", \"rt\"}\n",
    "\n",
    "# The add_pipe function appends our functions to the default pipeline.\n",
    "nlp_tweets_sentiws.add_pipe(\"sentencizer\", last=True)\n",
    "nlp_tweets_sentiws.add_pipe(\"Detect languages\", name='Detect languages', last=True)\n",
    "nlp_tweets_sentiws.add_pipe(\"Keep only German documents\", name='Keep only German documents', last=True)\n",
    "nlp_tweets_sentiws.add_pipe(\"Remove non alphabetic words\", name=\"Remove non alphabetic words\", last=True)\n",
    "nlp_tweets_sentiws.add_pipe(\"Remove stopwords\", name=\"Remove stopwords\", last=True)\n",
    "# nlp_tweets.add_pipe(\"Lemmatize text\", name=\"Lemmatize text\", last=True)\n",
    "# nlp_tweets.add_pipe(\"Lowercase Text\", name=\"Lowercase Text\", last=True)\n",
    "nlp_tweets_sentiws.add_pipe(\"Sentiment Appplication\", name=\"Sentiment Appplication\", last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.2.1 Sentiment Analysis for Twitter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we want to have a look at our Twitter data again. As with the TextBlob analysis we want to go through all the in dividual politicians and therefore create a loop. In difference to the first approach we used the raw data here as we want to apply our new pipeline to the dataset. After the application of the pipeline with sentiment functionality we go through the the preprocessed tweets and take the calculated sentiment of each token. Next we add the scores together and calculate the means for each tweet and then for the individual politician. Again we count the number of positive, negative, and neutral tweets as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c61e0660f1f4fdd8334aef0ec9b30ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a8b3051dc024b8d83958a774928bda5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xy/yfm2dhtj3jvfy85hqy16wmk00000gn/T/ipykernel_22192/393194382.py:23: RuntimeWarning: Mean of empty slice\n",
      "  sentence_score=np.nanmean(sentence_sum)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3c03f68c975458c90a6d1bf703ac5b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xy/yfm2dhtj3jvfy85hqy16wmk00000gn/T/ipykernel_22192/393194382.py:25: RuntimeWarning: Mean of empty slice\n",
      "  politician_score=np.nanmean(politician_sum)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c7225b419c549bd902ff45b6853d0d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb22382cfedc467d9d2e15d66a513f88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a01628f53f454228b880b770d634f897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e77aa01bac34f39b2a3268a01ebe0c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "660431d56755426e8bd9415638630505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd3eeeed144e40b0823d31ee8b4eeaad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee66aa03a4c842b489a549980f309987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd51411ec6c3490782f828cbf66f0e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5249d95e68654d2690bb1e8eaa6a2d27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99bea3d0d13d455981cf702b8b4b25fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63d3209e3e0448aca7a01f036cc7db6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9250b66949ce4d69ade514c19eb21217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f9b4bc0a9cb46b0a397b8bd021e1947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bffb4e48b2f143be97308fbeca5fca3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca9959d1f5c049109f7ce424f7a0cf60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bac242312534451acf4d73314876e79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0ce174476c7496cbc94a6bb2a42781e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5e90e7d355441a89dd1560246085511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23beea97c65445a8b5fe55d22bbf615d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2da82e4cc5844f98e72524884f8d2ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f440cc1610c94af8ba7dd27db64ec234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a67313086bef4658835713c269eeced5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab3010324a524bcda0ed34772ad192d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef872c6128854bd793b404b4742ba146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a425c3fc0d384b6fb68c0c234df48cf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ccebca7a6b948cd8fc074d027aa93be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b24653ced6954e1b95cca3b9b8660de2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0619813ff55c4bae988277fb9befc674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fac655f91960411da8f061729dd2eeb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78b5046291c54e51881286937de5ed7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "261168a910534a388a430fa08be04c3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d82fd4ffc38348bd8afbcbc2c3519282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "543119e5871a46ef85b9879052cfc849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed602c7652464c569749f7a9df3ac56e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "275db0518be74de0a24c7a0348cda130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "866239ae22894372aa97d1cde405f237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "465f2f1bb5e944b48a64a4d5bdab3ef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "135c4bdc30094aeca1cc88e86cc655c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a5817b0e9af4fa6a05395795f852914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "518f9538a7e94378a3b5425b2ef85a43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Apply the sentiment anaylsis to the Twitter accounts of the politicians\n",
    "data=[]\n",
    "for name in tqdm(['Ralph Brinkhaus','Hermann Gröhe', 'Nadine Schön' ,'Norbert Röttgen' , 'Peter Altmaier' , 'Jens Spahn' , 'Matthias Hauer',\n",
    "            'Christian Lindner' , 'Marco Buschmann' , 'Bettina Stark-Watzinger', 'Alexander Graf Lambsdorff' , 'Johannes Vogel' , 'Konstantin Kuhle' , 'Marie-Agnes Strack-Zimmermann',\n",
    "            'Lars Klingbeil' , 'Saskia Esken' , 'Hubertus Heil' , 'Heiko Maas' , 'Martin Schulz' , 'Karamba Diaby' , 'Karl Lauterbach',\n",
    "            'Steffi Lemke' , 'Cem Özdemir' , 'Katrin Göring-Eckardt' , 'Konstantin von Notz' , 'Britta Haßelmann' , 'Sven Lehmann' , 'Annalena Baerbock',\n",
    "            'Sahra Wagenknecht' , 'Bernd Riexinger' , 'Niema Movassat' , 'Jan Korte' , 'Dietmar Bartsch' , 'Gregor Gysi' , 'Sevim Dağdelen',\n",
    "            'Alice Weidel' , 'Beatrix von Storch' , 'Joana Cotar' , 'Stephan Brandner' , 'Tino Chrupalla' , 'Götz Frömming' , 'Leif-Erik Holm']):\n",
    "    #get tweets from the specific politician\n",
    "    tweets_analyzing = pre_data_twitter.loc[pre_data_twitter['full_name']==name]\n",
    "    tweets_analyzing1 = tweets_analyzing.text.progress_apply(nlp_tweets_sentiws)\n",
    "    #get the sentiment of the tweets\n",
    "    politician_sum=[]\n",
    "    for sentence in tweets_analyzing1:\n",
    "        sentence_sum=[]\n",
    "        for token in sentence:\n",
    "            if token._.sentiws == None:\n",
    "                a=0\n",
    "            elif token._.sentiws == 'nan':\n",
    "                a=0\n",
    "            else:\n",
    "                sentence_sum.append(token._.sentiws)\n",
    "        sentence_score=np.nanmean(sentence_sum)\n",
    "        politician_sum.append(sentence_score)\n",
    "    politician_score=np.nanmean(politician_sum)\n",
    "    #get the number of positive, neutral and negative tweets\n",
    "    positive_p=0\n",
    "    neutral_p=0\n",
    "    negative_p=0\n",
    "    for item_p in politician_sum:\n",
    "        if item_p>0:\n",
    "            positive_p += 1\n",
    "        elif item_p<0:\n",
    "            negative_p += 1\n",
    "        elif item_p == 'nan':\n",
    "            neutral_p += 1\n",
    "        else:\n",
    "            neutral_p += 1\n",
    "    #set up list to secure the values generated\n",
    "    data.append([name,politician_score,positive_p,neutral_p,negative_p])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transform the list into a dataframe that we can again analyze further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Polarity_mean</th>\n",
       "      <th>Num_pos_tweets</th>\n",
       "      <th>Num_neutral_tweets</th>\n",
       "      <th>Num_neg_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ralph Brinkhaus</td>\n",
       "      <td>0.02611</td>\n",
       "      <td>48</td>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hermann Gröhe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nadine Schön</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Norbert Röttgen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Peter Altmaier</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Name  Polarity_mean  Num_pos_tweets  Num_neutral_tweets  \\\n",
       "0  Ralph Brinkhaus        0.02611              48                  27   \n",
       "1    Hermann Gröhe            NaN               0                   0   \n",
       "2     Nadine Schön            NaN               0                   0   \n",
       "3  Norbert Röttgen            NaN               0                   0   \n",
       "4   Peter Altmaier            NaN               0                   0   \n",
       "\n",
       "   Num_neg_tweets  \n",
       "0              25  \n",
       "1               0  \n",
       "2               0  \n",
       "3               0  \n",
       "4               0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set up dataframe with all values\n",
    "dataf = pd.DataFrame(data, columns=['Name','Polarity_mean','Num_pos_tweets','Num_neutral_tweets','Num_neg_tweets'])\n",
    "dataf.to_csv('../data/processed/sentiment_scores_tweets_sentiws_01.csv')\n",
    "dataf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.2.2 Sentiment Analysis for Bundestag Speeches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we want to have a look at the Bundestag speeches and see how the SentiWS dictionary clasifies them in terms of sentiment. We use the same procedure as with the tweets before to calculate the scores and the counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca0b24100fd74c389f81a76a4b663dcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b2a7eefe4d44de58fe5456472c1c0f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "735210ad0b3c4849896719c70f1db9a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2424ec80b9a64ad0bcb79db0a2a9b47d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c76e1b549d7d468ab00ed1473bb7fea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xy/yfm2dhtj3jvfy85hqy16wmk00000gn/T/ipykernel_22192/1413779232.py:23: RuntimeWarning: Mean of empty slice\n",
      "  sentence_score=np.nanmean(sentence_sum)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5055e1f73f74fd5a850183719c6f76b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "238a585a02254de19b500ab3e4bd9983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77825905e9984c08978b1ac2d07d2d95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xy/yfm2dhtj3jvfy85hqy16wmk00000gn/T/ipykernel_22192/1413779232.py:25: RuntimeWarning: Mean of empty slice\n",
      "  politician_score=np.nanmean(politician_sum)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ec97f3215334242a5d323b00d0598d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35f6131316cb42d7a29436ab1dfb968d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe644616d67c4cf4a07e083f402482aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12d3dead552e445f91f245732f13f4c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e465c1744ac47428b28c0dfdc642af4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41a6703a3530421facebc8df4fe86232",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eeec84b01434313b17822174ea4fb15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b2f5cb89231489d9a444d041f3f3a5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82dc72b945f746c5b899bc791d7237e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "089516aad7e049b08f6461b436c62cd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96f043f62dea4849b07898e534428512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0ad0ab85dfd423e8d16a20b44926f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e5f3d9f20ae4bf79ae6c6ea2c62e2e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b09bc7eb567e4bcb99768a2b03651ece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "215cb7e20d394921b9d95674c0e79496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f51bfa6cc2414274be9cb536fae0b3ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff0233bf18f6433ba43090c717919d6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d8cf56720b04bcca5a37a72bf4443c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac40bf613dea4f0bac77f2dc4cced7e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0b042ecef1949d7b589380a9fb76566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1967901221ee4f57b135af5d1c46f27a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44ea9a1efe84415289c8ab270fd9b3c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4828be3a9a5d4b929e12ef9227ec7785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91c5439035904cd7b5fa9dba98141f7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a815a227d1042069d7f76382a1ebbd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "027d9962f67f4c23be70f5457108b0bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df0258f2319d4e58ad22459ace2efc20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be6487441a8449c2b0a58d626902f883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71540ec47c6b4a0b93176c5a88817c90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1748335f81b4faabdf21577746b3180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb5c08c455cf4ef8b542b35befcb63a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebae6ee113244774890c26f94d48d722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5af47dd59cde4b94a0885d65892512c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baa7d3c9c94449b384d7ab2e7ccfbbb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fc9ede602154b799ca83ce580332bcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Apply the sentiment analysis to the speeches accounts of the politicians\n",
    "data=[]\n",
    "for name in tqdm(['Ralph Brinkhaus','Hermann Gröhe', 'Nadine Schön' ,'Norbert Röttgen' , 'Peter Altmaier' , 'Jens Spahn' , 'Matthias Hauer',\n",
    "            'Christian Lindner' , 'Marco Buschmann' , 'Bettina Stark-Watzinger', 'Alexander Graf Lambsdorff' , 'Johannes Vogel' , 'Konstantin Kuhle' , 'Marie-Agnes Strack-Zimmermann',\n",
    "            'Lars Klingbeil' , 'Saskia Esken' , 'Hubertus Heil' , 'Heiko Maas' , 'Martin Schulz' , 'Karamba Diaby' , 'Karl Lauterbach',\n",
    "            'Steffi Lemke' , 'Cem Özdemir' , 'Katrin Göring-Eckardt' , 'Konstantin von Notz' , 'Britta Haßelmann' , 'Sven Lehmann' , 'Annalena Baerbock',\n",
    "            'Sahra Wagenknecht' , 'Bernd Riexinger' , 'Niema Movassat' , 'Jan Korte' , 'Dietmar Bartsch' , 'Gregor Gysi' , 'Sevim Dağdelen',\n",
    "            'Alice Weidel' , 'Beatrix von Storch' , 'Joana Cotar' , 'Stephan Brandner' , 'Tino Chrupalla' , 'Götz Frömming' , 'Leif-Erik Holm']):\n",
    "    #get speeches from the specific politician\n",
    "    speeches_analyzing = pre_data_speeches.loc[pre_data_speeches['full_name']==name]\n",
    "    speeches_analyzing1 = speeches_analyzing.text.progress_apply(nlp_tweets_sentiws)\n",
    "    #get the sentiment of the tweets\n",
    "    politician_sum=[]\n",
    "    for sentence in speeches_analyzing1:\n",
    "        sentence_sum=[]\n",
    "        for token in sentence:\n",
    "            if token._.sentiws == None:\n",
    "                a=0\n",
    "            elif token._.sentiws == 'nan':\n",
    "                a=0\n",
    "            else:\n",
    "                sentence_sum.append(token._.sentiws)\n",
    "        sentence_score=np.nanmean(sentence_sum)\n",
    "        politician_sum.append(sentence_score)\n",
    "    politician_score=np.nanmean(politician_sum)\n",
    "    #get the number of positive, neutral and negative tweets\n",
    "    positive_p=0\n",
    "    neutral_p=0\n",
    "    negative_p=0\n",
    "    for item_p in politician_sum:\n",
    "        if item_p>0:\n",
    "            positive_p += 1\n",
    "        elif item_p<0:\n",
    "            negative_p += 1\n",
    "        elif item_p == 'nan':\n",
    "            neutral_p += 1\n",
    "        else:\n",
    "            neutral_p += 1\n",
    "    #set up list to secure the values generated\n",
    "    data.append([name,politician_score,positive_p,neutral_p,negative_p])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And afterwards create a dataframe from the data for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Polarity_mean</th>\n",
       "      <th>Num_pos_speeches</th>\n",
       "      <th>Num_neutral_speeches</th>\n",
       "      <th>Num_neg_speeches</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ralph Brinkhaus</td>\n",
       "      <td>-0.024537</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hermann Gröhe</td>\n",
       "      <td>0.082749</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nadine Schön</td>\n",
       "      <td>-0.042718</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Norbert Röttgen</td>\n",
       "      <td>-0.099983</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Peter Altmaier</td>\n",
       "      <td>0.140338</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Name  Polarity_mean  Num_pos_speeches  Num_neutral_speeches  \\\n",
       "0  Ralph Brinkhaus      -0.024537                 0                     0   \n",
       "1    Hermann Gröhe       0.082749                 3                     0   \n",
       "2     Nadine Schön      -0.042718                 1                     0   \n",
       "3  Norbert Röttgen      -0.099983                 0                     1   \n",
       "4   Peter Altmaier       0.140338                 3                     2   \n",
       "\n",
       "   Num_neg_speeches  \n",
       "0                 2  \n",
       "1                 1  \n",
       "2                 1  \n",
       "3                 2  \n",
       "4                 0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set up dataframe with all values\n",
    "dataf = pd.DataFrame(data, columns=['Name','Polarity_mean','Num_pos_speeches','Num_neutral_speeches','Num_neg_speeches'])\n",
    "dataf.to_csv('../data/processed/sentiment_scores_speeches_sentiws_01.csv')\n",
    "dataf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the polarity scores for the SentiWS dictionary seem to be less significant due to their absolute values being smaller, we decided to conduct the further in depth analysis of the sentiment with the TextBlob model. These smaller values with the SentiWS dictionary could be a result from our loop used because the mean values could be to unrobust to mean neutral tweets. Another possible explenation could be htat there are no great outliers for the tweet or speech sentiments as the value range for polarity is only from -1 to 1. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
