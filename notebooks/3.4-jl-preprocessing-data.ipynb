{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3.4 Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can use the data for topic modeling and sentiment analysis, wen need to preprocess the data. This will be done with individual spacy pipelines for each use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy_langdetect import LanguageDetector\n",
    "from spacy.tokens.doc import Doc\n",
    "from spacy.vocab import Vocab\n",
    "# python -m spacy download de_core_news_sm\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 Prepare spacy pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For creating the spacy pipelines we define individual components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"Remove non alphabetic words\")\n",
    "def remove_non_alpha(doc):\n",
    "    return [token for token in doc if token.is_alpha]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.factory(\"Detect languages\")\n",
    "def create_language_detector(nlp, name):\n",
    "    return LanguageDetector(language_detection_function=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"Keep only German documents\")\n",
    "def remove_non_german(doc):\n",
    "    res = [sent for sent in doc.sents if sent._.language[\"language\"] == \"de\"]\n",
    "    if res:\n",
    "        return [token for sent in res for token in sent]\n",
    "    else:\n",
    "        return Doc(Vocab([]), words=[], spaces=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"Remove stopwords\")\n",
    "def remove_stopwords(doc): \n",
    "    return [token for token in doc if not token.is_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"Lemmatize text\")\n",
    "def lemmatize_text(doc):\n",
    "    return [token.lemma_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"Lowercase Text\")\n",
    "def lowercase(doc):\n",
    "    return [token.lower() for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_codes = re.compile(\"[\"\n",
    "                         u\"\\U0001F600-\\U0001F64F\"\n",
    "                         u\"\\U0001F300-\\U0001F5FF\"\n",
    "                         u\"\\U0001F680-\\U0001F6FF\"\n",
    "                         u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "                         u\"\\U00002500-\\U00002BEF\"\n",
    "                         u\"\\U00002702-\\U000027B0\"\n",
    "                         u\"\\U00002702-\\U000027B0\"\n",
    "                         u\"\\U000024C2-\\U0001F251\"\n",
    "                         u\"\\U0001f926-\\U0001f937\"\n",
    "                         u\"\\U00010000-\\U0010ffff\"\n",
    "                         u\"\\u2640-\\u2642\"\n",
    "                         u\"\\u2600-\\u2B55\"\n",
    "                         u\"\\u200d\"\n",
    "                         u\"\\u23cf\"\n",
    "                         u\"\\u23e9\"\n",
    "                         u\"\\u231a\"\n",
    "                         u\"\\ufe0f\"\n",
    "                         u\"\\u3030\"\n",
    "                         \"]+\", re.UNICODE)\n",
    "\n",
    "@Language.component(\"Remove emojis\")\n",
    "def remove_emojis(doc):\n",
    "    doc = [token.text for token in doc if not re.match(emoji_codes, token.text)]\n",
    "    doc = ' '.join(doc)\n",
    "    return nlp_twitter.make_doc(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"Remove URLs\")\n",
    "def remove_urls(doc):\n",
    "    doc = [token.text for token in doc if not token.like_url]\n",
    "    doc = ' '.join(doc)\n",
    "    return nlp_twitter.make_doc(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"Remove mentions\")\n",
    "def remove_mentions(doc):\n",
    "    doc = [token.text for token in doc if not re.match(\"@.*\", token.text)]\n",
    "    doc = ' '.join(doc)\n",
    "    return nlp_twitter.make_doc(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"Remove stopwords and punctuation\")\n",
    "def remove_stopwords(doc):\n",
    "    doc = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 Topic modeling preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude not needed pipeline elements\n",
    "pipeline_exclude = ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'ner', 'morphologizer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  3.4.2.1 Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "tweets_explored = pd.read_csv(\"../data/interim/tweets_explored.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spacy pipeline\n",
    "nlp_tweets = spacy.load('de_core_news_sm', exclude=pipeline_exclude)\n",
    "nlp_tweets.Defaults.stop_words |= {\"amp\", \"rt\"}\n",
    "\n",
    "# Add needed pipeline components\n",
    "nlp_tweets.add_pipe(\"sentencizer\", last=True)\n",
    "nlp_tweets.add_pipe(\"Detect languages\", name='Detect languages', last=True)\n",
    "nlp_tweets.add_pipe(\"Keep only German documents\", name='Keep only German documents', last=True)\n",
    "nlp_tweets.add_pipe(\"Remove non alphabetic words\", name=\"Remove non alphabetic words\", last=True)\n",
    "nlp_tweets.add_pipe(\"Remove stopwords\", name=\"Remove stopwords\", last=True)\n",
    "nlp_tweets.add_pipe(\"Lemmatize text\", name=\"Lemmatize text\", last=True)\n",
    "nlp_tweets.add_pipe(\"Lowercase Text\", name=\"Lowercase Text\", last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply pipeline to text\n",
    "tweets_explored[\"text_preprocessed\"] = tweets_explored.text.progress_apply(nlp_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add sentence structure\n",
    "tweets_explored[\"text_preprocessed_sentence\"] = tweets_explored[\"text_preprocessed\"].progress_apply(\n",
    "    lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset needed data\n",
    "tweets_preprocessed = tweets_explored[[\"full_name\", \"date\", \"party\", \"text\", \"text_preprocessed\",\n",
    "                                       \"text_preprocessed_sentence\", 'retweet_count', 'like_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop empty texts\n",
    "tweets_preprocessed.replace('', np.NaN, inplace=True)\n",
    "tweets_preprocessed.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data as pickle file\n",
    "pickle.dump(tweets_preprocessed, open(\"../data/processed/tweets_processed.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "speeches_explored = pd.read_csv(\"../data/interim/speeches_explored.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.lowercase(doc)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create spacy pipeline\n",
    "nlp_speeches = spacy.load('de_core_news_sm', exclude=pipeline_exclude)\n",
    "\n",
    "# Add needed pipeline components\n",
    "nlp_speeches.add_pipe('sentencizer', last=True)\n",
    "nlp_speeches.add_pipe(\"Detect languages\", name='Detect languages', last=True)\n",
    "nlp_speeches.add_pipe(\"Keep only German documents\", name='Keep only German documents', last=True)\n",
    "nlp_speeches.add_pipe(\"Remove non alphabetic words\", name=\"Remove non alphabetic words\", last=True)\n",
    "nlp_speeches.add_pipe(\"Remove stopwords\", name=\"Remove stopwords\", last=True)\n",
    "nlp_speeches.add_pipe(\"Lemmatize text\", name=\"Lemmatize text\", last=True)\n",
    "nlp_speeches.add_pipe(\"Lowercase Text\", name=\"Lowercase Text\", last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d15b1b45e59b490784f7e4a5e19b7150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2985 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply pipeline to text\n",
    "speeches_explored[\"text_preprocessed\"] = speeches_explored.text.progress_apply(nlp_speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "831d21ced4694cb69d0274f6258cc00b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2985 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add sentence structure\n",
    "speeches_explored[\"text_preprocessed_sentence\"] = speeches_explored[\"text_preprocessed\"].progress_apply(\n",
    "    lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset needed data\n",
    "speeches_preprocessed = speeches_explored[[\"full_name\", \"date\", \"party\", \"text\",\n",
    "                                           \"text_preprocessed\", \"text_preprocessed_sentence\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional preprocessing for Bertopic model\n",
    "long_string_speeches= ' '.join(speeches_preprocessed.text_preprocessed_sentence.tolist())\n",
    "counter_speeches = Counter(long_string_speeches.split())\n",
    "most_frequent_words = []\n",
    "for item in counter_speeches.most_common(200):\n",
    "    most_frequent_words.append(item[0])\n",
    "    \n",
    "# Define function for removing frequent words\n",
    "def remove_frequent_words(words_list, most_frequent_words):\n",
    "    return [word for word in words_list if word not in most_frequent_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f47c0fe8b59e4210ac828d90045d3743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2985 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aa410a0e20a47e4b0079a9b2b5fb22a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2985 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add columns with preprocessed text and removed frequent words\n",
    "speeches_preprocessed[\"text_preprocessed_infrequent\"] = speeches_preprocessed.text_preprocessed.progress_apply(remove_frequent_words,most_frequent_words = most_frequent_words)\n",
    "speeches_preprocessed[\"text_preprocessed_infrequent_sentence\"] = speeches_preprocessed[\"text_preprocessed_infrequent\"].progress_apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop empty texts\n",
    "speeches_preprocessed.replace('', np.NaN, inplace=True)\n",
    "speeches_preprocessed.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data as pickle file\n",
    "pickle.dump(speeches_preprocessed, open(\"../data/processed/speeches_processed.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.3 Sentiment analysis preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.3.1 Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.3.1 Speeches"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
