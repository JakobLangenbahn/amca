{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pipeline from spacy\n",
    "# python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn' based on false positives\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy_langdetect import LanguageDetector\n",
    "from spacy.tokens.doc import Doc\n",
    "from spacy.vocab import Vocab\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem Tweets die über mehrere gehen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Speeches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein Problem sind die Fälle wo wir englische und deutsche sätze in einem tweets haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"Remove non alphabetic words\")\n",
    "def remove_non_alpha(doc):\n",
    "    return [token for token in doc if token.is_alpha]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.factory(\"Detect languages\")\n",
    "def create_language_detector(nlp, name):\n",
    "    return LanguageDetector(language_detection_function=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"Keep only German documents\")\n",
    "def remove_non_german(doc):\n",
    "    res = [sent for sent in doc.sents if sent._.language[\"language\"] == \"de\"]\n",
    "    if res:\n",
    "        return [token for sent in res for token in sent]\n",
    "    else:\n",
    "        return Doc(Vocab([]), words=[], spaces=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"Remove stopwords\")\n",
    "def remove_stopwords(doc): \n",
    "    return [token for token in doc if not token.is_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"Lemmatize text\")\n",
    "def lemmatize_text(doc):\n",
    "    return [token.lemma_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"Lowercase Text\")\n",
    "def lowercase(doc):\n",
    "    return [token.lower() for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_codes = re.compile(\"[\"\n",
    "                         u\"\\U0001F600-\\U0001F64F\"\n",
    "                         u\"\\U0001F300-\\U0001F5FF\"\n",
    "                         u\"\\U0001F680-\\U0001F6FF\"\n",
    "                         u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "                         u\"\\U00002500-\\U00002BEF\"\n",
    "                         u\"\\U00002702-\\U000027B0\"\n",
    "                         u\"\\U00002702-\\U000027B0\"\n",
    "                         u\"\\U000024C2-\\U0001F251\"\n",
    "                         u\"\\U0001f926-\\U0001f937\"\n",
    "                         u\"\\U00010000-\\U0010ffff\"\n",
    "                         u\"\\u2640-\\u2642\"\n",
    "                         u\"\\u2600-\\u2B55\"\n",
    "                         u\"\\u200d\"\n",
    "                         u\"\\u23cf\"\n",
    "                         u\"\\u23e9\"\n",
    "                         u\"\\u231a\"\n",
    "                         u\"\\ufe0f\"\n",
    "                         u\"\\u3030\"\n",
    "                         \"]+\", re.UNICODE)\n",
    "\n",
    "@Language.component(\"Remove emojis\")\n",
    "def remove_emojis(doc):\n",
    "    doc = [token.text for token in doc if not re.match(emoji, token.text)]\n",
    "    doc = ' '.join(doc)\n",
    "    return nlp_twitter.make_doc(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"Remove URLs\")\n",
    "def remove_urls(doc):\n",
    "    doc = [token.text for token in doc if not token.like_url]\n",
    "    doc = ' '.join(doc)\n",
    "    return nlp_twitter.make_doc(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"Remove mentions\")\n",
    "def remove_mentions(doc):\n",
    "    doc = [token.text for token in doc if not re.match(\"@.*\", token.text)]\n",
    "    doc = ' '.join(doc)\n",
    "    return nlp_twitter.make_doc(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"Remove stopwords and punctuation\")\n",
    "def remove_stopwords(doc):\n",
    "    doc = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_exclude = ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'ner', 'morphologizer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein Problem sind die Fälle wo wir englische und deutsche sätze in einem tweets haben."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.1 Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_explored = pd.read_csv(\"../data/interim/tweets_explored.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>party</th>\n",
       "      <th>full_name</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mit allen Fußballfans freue ich mich heute auf...</td>\n",
       "      <td>3</td>\n",
       "      <td>32.0</td>\n",
       "      <td>CDU</td>\n",
       "      <td>Ralph Brinkhaus</td>\n",
       "      <td>2021-06-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mit @antennedowideit habe ich außerdem noch üb...</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CDU</td>\n",
       "      <td>Ralph Brinkhaus</td>\n",
       "      <td>2021-06-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wenn wir nachhaltig gegen den #Klimawandel käm...</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>CDU</td>\n",
       "      <td>Ralph Brinkhaus</td>\n",
       "      <td>2021-06-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wir brauchen nach der Pandemie gut bezahlte #A...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>CDU</td>\n",
       "      <td>Ralph Brinkhaus</td>\n",
       "      <td>2021-06-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In der #Wahldebatte von @welt und @insm ging e...</td>\n",
       "      <td>2</td>\n",
       "      <td>24.0</td>\n",
       "      <td>CDU</td>\n",
       "      <td>Ralph Brinkhaus</td>\n",
       "      <td>2021-06-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164455</th>\n",
       "      <td>@andreasloeschel @zeitonline @LauraCwiertnia C...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Grüne</td>\n",
       "      <td>Annalena Baerbock</td>\n",
       "      <td>2017-11-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164456</th>\n",
       "      <td>@DieLinkeBrdburg @Die_Gruenen Ich nehme das an...</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Grüne</td>\n",
       "      <td>Annalena Baerbock</td>\n",
       "      <td>2017-11-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164457</th>\n",
       "      <td>@LucaBrunsch widerspricht uns Grünen doch gar ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Grüne</td>\n",
       "      <td>Annalena Baerbock</td>\n",
       "      <td>2017-11-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164458</th>\n",
       "      <td>Weise Fußballerwahrheiten  https://t.co/lQzp1p...</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Grüne</td>\n",
       "      <td>Annalena Baerbock</td>\n",
       "      <td>2017-11-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164459</th>\n",
       "      <td>@c_lindner @Die_Gruenen Na sehen Sie. Bei über...</td>\n",
       "      <td>1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Grüne</td>\n",
       "      <td>Annalena Baerbock</td>\n",
       "      <td>2017-11-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>164460 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  retweet_count  \\\n",
       "0       Mit allen Fußballfans freue ich mich heute auf...              3   \n",
       "1       Mit @antennedowideit habe ich außerdem noch üb...              0   \n",
       "2       Wenn wir nachhaltig gegen den #Klimawandel käm...              0   \n",
       "3       Wir brauchen nach der Pandemie gut bezahlte #A...              0   \n",
       "4       In der #Wahldebatte von @welt und @insm ging e...              2   \n",
       "...                                                   ...            ...   \n",
       "164455  @andreasloeschel @zeitonline @LauraCwiertnia C...              0   \n",
       "164456  @DieLinkeBrdburg @Die_Gruenen Ich nehme das an...              0   \n",
       "164457  @LucaBrunsch widerspricht uns Grünen doch gar ...              0   \n",
       "164458  Weise Fußballerwahrheiten  https://t.co/lQzp1p...              1   \n",
       "164459  @c_lindner @Die_Gruenen Na sehen Sie. Bei über...              1   \n",
       "\n",
       "        like_count  party          full_name        date  \n",
       "0             32.0    CDU    Ralph Brinkhaus  2021-06-15  \n",
       "1              5.0    CDU    Ralph Brinkhaus  2021-06-11  \n",
       "2              4.0    CDU    Ralph Brinkhaus  2021-06-11  \n",
       "3              2.0    CDU    Ralph Brinkhaus  2021-06-11  \n",
       "4             24.0    CDU    Ralph Brinkhaus  2021-06-11  \n",
       "...            ...    ...                ...         ...  \n",
       "164455         1.0  Grüne  Annalena Baerbock  2017-11-09  \n",
       "164456        13.0  Grüne  Annalena Baerbock  2017-11-08  \n",
       "164457         0.0  Grüne  Annalena Baerbock  2017-11-08  \n",
       "164458         6.0  Grüne  Annalena Baerbock  2017-11-05  \n",
       "164459        12.0  Grüne  Annalena Baerbock  2017-11-05  \n",
       "\n",
       "[164460 rows x 6 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_explored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.lowercase(doc)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create spacy pipeline\n",
    "nlp_tweets = spacy.load('de_core_news_sm', exclude=pipeline_exclude)\n",
    "nlp_tweets.Defaults.stop_words |= {\"amp\", \"rt\"}\n",
    "\n",
    "# The add_pipe function appends our functions to the default pipeline.\n",
    "nlp_tweets.add_pipe(\"sentencizer\", last=True)\n",
    "nlp_tweets.add_pipe(\"Detect languages\", name='Detect languages', last=True)\n",
    "nlp_tweets.add_pipe(\"Keep only German documents\", name='Keep only German documents', last=True)\n",
    "nlp_tweets.add_pipe(\"Remove non alphabetic words\", name=\"Remove non alphabetic words\", last=True)\n",
    "nlp_tweets.add_pipe(\"Remove stopwords\", name=\"Remove stopwords\", last=True)\n",
    "nlp_tweets.add_pipe(\"Lemmatize text\", name=\"Lemmatize text\", last=True)\n",
    "nlp_tweets.add_pipe(\"Lowercase Text\", name=\"Lowercase Text\", last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c62aae5f50a04c088a01de0f7a3d4d77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/164460 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tweets_explored[\"text_preprocessed\"] = tweets_explored.text.progress_apply(nlp_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcf25e8d419c4d07a8d048a1453c34ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/164460 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tweets_explored[\"text_preprocessed_sentence\"] = tweets_explored[\"text_preprocessed\"].progress_apply(\n",
    "    lambda x: \" \".join(x))\n",
    "tweets_preprocessed = tweets_explored[[\"full_name\", \"date\", \"party\", \"text\", \"text_preprocessed\",\n",
    "                                       \"text_preprocessed_sentence\", 'retweet_count', 'like_count']]\n",
    "tweets_preprocessed.replace('', np.NaN, inplace=True)\n",
    "tweets_preprocessed.dropna(inplace=True)\n",
    "pickle.dump(tweets_preprocessed, open(\"../data/processed/tweets_processed.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_explored = pd.read_csv(\"../data/interim/speeches_explored.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_explored.columns = [\"text\", \"date\", \"full_name\", \"party\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.lowercase(doc)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create spacy pipeline\n",
    "nlp_speeches = spacy.load('de_core_news_sm', exclude=pipeline_exclude)\n",
    "\n",
    "# The add_pipe function appends our functions to the default pipeline.\n",
    "nlp_speeches.add_pipe('sentencizer', last=True)\n",
    "nlp_speeches.add_pipe(\"Detect languages\", name='Detect languages', last=True)\n",
    "nlp_speeches.add_pipe(\"Keep only German documents\", name='Keep only German documents', last=True)\n",
    "nlp_speeches.add_pipe(\"Remove non alphabetic words\", name=\"Remove non alphabetic words\", last=True)\n",
    "nlp_speeches.add_pipe(\"Remove stopwords\", name=\"Remove stopwords\", last=True)\n",
    "nlp_speeches.add_pipe(\"Lemmatize text\", name=\"Lemmatize text\", last=True)\n",
    "nlp_speeches.add_pipe(\"Lowercase Text\", name=\"Lowercase Text\", last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fe759f8066a48a298c2ff18375c166b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4099 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "speeches_explored[\"text_preprocessed\"] = speeches_explored.text.progress_apply(nlp_speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86f340ac29d64f389c267827ed93a848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4099 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "speeches_explored[\"text_preprocessed_sentence\"] = speeches_explored[\"text_preprocessed\"].progress_apply(\n",
    "    lambda x: \" \".join(x))\n",
    "speeches_preprocessed = speeches_explored[[\"full_name\", \"date\", \"party\", \"text\",\n",
    "                                           \"text_preprocessed\", \"text_preprocessed_sentence\"]]\n",
    "speeches_preprocessed.replace('', np.NaN, inplace=True)\n",
    "speeches_preprocessed.dropna(inplace=True)\n",
    "pickle.dump(speeches_preprocessed, open(\"../data/processed/speeches_processed.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Speeches"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
